---
title: Build an AI PR Reviewer
---

import { Aside } from '@astrojs/starlight/components';

`kit` shines when an LLM needs to *understand a change in the context of the **entire** code-base*—exactly what a human reviewer does. A good review often requires looking beyond the immediate lines changed to understand their implications, check for consistency with existing patterns, and ensure no unintended side-effects arise. This tutorial walks through a **minimal but complete** AI PR-review bot that demonstrates how `kit` provides this crucial whole-repo context. The bot will:

1.  Fetches a GitHub PR (diff + metadata).
2.  Builds a `kit.Repository` for the **changed branch** so we can query *any* file, symbol or dependency as it exists in that PR.
3.  Generates a focused context bundle with `kit.llm_context.ContextAssembler`, which intelligently combines the diff, the full content of changed files, relevant neighboring code, and even semantically similar code from elsewhere in the repository.
4.  Sends the bundle to an LLM and posts the comments back to GitHub.

By the end you will see how a few dozen lines of Python—plus `kit`—give your LLM the *whole-repo* super-power, enabling it to perform more insightful and human-like code reviews.

## 1. Fetch PR data

To start, our AI reviewer needs the raw materials of the pull request. We'll use the GitHub REST API to grab two key pieces of information:

*   The **diff**: This contains the actual line-by-line changes proposed in the PR. It's the primary subject of the review.
*   The PR-head **commit SHA**: This is a unique identifier for the specific version of the code that the PR branch represents. `kit` will use this SHA to load and analyze the codebase exactly as it appears in the PR, ensuring our review is based on the correct snapshot of the files.

Use the GitHub REST API to grab the *diff* **and** the PR-head **commit SHA** (we'll need that to load files as they exist in the PR):

```python
import os, requests

def fetch_pr(repo, pr_number):
    token = os.getenv("GITHUB_TOKEN")
    url   = f"https://api.github.com/repos/{repo}/pulls/{pr_number}"
    r = requests.get(url, headers={
        "Accept": "application/vnd.github.v3.diff",
        "Authorization": f"token {token}",
    })
    r.raise_for_status()
    diff = r.text
    head_sha = r.headers.get("X-GitHub-Head-SHA") or r.json()["head"]["sha"]
    return diff, head_sha
```

---
## 2. Create a `Repository` for the PR branch

With the `head_sha` obtained, we can now leverage `kit.Repository` to access the codebase as it exists at that specific commit. A powerful feature of `kit` is its ability to work with remote Git repositories by specifying a URL and a `ref` (like our `head_sha`). This means `kit` can analyze the PR's code state often without needing a full, persistent local clone of that specific branch, making the process efficient.

`kit` can load a remote Git repo *at any commit*—no full clone required:

```python
from kit import Repository

# For this tutorial, we assume the remote repository's default branch 
# reflects the PR's base or that a local clone is already checked out 
# to the correct PR head commit (head_sha).
# kit.Repository, when given a URL, currently clones the default branch.
# Supporting checkout of a specific ref (like head_sha) directly via Repository 
# initialization is forthcoming.

repo = Repository(
    path_or_url="https://github.com/OWNER/REPO.git", # Replace with actual repo URL
    github_token=os.getenv("GITHUB_TOKEN"),
    cache_dir="~/.cache/kit",  # clones are cached for speed
)
```

The `cache_dir` parameter tells `kit` where to store parts of remote repositories it fetches. This caching significantly speeds up subsequent operations on the same repository or commit, which is very beneficial for a bot that might process multiple PRs or re-analyze a PR if it's updated.

Now `repo` can *instantly* answer questions like:
`repo.search_text("TODO")` (useful for checking if the PR resolves or introduces to-do items),
`repo.extract_symbols('src/foo.py')` (to understand the structure of a changed file),
`repo.find_symbol_usages('User')` (to see how a modified class or function is used elsewhere, helping to assess the impact of changes).
These capabilities allow our AI reviewer to gather rich contextual information far beyond the simple diff.

---
## 3. Build context for the LLM

The `ContextAssembler` is the workhorse for preparing the input to the LLM. It orchestrates several `kit` features to build a comprehensive understanding of the PR:

```python
from kit import Repository, ContextAssembler
from unidiff import PatchedFile, PatchSet

# Assume `repo`, `diff`, `pr_title`, `pr_description` are defined
# `diff` is the raw diff string
# `pr_title`, `pr_description` are strings from your PR metadata

assembler = repo.get_context_assembler()
patch = PatchSet(diff)
changed_files_paths = [pf.path for pf in patch]

# Add the raw diff
assembler.add_diff(diff)

# Add full content of files that were added or modified
# For deleted files, their content won't be in the head_sha, so get_file_content would fail.
# For simplicity, this tutorial will only add content for existing files in the PR head.
for p_file in patch: # Iterate through PatchedFile objects from unidiff
    if not p_file.is_removed_file: # Only add content for new or modified files
        assembler.add_file(p_file.path) 

# Optional but powerful: semantic search for related code using PR title/description.
# This leverages `repo.search_semantic()`. For the most insightful results, 
# this method can be backed by a `DocstringIndexer` that uses AI-generated summaries, 
# enabling matches based on conceptual similarity rather than just keyword overlap.
# Ensure your `Repository` instance is configured with an appropriate embedding function
# and (ideally) a `DocstringIndexer`. 
# See the "[Docstring Search](/tutorials/docstring_search)" and "[Semantic Code Search](/tutorials/semantic_code_search)" tutorials for setup details.

# Example (ensure pr_title and pr_description are available strings from your PR data):
queries_for_semantic_search = []
if pr_title: queries_for_semantic_search.append(pr_title)
if pr_description: queries_for_semantic_search.append(pr_description)

for original_query in queries_for_semantic_search:
    if original_query.strip(): # Ensure the query string is not empty or just whitespace
        search_hits = repo.search_semantic(original_query, top_k=3) 
        # Add a descriptive label for the LLM to understand the source of these results
        assembler.add_search_results(
            search_hits, 
            query=f"Code semantically related to: '{original_query}'"
        )

context_blob = assembler.format_context()
```

The `ContextAssembler` is used as follows:

1.  **`assembler.add_diff(diff)`**: This provides the LLM with the direct changes from the PR.
2.  **`assembler.add_file(p_file.path)`**: Supplying the full content of changed files allows the LLM to see modifications in their complete original context, not just the diff hunks.
3.  **Augment with Semantic Search (`assembler.add_search_results(...)`)**: This is a key step where `kit` truly empowers the AI reviewer. Beyond direct code connections, `kit`'s `repo.search_semantic()` method can unearth other code sections that are *conceptually related* to the PR's intent, even if not directly linked by calls or imports.

    You can use queries derived from the PR's title or description to find examples of similar functionality, relevant design patterns, or areas that might require parallel updates.

    **The Power of Summaries**: While `repo.search_semantic()` can operate on raw code, its effectiveness is significantly amplified when your `Repository` instance is configured with a `DocstringIndexer`. The `DocstringIndexer` (as detailed in the [Docstring Search Tutorial](/tutorials/docstring_search)) preprocesses your codebase, generating AI summaries for files or symbols. When `repo.search_semantic()` leverages this index, it matches based on the *meaning and purpose captured in these summaries*, leading to far more relevant and insightful results than simple keyword or raw code vector matching. This allows the AI reviewer to understand context like "find other places where we handle user authentication" even if the exact phrasing or code structure varies.

    The Python code snippet above illustrates how you might integrate this. Remember to ensure your `repo` object is properly set up with an embedding function and, for best results, a `DocstringIndexer`. Refer to the "[Docstring Search](/tutorials/docstring_search)" and "[Semantic Code Search](/tutorials/semantic_code_search)" tutorials for detailed setup guidance.

Finally, `assembler.format_context()` consolidates all the added information into a single string (`context_blob`), ready to be sent to the LLM. This step might also involve applying truncation or specific formatting to optimize for the LLM's input requirements.

---
## 4. Prepare the LLM Prompt

With the meticulously assembled `context_blob` from `kit`, we can now prompt an LLM. The quality of the prompt, including the system message that sets the LLM's role and the user message containing the context, is vital. Because `kit` has provided such comprehensive and well-structured context, the LLM is significantly better equipped to act like an "expert software engineer" and provide a nuanced, insightful review.

```python
from openai import OpenAI

client = OpenAI()
msg = client.chat.completions.create(
    model="gpt-4o",
    temperature=0.2,
    messages=[
        {"role": "system", "content": "You are an expert software engineer …"},
        {"role": "user",   "content": f"PR context:\n```\n{context_blob}\n```\nGive a review."},
    ],
)
review = msg.choices[0].message.content.strip()
```

---
## 5. Post the review back to GitHub

This final step completes the loop by taking the LLM's generated review and posting it as a comment on the GitHub pull request. This delivers the AI's insights directly to the developers, integrating the AI reviewer into the existing development workflow.

```python
requests.post(
    f"https://api.github.com/repos/{repo_full}/issues/{pr_number}/comments",
    headers={"Authorization": f"token {os.getenv('GITHUB_TOKEN')}",
             "Accept": "application/vnd.github.v3+json"},
    json={"body": review},
    timeout=10,
).raise_for_status()
```

---
## Where to go next?

This tutorial provides a foundational AI PR reviewer. `kit`'s components can help you extend it further:

*   **Chunk large diffs or files**: If a PR is very large, the `ContextAssembler` currently adds full content. You might need strategies to chunk very large files (e.g., using `repo.chunk_file_by_symbols` or `repo.chunk_file_by_lines`) or diffs before adding them to the assembler, or implement more granular context addition to stay within LLM limits.
*   **Custom ranking**: The `ContextAssembler` could be configured or extended to allow different weights for various context pieces (e.g., prioritizing semantic search matches that are highly relevant over less critical dependency information). `kit`'s search results, which often include scores, can inform this process.
*   **Inline comments**: To provide more granular feedback, parse the LLM's output to identify suggestions pertaining to specific files and lines. `kit`'s symbol information (which includes line numbers from `RepoMapper`) is crucial for accurately mapping these suggestions back to the codebase and using the GitHub *review* API to post comments directly on the diff.
*   **Supersonic**: For more advanced automation, tools like Supersonic could leverage `kit`'s understanding and context to not just suggest, but also *automatically apply* LLM-suggested changes, potentially opening follow-up PRs. This might involve `kit.Repository`'s file access methods and (in the future) more advanced code manipulation primitives.

> With `kit` your LLM sees code the way *humans* do: in the rich context of the entire repository.  Better signal in → better reviews out.
