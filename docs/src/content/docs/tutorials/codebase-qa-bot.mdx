---
title: Codebase Q&A Bot with Summaries
---

import { Steps } from '@astrojs/starlight/components';

This tutorial demonstrates how to build a simple question-answering bot for your codebase. The bot will:

1.  Use `DocstringIndexer` to create semantic summaries of each file.
2.  When a user asks a question, use `SummarySearcher` to find relevant file summaries.
3.  Fetch the full code of those top files.
4.  Use `ContextAssembler` to build a concise prompt for an LLM.
5.  Get an answer from the LLM.

This approach is powerful because it combines semantic understanding (from summaries) with the full detail of the source code, allowing an LLM to answer nuanced questions.

## Prerequisites

*   You have `kit` installed (`pip install cased-kit`).
*   You have an OpenAI API key set (`export OPENAI_API_KEY=...`).
*   You have a local Git repository you want to query.

## Steps

<Steps>

1.  **Initialize Components**

    First, let's set up our `Repository`, `DocstringIndexer`, `Summarizer` (for the indexer), `SummarySearcher`, and `ContextAssembler`.

    ```python
    from kit import Repository, DocstringIndexer, Summarizer, SummarySearcher, ContextAssembler
    from kit.llms.openai import OpenAIConfig # Or AnthropicConfig, GoogleConfig

    # --- Configuration ---
    REPO_PATH = "/path/to/your/local/git/repo" #! MODIFY
    INDEX_DB_PATH = "./my_code_qa_index.db" # vector database path

    # Use a specific summarizer model for indexing, can be different from Q&A LLM
    INDEXER_LLM_CONFIG = OpenAIConfig(model="gpt-4o")

    # LLM for answering the question based on context
    QA_LLM_CONFIG = OpenAIConfig(model="gpt-4o") # Or your preferred model
    MAX_CONTEXT_CHARS = 12000 # For ContextAssembler
    TOP_K_SUMMARIES = 3 # How many file summaries to retrieve
    # --- END Configuration ---

    repo = Repository(REPO_PATH)

    # For DocstringIndexer
    summarizer_for_indexing = Summarizer(config=INDEXER_LLM_CONFIG)
    indexer = DocstringIndexer(repo, summarizer_for_indexing, db_path=INDEX_DB_PATH)

    # For SummarySearcher
    searcher = SummarySearcher(repo, db_path=INDEX_DB_PATH)

    # For assembling context for the Q&A LLM
    assembler = ContextAssembler(repo) # Using default max_chars, etc.

    # We'll need an LLM client to ask the final question
    # (Using Summarizer as a convenient way to get a configured client)
    qa_llm_client = Summarizer(config=QA_LLM_CONFIG)._get_llm_client()
    print("Components initialized.")
    ```

    Make sure to replace `"/path/to/your/local/git/repo"` with the actual path to your repository.

2.  **Build or Load the Index**

    The `DocstringIndexer` needs to process your repository to create summaries and embed them. This can take time for large repositories. We'll check if an index already exists and build it if not.

    ```python
    import os

    if not os.path.exists(INDEX_DB_PATH):
        print(f"Index not found at {INDEX_DB_PATH}. Building...")
        # Build a symbol-level index for more granular results
        indexer.build(level="symbol", file_extensions=[".py", ".js", ".md"], force=True)
        print("Symbol-level index built successfully.")
    else:
        print(f"Found existing index at {INDEX_DB_PATH}.")
    ```

3.  **Define the Question-Answering Function**

    This function will orchestrate the search, context assembly, and LLM query.

    ```python
    def answer_question(user_query: str) -> str:
        print(f"\nSearching for files relevant to: '{user_query}'")
        # 1. Search for relevant file summaries
        #    search_results format: [{'file_path': str, 'summary': str, 'score': float}, ...]
        search_results = searcher.search(user_query, top_k=TOP_K_SUMMARIES)

        if not search_results:
            return "I couldn't find any relevant files in the codebase to answer your question."

        print(f"Found {len(search_results)} relevant document summaries.")
        for i, res in enumerate(search_results):
            if res.get('level') == 'symbol':
                print(f"  {i+1}. Symbol: {res.get('symbol_name')} in {res['file_path']} (Type: {res.get('symbol_type')}, Score: {res['score']:.4f})")
            else:
                print(f"  {i+1}. File: {res['file_path']} (Score: {res['score']:.4f})")

        # 2. Get code for these top results to build context
        #    ContextAssembler expects chunks like [{'code': str, 'file_path': str}, ...]
        context_chunks = []
        for res in search_results:
            try:
                code_content = ""
                chunk_identifier = ""
                if res.get('level') == 'symbol':
                    target_node_path = res.get('symbol_name') # This is the node_path from index metadata
                    file_path_for_symbols = res['file_path']
                    if target_node_path:
                        all_symbols_in_file = repo.extract_symbols(file_path_for_symbols)
                        found_symbol_code = None
                        for sym_info in all_symbols_in_file:
                            if sym_info.get('node_path') == target_node_path:
                                found_symbol_code = sym_info.get('code')
                                break
                        if found_symbol_code:
                            code_content = found_symbol_code
                        else:
                            print(f"Warning: Could not retrieve code for symbol via node_path '{target_node_path}' in {file_path_for_symbols}")
                            continue # Skip to next search result
                        chunk_identifier = f"{file_path_for_symbols}::{target_node_path}"
                    else:
                        print(f"Warning: Symbol level result missing 'symbol_name' (node_path): {res}")
                        continue # Skip to next search result
                else: # Default to file level
                    code_content = repo.get_file_content(res['file_path'])
                    chunk_identifier = res['file_path']
                
                if not code_content.strip():
                    print(f"Warning: No content retrieved for {chunk_identifier}, skipping.")
                    continue

                context_chunks.append({
                    "code": code_content,
                    "identifier": chunk_identifier
                })
            except FileNotFoundError: # Still relevant for get_file_content or if extract_symbols fails to find file
                print(f"Warning: File not found when trying to retrieve content for {res['file_path']}")
        
        if not context_chunks:
             return "Found relevant file names, but could not retrieve their content."

        # 3. Assemble the context for the LLM
        #    Here, we use the simpler from_chunks method. The ContextAssembler class has
        #    more granular methods (add_file, add_diff) if you need more control.
        prompt_context = assembler.from_chunks(context_chunks, max_chars=MAX_CONTEXT_CHARS)

        # 4. Formulate the prompt and ask the LLM
        system_message = (
            "You are a helpful AI assistant with expertise in the provided codebase. "
            "Answer the user's question based *only* on the following code context. "
            "If the answer is not found in the context, say so. Be concise."
        )
        final_prompt = f"## Code Context:\n\n{prompt_context}\n\n## User Question:\n\n{user_query}\n\n## Answer:"

        print("\nSending request to LLM...")
        
        # Assuming OpenAI client for this example structure
        # Adapt if using Anthropic or Google
        if isinstance(QA_LLM_CONFIG, OpenAIConfig):
            response = qa_llm_client.chat.completions.create(
                model=QA_LLM_CONFIG.model,
                messages=[
                    {"role": "system", "content": system_message},
                    {"role": "user", "content": final_prompt}
                ]
            )
            answer = response.choices[0].message.content
        # Add elif for AnthropicConfig, GoogleConfig if desired, or abstract further
        else:
            # Simplified fallback or placeholder for other LLMs
            # In a real app, you'd implement the specific API calls here
            raise NotImplementedError(f"LLM client for {type(QA_LLM_CONFIG)} not fully implemented in this example.")

        return answer
    ```

4.  **Ask a Question!**

    Now, let's try it out.

    ```python
    my_question = "How does the authentication middleware handle expired JWTs?"
    # Or try: "What's the main purpose of the UserNotifications class's send_email method?"
    # Or: "Where is the database connection retry logic implemented in the db_utils module?"

    llm_answer = answer_question(my_question)
    print(f"\nLLM's Answer:\n{llm_answer}")
    ```

    ```text title="Example Output (will vary based on your repo & LLM)"
    Components initialized.
    Found existing index at ./my_code_qa_index.db.

    Searching for files relevant to: 'How does the authentication middleware handle expired JWTs?'
    Found 3 relevant document summaries.
      1. Symbol: authenticate in src/auth/middleware.py (Type: function, Score: 0.8765)
      2. File: src/utils/jwt_helpers.py (Score: 0.7912)
      3. File: tests/auth/test_middleware.py (Score: 0.7500)

    Sending request to LLM...

    LLM's Answer:
    The `authenticate` function in `src/auth/middleware.py` checks for JWT expiration. If an `ExpiredSignatureError` is caught during token decoding (likely using a helper from `src/utils/jwt_helpers.py`), it returns a 401 Unauthorized response, typically with a JSON body like `{"error": "Token expired"}`.
    ```

</Steps>