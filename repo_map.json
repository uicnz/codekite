{
  "repository": {
    "path": ".",
    "is_remote": false,
    "file_count": 131,
    "directory_count": 43,
    "python_file_count": 51
  },
  "file_tree": [
    "repo_map.json",
    "kit_index.json",
    "LICENSE",
    "uv.lock",
    ".pytest_cache",
    "Makefile",
    "pyproject.toml",
    "tests",
    "queries",
    "docs",
    "README.md",
    ".mypy_cache",
    ".gitignore",
    "package-lock.json",
    "examples",
    ".venv",
    "scripts",
    ".github",
    "test.sh",
    "src",
    "tests/test_dependency_analyzer.py",
    "tests/golden_go.go",
    "tests/test_repo_integration.py",
    "tests/golden_typescript_complex.ts",
    "tests/test_tree_sitter_languages.py",
    "tests/test_symbol_extraction_multilang.py",
    "tests/test_summaries.py",
    "tests/test_ruby_c_symbols.py",
    "tests/golden_rust.rs",
    "tests/test_summarizer.py",
    "tests/test_hcl_symbols.py",
    "tests/__pycache__",
    "tests/test_cross_file_impact.py",
    "tests/golden_hcl.tf",
    "tests/test_vector_searcher.py",
    "tests/golden_python.py",
    "tests/test_golden_symbols.py",
    "tests/test_code_searcher.py",
    "tests/test_repo_mapper.py",
    "tests/test_repo.py",
    "tests/test_docstring_indexer.py",
    "tests/test_context_assembler.py",
    "tests/golden_typescript.ts",
    "tests/test_context_extractor.py",
    "tests/fixtures",
    "tests/test_java_symbols.py",
    "tests/golden_python_complex.py",
    "tests/test_typescript_symbol_extraction.py",
    "tests/test_docstring_incremental.py",
    "queries/go",
    "queries/python",
    "queries/typescript",
    "queries/rust",
    "queries/java",
    "queries/hcl",
    "queries/javascript",
    "queries/c",
    "queries/ruby",
    "docs/astro.config.mjs",
    "docs/public",
    "docs/.gitignore",
    "docs/package-lock.json",
    "docs/package.json",
    "docs/tsconfig.json",
    "docs/favicon.svg",
    "docs/src",
    "examples/test_remote_repo.py",
    "examples/map_repository.py",
    "examples/README.md",
    "examples/semantic_code_search.py",
    "examples/test_llm_summarization.py",
    "examples/test_kit_capabilities.py",
    "scripts/benchmark.py",
    "scripts/index.py",
    "scripts/typecheck.sh",
    "scripts/release.sh",
    "scripts/test.sh",
    ".github/workflows",
    "src/kit",
    "src/cased_kit.egg-info",
    "src/kit/llm_context.py",
    "src/kit/docstring_indexer.py",
    "src/kit/context_extractor.py",
    "src/kit/dependency_analyzer.py",
    "src/kit/__init__.py",
    "src/kit/__pycache__",
    "src/kit/tree_sitter_symbol_extractor.py",
    "src/kit/cli.py",
    "src/kit/code_searcher.py",
    "src/kit/api",
    "src/kit/repository.py",
    "src/kit/repo_mapper.py",
    "src/kit/vector_searcher.py",
    "src/kit/summaries.py",
    "src/kit/api/__init__.py",
    "src/kit/api/__pycache__",
    "src/kit/api/app.py",
    ".github/workflows/ci.yml",
    "docs/public/kit.png",
    "docs/src/env.d.ts",
    "docs/src/content",
    "docs/src/styles",
    "docs/src/components",
    "docs/src/content/docs",
    "docs/src/content/config.ts",
    "docs/src/styles/theme.css",
    "docs/src/styles/fonts",
    "docs/src/components/Card.astro",
    "docs/src/components/Update.astro",
    "docs/src/components/Changelog.astro",
    "docs/src/components/CasedCard.astro",
    "docs/src/components/PageIntro.astro",
    "docs/src/components/CardGroup.astro",
    "docs/src/components/Frame.astro",
    "docs/src/styles/fonts/iAWriterQuattroV.ttf",
    "docs/src/styles/fonts/IBMPlexSansV.ttf",
    "docs/src/content/docs/recipes.mdx",
    "docs/src/content/docs/development",
    "docs/src/content/docs/introduction",
    "docs/src/content/docs/index.mdx",
    "docs/src/content/docs/README.mdx",
    "docs/src/content/docs/extending",
    "docs/src/content/docs/api",
    "docs/src/content/docs/tutorials",
    "docs/src/content/docs/core-concepts",
    "docs/src/content/docs/development/roadmap.mdx",
    "docs/src/content/docs/development/running-tests.mdx",
    "docs/src/content/docs/introduction/overview.mdx",
    "docs/src/content/docs/introduction/usage-guide.mdx",
    "docs/src/content/docs/introduction/quickstart.mdx",
    "docs/src/content/docs/extending/adding-languages.mdx",
    "docs/src/content/docs/api/code_searcher.mdx",
    "docs/src/content/docs/api/summarizer.mdx",
    "docs/src/content/docs/api/summary-searcher.mdx",
    "docs/src/content/docs/api/docstring-indexer.mdx",
    "docs/src/content/docs/api/repository.mdx",
    "docs/src/content/docs/tutorials/ai_pr_reviewer.mdx",
    "docs/src/content/docs/tutorials/semantic_code_search.mdx",
    "docs/src/content/docs/tutorials/recipes.mdx",
    "docs/src/content/docs/tutorials/dump_repo_map.mdx",
    "docs/src/content/docs/tutorials/codebase-qa-bot.mdx",
    "docs/src/content/docs/tutorials/exploring-kit-interactively.mdx",
    "docs/src/content/docs/tutorials/integrating_supersonic.mdx",
    "docs/src/content/docs/tutorials/docstring_search.mdx",
    "docs/src/content/docs/tutorials/dependency_graph_visualizer.mdx",
    "docs/src/content/docs/tutorials/codebase_summarizer.mdx",
    "docs/src/content/docs/core-concepts/code-summarization.mdx",
    "docs/src/content/docs/core-concepts/repository-api.mdx",
    "docs/src/content/docs/core-concepts/context-assembly.mdx",
    "docs/src/content/docs/core-concepts/configuring-semantic-search.mdx",
    "docs/src/content/docs/core-concepts/docstring-indexing.mdx",
    "docs/src/content/docs/core-concepts/search-approaches.mdx",
    "docs/src/content/docs/core-concepts/llm-context-best-practices.mdx",
    "docs/src/content/docs/core-concepts/semantic-search.mdx",
    "queries/go/tags.scm",
    "queries/python/tags.scm",
    "queries/typescript/tags.scm",
    "queries/rust/tags.scm",
    "queries/java/tags.scm",
    "queries/hcl/tags.scm",
    "queries/javascript/tags.scm",
    "queries/c/tags.scm",
    "queries/ruby/tags.scm",
    "tests/fixtures/realistic_repo",
    "tests/fixtures/realistic_repo/__init__.py",
    "tests/fixtures/realistic_repo/models",
    "tests/fixtures/realistic_repo/utils.py",
    "tests/fixtures/realistic_repo/app.py",
    "tests/fixtures/realistic_repo/services",
    "tests/fixtures/realistic_repo/models/user.py",
    "tests/fixtures/realistic_repo/models/__init__.py",
    "tests/fixtures/realistic_repo/services/auth.py",
    "tests/fixtures/realistic_repo/services/db.py",
    "tests/fixtures/realistic_repo/services/__init__.py"
  ],
  "symbols": {
    "tests/test_dependency_analyzer.py": [
      {
        "name": "test_dependency_analyzer_basic",
        "type": "function",
        "start_line": 12,
        "end_line": 47,
        "code": "def test_dependency_analyzer_basic():\n    \"\"\"Test basic functionality of the DependencyAnalyzer.\"\"\"\n    with tempfile.TemporaryDirectory() as tmpdir:\n        os.makedirs(f\"{tmpdir}/mypackage\")\n        \n        with open(f\"{tmpdir}/mypackage/__init__.py\", \"w\") as f:\n            f.write(\"# Empty init file\\n\")\n        \n        with open(f\"{tmpdir}/mypackage/module1.py\", \"w\") as f:\n            f.write(\"\"\"\nfrom mypackage import module2\n\ndef function1():\n    return module2.function2()\n\"\"\")\n        \n        with open(f\"{tmpdir}/mypackage/module2.py\", \"w\") as f:\n            f.write(\"\"\"\nimport os\nimport sys\n\ndef function2():\n    return os.path.join('a', 'b')\n\"\"\")\n        \n        repo = Repository(tmpdir)\n        analyzer = DependencyAnalyzer(repo)\n        \n        graph = analyzer.build_dependency_graph()\n        \n        assert \"mypackage.module1\" in graph\n        assert \"mypackage.module2\" in graph\n        assert \"os\" in graph\n        \n        assert \"mypackage.module2\" in graph[\"mypackage.module1\"][\"dependencies\"]\n        assert \"os\" in graph[\"mypackage.module2\"][\"dependencies\"]",
        "file": "tests/test_dependency_analyzer.py"
      },
      {
        "name": "test_dependency_analyzer_cycles",
        "type": "function",
        "start_line": 50,
        "end_line": 99,
        "code": "def test_dependency_analyzer_cycles():\n    \"\"\"Test the cycle detection in the DependencyAnalyzer.\"\"\"\n    with tempfile.TemporaryDirectory() as tmpdir:\n        os.makedirs(f\"{tmpdir}/cyclicpackage\")\n        \n        with open(f\"{tmpdir}/cyclicpackage/__init__.py\", \"w\") as f:\n            f.write(\"# Empty init file\\n\")\n        \n        with open(f\"{tmpdir}/cyclicpackage/a.py\", \"w\") as f:\n            f.write(\"\"\"\nfrom cyclicpackage import b\n\ndef func_a():\n    return b.func_b()\n\"\"\")\n        \n        with open(f\"{tmpdir}/cyclicpackage/b.py\", \"w\") as f:\n            f.write(\"\"\"\nfrom cyclicpackage import c\n\ndef func_b():\n    return c.func_c()\n\"\"\")\n        \n        with open(f\"{tmpdir}/cyclicpackage/c.py\", \"w\") as f:\n            f.write(\"\"\"\nfrom cyclicpackage import a\n\ndef func_c():\n    return a.func_a()\n\"\"\")\n        \n        repo = Repository(tmpdir)\n        analyzer = DependencyAnalyzer(repo)\n        \n        analyzer.build_dependency_graph()\n        \n        cycles = analyzer.find_cycles()\n        \n        assert len(cycles) > 0\n        \n        found_cycle = False\n        for cycle in cycles:\n            if ('cyclicpackage.a' in cycle and \n                'cyclicpackage.b' in cycle and \n                'cyclicpackage.c' in cycle):\n                found_cycle = True\n                break\n        \n        assert found_cycle, \"Expected cycle between a, b, and c was not found\"",
        "file": "tests/test_dependency_analyzer.py"
      },
      {
        "name": "test_dependency_analyzer_exports",
        "type": "function",
        "start_line": 102,
        "end_line": 144,
        "code": "def test_dependency_analyzer_exports():\n    \"\"\"Test the export functionality of the DependencyAnalyzer.\"\"\"\n    with tempfile.TemporaryDirectory() as tmpdir:\n        with open(f\"{tmpdir}/main.py\", \"w\") as f:\n            f.write(\"\"\"\nimport helper\n\ndef main():\n    return helper.helper_func()\n\"\"\")\n        \n        with open(f\"{tmpdir}/helper.py\", \"w\") as f:\n            f.write(\"\"\"\nimport os\n\ndef helper_func():\n    return os.path.exists('test')\n\"\"\")\n        \n        repo = Repository(tmpdir)\n        analyzer = DependencyAnalyzer(repo)\n        \n        analyzer.build_dependency_graph()\n        \n        export_file = f\"{tmpdir}/deps.json\"\n        result = analyzer.export_dependency_graph(output_format=\"json\", output_path=export_file)\n        \n        assert os.path.exists(export_file)\n        assert result == export_file\n        \n        with open(export_file, 'r') as f:\n            data = json.load(f)\n            assert \"main\" in data\n            assert \"helper\" in data[\"main\"][\"dependencies\"]\n        \n        dot_file = f\"{tmpdir}/deps.dot\"\n        result = analyzer.export_dependency_graph(output_format=\"dot\", output_path=dot_file)\n        \n        assert os.path.exists(dot_file)\n        with open(dot_file, 'r') as f:\n            content = f.read()\n            assert 'digraph G' in content\n            assert '\"main\" -> \"helper\"' in content",
        "file": "tests/test_dependency_analyzer.py"
      },
      {
        "name": "test_get_module_dependencies",
        "type": "function",
        "start_line": 147,
        "end_line": 171,
        "code": "def test_get_module_dependencies():\n    \"\"\"Test getting dependencies for a specific module.\"\"\"\n    with tempfile.TemporaryDirectory() as tmpdir:\n        with open(f\"{tmpdir}/complex.py\", \"w\") as f:\n            f.write(\"\"\"\nimport os\nimport sys\nimport json\nfrom datetime import datetime\n\ndef complex_func():\n    return os.path.join(str(datetime.now()), 'file.json')\n\"\"\")\n        \n        repo = Repository(tmpdir)\n        analyzer = DependencyAnalyzer(repo)\n        \n        analyzer.build_dependency_graph()\n        \n        direct_deps = analyzer.get_module_dependencies(\"complex\")\n        \n        assert \"os\" in direct_deps\n        assert \"sys\" in direct_deps\n        assert \"json\" in direct_deps\n        assert \"datetime\" in direct_deps",
        "file": "tests/test_dependency_analyzer.py"
      },
      {
        "name": "test_get_dependents",
        "type": "function",
        "start_line": 174,
        "end_line": 205,
        "code": "def test_get_dependents():\n    \"\"\"Test getting modules that depend on a specified module.\"\"\"\n    with tempfile.TemporaryDirectory() as tmpdir:\n        with open(f\"{tmpdir}/utils.py\", \"w\") as f:\n            f.write(\"\"\"\ndef utility_func():\n    return \"util\"\n\"\"\")\n        \n        with open(f\"{tmpdir}/module1.py\", \"w\") as f:\n            f.write(\"import utils\\ndef func1(): return utils.utility_func()\")\n        \n        with open(f\"{tmpdir}/module2.py\", \"w\") as f:\n            f.write(\"from utils import utility_func\\ndef func2(): return utility_func()\")\n        \n        with open(f\"{tmpdir}/module3.py\", \"w\") as f:\n            f.write(\"import module1\\ndef func3(): return module1.func1()\")\n        \n        repo = Repository(tmpdir)\n        analyzer = DependencyAnalyzer(repo)\n        \n        analyzer.build_dependency_graph()\n        \n        direct_dependents = analyzer.get_dependents(\"utils\")\n        assert \"module1\" in direct_dependents\n        assert \"module2\" in direct_dependents\n        assert \"module3\" not in direct_dependents\n        \n        all_dependents = analyzer.get_dependents(\"utils\", include_indirect=True)\n        assert \"module1\" in all_dependents\n        assert \"module2\" in all_dependents\n        assert \"module3\" in all_dependents",
        "file": "tests/test_dependency_analyzer.py"
      },
      {
        "name": "test_file_dependencies",
        "type": "function",
        "start_line": 208,
        "end_line": 245,
        "code": "def test_file_dependencies():\n    \"\"\"Test getting detailed dependency information for a specific file.\"\"\"\n    with tempfile.TemporaryDirectory() as tmpdir:\n        with open(f\"{tmpdir}/app.py\", \"w\") as f:\n            f.write(\"\"\"\nimport os\nimport sys\nimport json\n\ndef app_function():\n    return json.dumps({'status': 'ok'})\n\"\"\")\n        \n        with open(f\"{tmpdir}/server.py\", \"w\") as f:\n            f.write(\"\"\"\nimport app\n\ndef start_server():\n    return app.app_function()\n\"\"\")\n        \n        repo = Repository(tmpdir)\n        analyzer = DependencyAnalyzer(repo)\n        \n        analyzer.build_dependency_graph()\n        \n        file_deps = analyzer.get_file_dependencies(\"app.py\")\n        \n        assert file_deps[\"file_path\"] == \"app.py\"\n        assert file_deps[\"module_name\"] == \"app\"\n        \n        dependencies = {d[\"module\"] for d in file_deps[\"dependencies\"]}\n        assert \"os\" in dependencies\n        assert \"sys\" in dependencies\n        assert \"json\" in dependencies\n        \n        dependents = {d[\"module\"] for d in file_deps[\"dependents\"]}\n        assert \"server\" in dependents",
        "file": "tests/test_dependency_analyzer.py"
      },
      {
        "name": "test_dependency_report",
        "type": "function",
        "start_line": 248,
        "end_line": 297,
        "code": "def test_dependency_report():\n    \"\"\"Test generating a comprehensive dependency report.\"\"\"\n    with tempfile.TemporaryDirectory() as tmpdir:\n        os.makedirs(f\"{tmpdir}/myproject\")\n        \n        with open(f\"{tmpdir}/myproject/__init__.py\", \"w\") as f:\n            f.write(\"# Project init\\n\")\n        \n        with open(f\"{tmpdir}/myproject/core.py\", \"w\") as f:\n            f.write(\"\"\"\nimport os\nimport sys\n\ndef core_function():\n    return os.path.join('a', 'b')\n\"\"\")\n        \n        with open(f\"{tmpdir}/myproject/api.py\", \"w\") as f:\n            f.write(\"\"\"\nfrom myproject import core\nimport json\n\ndef api_function():\n    return json.dumps(core.core_function())\n\"\"\")\n        \n        with open(f\"{tmpdir}/myproject/utils.py\", \"w\") as f:\n            f.write(\"\"\"\nimport os\nimport datetime\n\ndef utils_function():\n    return os.path.join(str(datetime.datetime.now()), 'log.txt')\n\"\"\")\n        \n        repo = Repository(tmpdir)\n        analyzer = DependencyAnalyzer(repo)\n        \n        analyzer.build_dependency_graph()\n        \n        report_file = f\"{tmpdir}/report.json\"\n        report = analyzer.generate_dependency_report(report_file)\n        \n        assert \"summary\" in report\n        assert \"external_dependencies\" in report\n        \n        assert os.path.exists(report_file)\n        \n        assert report[\"summary\"][\"total_modules\"] > 5\n        assert \"os\" in report[\"external_dependencies\"]",
        "file": "tests/test_dependency_analyzer.py"
      }
    ],
    "tests/test_repo_integration.py": [
      {
        "name": "test_repo_index_and_chunking",
        "type": "function",
        "start_line": 3,
        "end_line": 18,
        "code": "def test_repo_index_and_chunking():\n    with tempfile.TemporaryDirectory() as tmpdir:\n        with open(f\"{tmpdir}/a.py\", \"w\") as f:\n            f.write(\"\"\"def foo(): pass\\nclass Bar: pass\\n\"\"\")\n        repository = Repository(tmpdir)\n        idx = repository.index()\n        assert \"file_tree\" in idx and \"symbols\" in idx\n        assert any(\"a.py\" in f for f in idx[\"symbols\"])\n        lines = repository.chunk_file_by_lines(\"a.py\", max_lines=1)\n        assert len(lines) > 1\n        syms = repository.chunk_file_by_symbols(\"a.py\")\n        names = {s[\"name\"] for s in syms}\n        assert \"foo\" in names\n        assert \"Bar\" in names\n        ctx = repository.extract_context_around_line(\"a.py\", 1)\n        assert ctx is not None",
        "file": "tests/test_repo_integration.py"
      }
    ],
    "tests/test_tree_sitter_languages.py": [
      {
        "name": "test_parser_root_node",
        "type": "function",
        "start_line": 15,
        "end_line": 25,
        "code": "def test_parser_root_node(lang, src):\n    parser = get_parser(lang)\n    tree = parser.parse(src)\n    root = tree.root_node\n    assert root is not None\n    # Root node type should be non-empty string\n    assert isinstance(root.type, str) and root.type\n    # Should have at least one child node\n    assert root.child_count > 0\n    # Print for debug\n    print(f\"{lang} root: {root.type}, children: {root.child_count}\")",
        "file": "tests/test_tree_sitter_languages.py"
      }
    ],
    "tests/test_symbol_extraction_multilang.py": [
      {
        "name": "test_symbol_extraction",
        "type": "function",
        "start_line": 16,
        "end_line": 28,
        "code": "def test_symbol_extraction(ext: str, code: str):\n    # Ensure tree-sitter has a parser+query for this extension\n    parser = TreeSitterSymbolExtractor.get_parser(ext)\n    query = TreeSitterSymbolExtractor.get_query(ext)\n    if not parser or not query:\n        pytest.skip(f\"Language for {ext} not supported in this environment\")\n\n    symbols = TreeSitterSymbolExtractor.extract_symbols(ext, code)\n    assert symbols, f\"No symbols extracted for {ext}\"\n\n    # Simple sanity: expect 'foo' OR 'Bar' present\n    names = {s.get(\"name\") for s in symbols}\n    assert any(name in names for name in {\"foo\", \"Bar\", \"main\"}), f\"Expected symbols missing for {ext}: {names}\"",
        "file": "tests/test_symbol_extraction_multilang.py"
      }
    ],
    "tests/test_summaries.py": [
      {
        "name": "mock_repo",
        "type": "function",
        "start_line": 47,
        "end_line": 54,
        "code": "def mock_repo():\n    \"\"\"Provides a MagicMock instance of the Repository with required methods.\"\"\"\n    repo = MagicMock()  # Do not enforce spec to allow arbitrary attributes\n    repo.get_abs_path = MagicMock(side_effect=lambda x: f\"/abs/path/to/{x}\")  # Mock get_abs_path\n    repo.get_symbol_text = MagicMock()\n    repo.get_file_content = MagicMock()  # Mock get_file_content\n    repo.extract_symbols = MagicMock()  # Mock extract_symbols\n    return repo",
        "file": "tests/test_summaries.py"
      },
      {
        "name": "temp_code_file",
        "type": "function",
        "start_line": 57,
        "end_line": 62,
        "code": "def temp_code_file(tmp_path):\n    \"\"\"Creates a temporary code file and returns its path.\"\"\"\n    file_path = tmp_path / \"sample_code.py\"\n    file_content = \"def hello():\\n    print('Hello, world!')\\n\"\n    file_path.write_text(file_content)\n    return str(file_path)",
        "file": "tests/test_summaries.py"
      },
      {
        "name": "test_summarizer_init_default_is_openai",
        "type": "function",
        "start_line": 68,
        "end_line": 86,
        "code": "def test_summarizer_init_default_is_openai(mock_tiktoken, mock_openai_constructor, mock_repo):\n    # Test that Summarizer defaults to OpenAIConfig if no config is provided.\n    # The Summarizer will then attempt to initialize an OpenAI client.\n    # We patch os.environ to simulate API key being set to avoid actual ValueError.\n    \n    mock_openai_client_instance = MagicMock()\n    mock_openai_constructor.return_value = mock_openai_client_instance\n\n    # We need to patch the import in __init__ to return our mock module\n    with patch.dict(os.environ, {\"OPENAI_API_KEY\": \"test_dummy_key\"}):\n        with patch('builtins.__import__', side_effect=lambda name, *args, **kwargs: \n                  MagicMock(OpenAI=mock_openai_constructor) if name == 'openai' else __import__(name, *args, **kwargs)):\n            try:\n                summarizer = Summarizer(repo=mock_repo) # No config provided\n                assert isinstance(summarizer.config, OpenAIConfig), \"Config should default to OpenAIConfig\"\n                # The OpenAI constructor should be called once with our API key\n                mock_openai_constructor.assert_called_once_with(api_key=\"test_dummy_key\")\n            except ValueError as e:\n                pytest.fail(f\"Summarizer initialization with dummy API key failed unexpectedly: {e}\")",
        "file": "tests/test_summaries.py"
      },
      {
        "name": "test_summarizer_init_openai",
        "type": "function",
        "start_line": 88,
        "end_line": 93,
        "code": "def test_summarizer_init_openai(mock_repo):\n    config = OpenAIConfig(api_key=\"test_openai_key\")\n    summarizer = Summarizer(repo=mock_repo, config=config)\n    assert summarizer.repo == mock_repo\n    assert summarizer.config == config\n    assert isinstance(summarizer.config, OpenAIConfig)",
        "file": "tests/test_summaries.py"
      },
      {
        "name": "test_summarizer_init_anthropic",
        "type": "function",
        "start_line": 95,
        "end_line": 100,
        "code": "def test_summarizer_init_anthropic(mock_repo):\n    config = AnthropicConfig(api_key=\"test_anthropic_key\")\n    summarizer = Summarizer(repo=mock_repo, config=config)\n    assert summarizer.repo == mock_repo\n    assert summarizer.config == config\n    assert isinstance(summarizer.config, AnthropicConfig)",
        "file": "tests/test_summaries.py"
      },
      {
        "name": "test_summarizer_init_google",
        "type": "function",
        "start_line": 102,
        "end_line": 107,
        "code": "def test_summarizer_init_google(mock_repo):\n    config = GoogleConfig(api_key=\"test_google_key\")\n    summarizer = Summarizer(repo=mock_repo, config=config)\n    assert summarizer.repo == mock_repo\n    assert summarizer.config == config\n    assert isinstance(summarizer.config, GoogleConfig)",
        "file": "tests/test_summaries.py"
      },
      {
        "name": "test_summarizer_init_invalid_config_type",
        "type": "function",
        "start_line": 109,
        "end_line": 114,
        "code": "def test_summarizer_init_invalid_config_type(mock_repo):\n    class InvalidConfig:\n        pass\n    config = InvalidConfig()\n    with pytest.raises(TypeError, match=\"Unsupported LLM configuration\"): # As per Summarizer.__init__\n        Summarizer(repo=mock_repo, config=config)",
        "file": "tests/test_summaries.py"
      },
      {
        "name": "InvalidConfig",
        "type": "class",
        "start_line": 110,
        "end_line": 111,
        "code": "class InvalidConfig:\n        pass",
        "file": "tests/test_summaries.py"
      },
      {
        "name": "test_summarizer_init_openai_config_with_base_url",
        "type": "function",
        "start_line": 116,
        "end_line": 138,
        "code": "def test_summarizer_init_openai_config_with_base_url(mock_repo):\n    \"\"\"Test Summarizer correctly initializes OpenAI client with a custom base_url.\"\"\"\n    custom_api_key = \"test_openrouter_key\"\n    custom_base_url = \"https://openrouter.ai/api/v1/test\"\n    custom_model = \"openrouter/some-model\"\n\n    config = OpenAIConfig(\n        api_key=custom_api_key,\n        base_url=custom_base_url,\n        model=custom_model\n    )\n\n    with patch('openai.OpenAI', create=True) as mock_openai_constructor:\n        mock_openai_client_instance = MagicMock()\n        mock_openai_constructor.return_value = mock_openai_client_instance\n        \n        summarizer = Summarizer(repo=mock_repo, config=config)\n        \n        mock_openai_constructor.assert_called_once_with(\n            api_key=custom_api_key,\n            base_url=custom_base_url\n        )\n        assert summarizer._llm_client == mock_openai_client_instance",
        "file": "tests/test_summaries.py"
      },
      {
        "name": "test_get_llm_client_openai",
        "type": "function",
        "start_line": 143,
        "end_line": 158,
        "code": "def test_get_llm_client_openai(mock_openai_constructor, mock_repo):\n    \"\"\"Test _get_llm_client returns the client created in __init__.\"\"\"\n    mock_openai_instance = MagicMock()\n    mock_openai_constructor.return_value = mock_openai_instance\n    \n    config = OpenAIConfig(api_key=\"test_openai_key\")\n    with patch('openai.OpenAI', new=mock_openai_constructor):\n        summarizer = Summarizer(repo=mock_repo, config=config)\n        mock_openai_constructor.assert_called_once_with(api_key=\"test_openai_key\")\n        \n        client = summarizer._get_llm_client()\n        assert client is summarizer._llm_client\n\n    client2 = summarizer._get_llm_client()\n    mock_openai_constructor.assert_called_once() \n    assert client2 == client",
        "file": "tests/test_summaries.py"
      },
      {
        "name": "test_get_llm_client_openai_with_base_url_lazy_load",
        "type": "function",
        "start_line": 161,
        "end_line": 180,
        "code": "def test_get_llm_client_openai_with_base_url_lazy_load(mock_openai_lazy_constructor, mock_repo):\n    \"\"\"Test _get_llm_client lazy loads OpenAI client with base_url if not already initialized.\"\"\"\n    custom_api_key = \"test_lazy_key\"\n    custom_base_url = \"http://lazy_load_url.com/v1\"\n    config = OpenAIConfig(api_key=custom_api_key, base_url=custom_base_url)\n\n    with patch('openai.OpenAI', new=mock_openai_lazy_constructor) as patched_constructor_for_lazy:\n        summarizer = Summarizer(repo=mock_repo, config=config, llm_client=None)\n        patched_constructor_for_lazy.assert_called_once_with(api_key=custom_api_key, base_url=custom_base_url)\n        \n        summarizer._llm_client = None\n        mock_openai_lazy_constructor.reset_mock()\n\n        client = summarizer._get_llm_client()\n        \n        patched_constructor_for_lazy.assert_called_once_with(\n            api_key=custom_api_key,\n            base_url=custom_base_url\n        )\n        assert client is not None",
        "file": "tests/test_summaries.py"
      },
      {
        "name": "test_get_llm_client_anthropic",
        "type": "function",
        "start_line": 183,
        "end_line": 205,
        "code": "def test_get_llm_client_anthropic(mock_anthropic_constructor, mock_repo):\n    \"\"\"Test _get_llm_client returns the client created in __init__.\"\"\"\n    # Set up mock before creating Summarizer\n    mock_anthropic_instance = MagicMock()\n    mock_anthropic_constructor.return_value = mock_anthropic_instance\n    \n    # Patch the import to return our mock module\n    with patch('builtins.__import__', side_effect=lambda name, *args, **kwargs: \n              MagicMock(Anthropic=mock_anthropic_constructor) if name == 'anthropic' else __import__(name, *args, **kwargs)):\n        config = AnthropicConfig(api_key=\"test_anthropic_key\")\n        summarizer = Summarizer(repo=mock_repo, config=config)\n        \n        # The client should have been created in __init__\n        mock_anthropic_constructor.assert_called_once_with(api_key=\"test_anthropic_key\")\n        \n        # _get_llm_client should return the already created client\n        client = summarizer._get_llm_client()\n        assert client is summarizer._llm_client\n\n    # Call again to check caching\n    client2 = summarizer._get_llm_client()\n    mock_anthropic_constructor.assert_called_once() # Should not be called again\n    assert client2 == client",
        "file": "tests/test_summaries.py"
      },
      {
        "name": "test_get_llm_client_google",
        "type": "function",
        "start_line": 208,
        "end_line": 228,
        "code": "def test_get_llm_client_google(mock_google_client_constructor, mock_repo):\n    \"\"\"Test _get_llm_client returns and caches Google client.\"\"\"\n    if kit_s_genai is None:\n        pytest.skip(\"google.genai not available to kit.summaries\")\n\n    # Set up the mock before creating the Summarizer\n    mock_client_instance = MagicMock()\n    mock_google_client_constructor.return_value = mock_client_instance\n    \n    config = GoogleConfig(api_key=\"test_google_key\", model=\"gemini-test\")\n    summarizer = Summarizer(repo=mock_repo, config=config)\n\n    # First call: client should be created and cached\n    client1 = summarizer._get_llm_client()\n    assert client1 == mock_client_instance\n    mock_google_client_constructor.assert_called_once_with(api_key=\"test_google_key\")\n\n    # Second call: cached client should be returned\n    client2 = summarizer._get_llm_client()\n    assert client2 == mock_client_instance\n    mock_google_client_constructor.assert_called_once() # Still called only once",
        "file": "tests/test_summaries.py"
      },
      {
        "name": "test_summarize_file_openai",
        "type": "function",
        "start_line": 234,
        "end_line": 268,
        "code": "def test_summarize_file_openai(mock_openai_constructor, mock_repo, temp_code_file):\n    \"\"\"Test summarize_file with OpenAIConfig.\"\"\"\n    mock_file_content = \"# A simple Python script\\nprint('Hello, world!')\"\n    mock_repo.get_file_content.return_value = mock_file_content # Mock repo method\n\n    # Mock the OpenAI client and its response\n    mock_openai_client = MagicMock()\n    mock_response = MagicMock()\n    mock_response.choices[0].message.content = \"This is an OpenAI summary.\"\n    mock_openai_client.chat.completions.create.return_value = mock_response\n    mock_openai_constructor.return_value = mock_openai_client\n\n    config = OpenAIConfig(api_key=\"test_openai_key\", model=\"gpt-test\", temperature=0.5, max_tokens=100)\n    summarizer = Summarizer(repo=mock_repo, config=config)\n\n    file_to_summarize = \"sample_code.py\"\n    summary = summarizer.summarize_file(file_to_summarize)\n\n    mock_repo.get_abs_path.assert_called_once_with(file_to_summarize)\n    mock_repo.get_file_content.assert_called_once_with(f\"/abs/path/to/{file_to_summarize}\")\n\n    expected_system_prompt = \"You are an expert assistant skilled in creating concise and informative code summaries.\"\n    expected_user_prompt = f\"Summarize the following code from the file '{file_to_summarize}'. Provide a high-level overview of its purpose, key components, and functionality. Focus on what the code does, not just how it's written. The code is:\\n\\n```\\n{mock_file_content}\\n```\"\n\n    mock_openai_client.chat.completions.create.assert_called_once_with(\n        model=\"gpt-test\",\n        messages=[\n            {\"role\": \"system\", \"content\": expected_system_prompt},\n            {\"role\": \"user\", \"content\": expected_user_prompt}\n        ],\n        temperature=0.5,\n        max_tokens=100,\n    )\n\n    assert summary == \"This is an OpenAI summary.\"",
        "file": "tests/test_summaries.py"
      },
      {
        "name": "test_summarize_file_not_found",
        "type": "function",
        "start_line": 270,
        "end_line": 284,
        "code": "def test_summarize_file_not_found(mock_repo):\n    \"\"\"Test summarize_file raises FileNotFoundError if file does not exist.\"\"\"\n    abs_path_to_non_existent_file = \"/abs/path/to/non_existent.py\"\n    mock_repo.get_abs_path.return_value = abs_path_to_non_existent_file\n    mock_repo.get_file_content.side_effect = FileNotFoundError(f\"File not found: {abs_path_to_non_existent_file}\") # Actual error from get_file_content\n\n    summarizer = Summarizer(repo=mock_repo, config=OpenAIConfig(api_key=\"dummy\"))\n    \n    # The error message re-raised by summarize_file will include 'File not found via repo: '\n    expected_error_message = f\"File not found via repo: {abs_path_to_non_existent_file}\"\n    with pytest.raises(FileNotFoundError, match=re.escape(expected_error_message)):\n        summarizer.summarize_file(\"non_existent.py\")\n    \n    mock_repo.get_abs_path.assert_called_once_with(\"non_existent.py\")\n    mock_repo.get_file_content.assert_called_once_with(abs_path_to_non_existent_file)",
        "file": "tests/test_summaries.py"
      },
      {
        "name": "test_summarize_file_llm_error_empty_summary",
        "type": "function",
        "start_line": 287,
        "end_line": 299,
        "code": "def test_summarize_file_llm_error_empty_summary(mock_openai_constructor, mock_repo, temp_code_file):\n    \"\"\"Test summarize_file raises LLMError if LLM returns an empty summary.\"\"\"\n    mock_repo.get_file_content.return_value = \"def hello():\\n    print('Hello, world!')\"\n    mock_openai_client = MagicMock()\n    mock_response = MagicMock()\n    mock_response.choices[0].message.content = \"\" # Empty summary\n    mock_openai_client.chat.completions.create.return_value = mock_response\n    mock_openai_constructor.return_value = mock_openai_client\n\n    config = OpenAIConfig(api_key=\"test_key\")\n    summarizer = Summarizer(repo=mock_repo, config=config)\n    with pytest.raises(LLMError, match=\"LLM returned an empty summary.\"):\n        summarizer.summarize_file(temp_code_file)",
        "file": "tests/test_summaries.py"
      },
      {
        "name": "test_summarize_file_llm_api_error",
        "type": "function",
        "start_line": 302,
        "end_line": 312,
        "code": "def test_summarize_file_llm_api_error(mock_openai_constructor, mock_repo, temp_code_file):\n    \"\"\"Test summarize_file raises LLMError on API communication failure.\"\"\"\n    mock_repo.get_file_content.return_value = \"def hello():\\n    print('Hello, world!')\"\n    mock_openai_client = MagicMock()\n    mock_openai_client.chat.completions.create.side_effect = Exception(\"API Down\")\n    mock_openai_constructor.return_value = mock_openai_client\n\n    config = OpenAIConfig(api_key=\"test_key\")\n    summarizer = Summarizer(repo=mock_repo, config=config)\n    with pytest.raises(LLMError, match=\"Error communicating with LLM API: API Down\"):\n        summarizer.summarize_file(temp_code_file)",
        "file": "tests/test_summaries.py"
      },
      {
        "name": "test_summarize_file_anthropic",
        "type": "function",
        "start_line": 315,
        "end_line": 348,
        "code": "def test_summarize_file_anthropic(mock_anthropic_constructor, mock_repo, temp_code_file):\n    \"\"\"Test summarize_file with AnthropicConfig.\"\"\"\n    mock_file_content = \"# Another script for Anthropic\\nprint('Claude is neat!')\"\n    mock_repo.get_file_content.return_value = mock_file_content # Mock repo method\n\n    mock_anthropic_client = MagicMock()\n    mock_response = MagicMock()\n    mock_response.content[0].text = \"This is an Anthropic summary.\"\n    mock_anthropic_client.messages.create.return_value = mock_response\n    mock_anthropic_constructor.return_value = mock_anthropic_client\n\n    config = AnthropicConfig(api_key=\"test_anthropic_key\", model=\"claude-test\", temperature=0.6, max_tokens=150)\n    summarizer = Summarizer(repo=mock_repo, config=config)\n\n    file_to_summarize = \"sample_anthropic_code.py\"\n    summary = summarizer.summarize_file(file_to_summarize)\n\n    mock_repo.get_abs_path.assert_called_once_with(file_to_summarize)\n    mock_repo.get_file_content.assert_called_once_with(f\"/abs/path/to/{file_to_summarize}\")\n\n    expected_system_prompt = \"You are an expert assistant skilled in creating concise and informative code summaries.\"\n    expected_user_prompt = f\"Summarize the following code from the file '{file_to_summarize}'. Provide a high-level overview of its purpose, key components, and functionality. Focus on what the code does, not just how it's written. The code is:\\n\\n```\\n{mock_file_content}\\n```\"\n\n    mock_anthropic_client.messages.create.assert_called_once_with(\n        model=\"claude-test\",\n        system=expected_system_prompt,\n        messages=[\n            {\"role\": \"user\", \"content\": expected_user_prompt}\n        ],\n        temperature=0.6,\n        max_tokens=150,\n    )\n\n    assert summary == \"This is an Anthropic summary.\"",
        "file": "tests/test_summaries.py"
      },
      {
        "name": "test_summarize_file_google",
        "type": "function",
        "start_line": 351,
        "end_line": 389,
        "code": "def test_summarize_file_google(mock_google_client_constructor, mock_repo, temp_code_file):\n    \"\"\"Test summarize_file with GoogleConfig.\"\"\"\n    if kit_s_genai is None:\n        pytest.skip(\"google.genai not available to kit.summaries\")\n\n    mock_file_content = \"# A simple Python script\\nprint('Google AI is fun!')\"\n    # Ensure get_abs_path returns a consistent path\n    abs_path = f\"/abs/path/to/{temp_code_file}\"\n    mock_repo.get_abs_path.return_value = abs_path\n    mock_repo.get_file_content.return_value = mock_file_content\n    \n    mock_google_client_instance = MagicMock()\n    mock_response = MagicMock()\n    mock_response.text = \"This is a Google file summary.\"\n    mock_response.prompt_feedback = None # Assume no blocking for this test\n    mock_google_client_instance.models.generate_content.return_value = mock_response\n    mock_google_client_constructor.return_value = mock_google_client_instance\n\n    config = GoogleConfig(api_key=\"test_google_key\", model=\"gemini-file-test\", temperature=0.6, max_output_tokens=110)\n    summarizer = Summarizer(repo=mock_repo, config=config)\n    \n    summary = summarizer.summarize_file(temp_code_file)\n\n    mock_repo.get_abs_path.assert_called_once_with(temp_code_file)\n    mock_repo.get_file_content.assert_called_once_with(abs_path)\n    mock_google_client_constructor.assert_called_once_with(api_key=\"test_google_key\")\n\n    # The actual implementation uses this format for Google clients\n    expected_user_prompt = f\"Summarize the following code from the file '{temp_code_file}'. Provide a high-level overview of its purpose, key components, and functionality. Focus on what the code does, not just how it's written. The code is:\\n\\n```\\n{mock_file_content}\\n```\"\n    \n    expected_generation_params = {'temperature': 0.6, 'max_output_tokens': 110}\n\n    mock_google_client_instance.models.generate_content.assert_called_once_with(\n        model=\"gemini-file-test\",\n        contents=expected_user_prompt,\n        generation_config=expected_generation_params\n    )\n\n    assert summary == \"This is a Google file summary.\"",
        "file": "tests/test_summaries.py"
      },
      {
        "name": "test_summarize_function_openai",
        "type": "function",
        "start_line": 394,
        "end_line": 432,
        "code": "def test_summarize_function_openai(mock_openai_constructor, mock_repo):\n    \"\"\"Test summarize_function with OpenAIConfig.\"\"\"\n    mock_func_code = \"def my_func(a, b):\\n    return a + b\"\n    mock_repo.extract_symbols.return_value = [{ \n        \"name\": \"my_func\",\n        \"type\": \"FUNCTION\",\n        \"code\": mock_func_code\n    }]\n\n    # Mock the OpenAI client and its response\n    mock_openai_client = MagicMock()\n    mock_response = MagicMock()\n    mock_response.choices[0].message.content = \"This is an OpenAI function summary.\"\n    mock_openai_client.chat.completions.create.return_value = mock_response\n    mock_openai_constructor.return_value = mock_openai_client\n\n    config = OpenAIConfig(api_key=\"test_openai_key\", model=\"gpt-func-test\", temperature=0.4, max_tokens=90)\n    summarizer = Summarizer(repo=mock_repo, config=config)\n\n    file_path = \"src/module.py\"\n    func_name = \"my_func\"\n    summary = summarizer.summarize_function(file_path, func_name)\n\n    mock_repo.extract_symbols.assert_called_once_with(file_path)\n\n    expected_system_prompt = \"You are an expert assistant skilled in creating concise code summaries for functions.\"\n    expected_user_prompt = f\"Summarize the following function named '{func_name}' from the file '{file_path}'. Describe its purpose, parameters, and return value. The function definition is:\\n\\n```\\n{mock_func_code}\\n```\"\n\n    mock_openai_client.chat.completions.create.assert_called_once_with(\n        model=\"gpt-func-test\",\n        messages=[\n            {\"role\": \"system\", \"content\": expected_system_prompt},\n            {\"role\": \"user\", \"content\": expected_user_prompt}\n        ],\n        temperature=0.4,\n        max_tokens=90,\n    )\n\n    assert summary == \"This is an OpenAI function summary.\"",
        "file": "tests/test_summaries.py"
      },
      {
        "name": "test_summarize_function_not_found",
        "type": "function",
        "start_line": 434,
        "end_line": 440,
        "code": "def test_summarize_function_not_found(mock_repo):\n    \"\"\"Test summarize_function raises ValueError if function symbol is not found.\"\"\"\n    mock_repo.extract_symbols.return_value = [] # Simulate symbol not found\n    config = OpenAIConfig(api_key=\"test_key\") # Can use any config for this test\n    summarizer = Summarizer(repo=mock_repo, config=config)\n    with pytest.raises(ValueError, match=\"Could not find function 'non_existent_func' in 'some_file.py'.\"):\n        summarizer.summarize_function(\"some_file.py\", \"non_existent_func\")",
        "file": "tests/test_summaries.py"
      },
      {
        "name": "test_summarize_function_llm_error_empty_summary",
        "type": "function",
        "start_line": 443,
        "end_line": 459,
        "code": "def test_summarize_function_llm_error_empty_summary(mock_openai_constructor, mock_repo):\n    \"\"\"Test summarize_function raises LLMError if LLM returns an empty summary.\"\"\"\n    mock_repo.extract_symbols.return_value = [{ \n        \"name\": \"my_func_empty\",\n        \"type\": \"FUNCTION\",\n        \"code\": \"def f(): pass\"\n    }]\n    mock_openai_client = MagicMock()\n    mock_response = MagicMock()\n    mock_response.choices[0].message.content = \"\" # Empty summary\n    mock_openai_client.chat.completions.create.return_value = mock_response\n    mock_openai_constructor.return_value = mock_openai_client\n\n    config = OpenAIConfig(api_key=\"test_key\")\n    summarizer = Summarizer(repo=mock_repo, config=config)\n    with pytest.raises(LLMError, match=\"LLM returned an empty summary for function my_func_empty.\"):\n        summarizer.summarize_function(\"file.py\", \"my_func_empty\")",
        "file": "tests/test_summaries.py"
      },
      {
        "name": "test_summarize_function_llm_api_error",
        "type": "function",
        "start_line": 462,
        "end_line": 476,
        "code": "def test_summarize_function_llm_api_error(mock_openai_constructor, mock_repo):\n    \"\"\"Test summarize_function raises LLMError on API communication failure.\"\"\"\n    mock_repo.extract_symbols.return_value = [{ \n        \"name\": \"my_func_api_err\",\n        \"type\": \"FUNCTION\",\n        \"code\": \"def f(): pass\"\n    }]\n    mock_openai_client = MagicMock()\n    mock_openai_client.chat.completions.create.side_effect = Exception(\"API Error\")\n    mock_openai_constructor.return_value = mock_openai_client\n\n    config = OpenAIConfig(api_key=\"test_key\")\n    summarizer = Summarizer(repo=mock_repo, config=config)\n    with pytest.raises(LLMError, match=\"Error communicating with LLM API for function my_func_api_err: API Error\"):\n        summarizer.summarize_function(\"file.py\", \"my_func_api_err\")",
        "file": "tests/test_summaries.py"
      },
      {
        "name": "test_summarize_function_anthropic",
        "type": "function",
        "start_line": 479,
        "end_line": 516,
        "code": "def test_summarize_function_anthropic(mock_anthropic_constructor, mock_repo):\n    \"\"\"Test summarize_function with AnthropicConfig.\"\"\"\n    mock_func_code = \"def greet(name: str) -> str:\\n    return f'Hello, {name}'\"\n    mock_repo.extract_symbols.return_value = [{ \n        \"name\": \"greet\",\n        \"type\": \"FUNCTION\",\n        \"code\": mock_func_code\n    }]\n\n    mock_anthropic_client = MagicMock()\n    mock_response = MagicMock()\n    mock_response.content[0].text = \"This is an Anthropic function summary.\"\n    mock_anthropic_client.messages.create.return_value = mock_response\n    mock_anthropic_constructor.return_value = mock_anthropic_client\n\n    config = AnthropicConfig(api_key=\"test_anthropic_key\", model=\"claude-func-test\", temperature=0.5, max_tokens=100)\n    summarizer = Summarizer(repo=mock_repo, config=config)\n\n    file_path = \"src/greetings.py\"\n    func_name = \"greet\"\n    summary = summarizer.summarize_function(file_path, func_name)\n\n    mock_repo.extract_symbols.assert_called_once_with(file_path)\n\n    expected_system_prompt = \"You are an expert assistant skilled in creating concise code summaries for functions.\"\n    expected_user_prompt = f\"Summarize the following function named '{func_name}' from the file '{file_path}'. Describe its purpose, parameters, and return value. The function definition is:\\n\\n```\\n{mock_func_code}\\n```\"\n\n    mock_anthropic_client.messages.create.assert_called_once_with(\n        model=\"claude-func-test\",\n        system=expected_system_prompt,\n        messages=[\n            {\"role\": \"user\", \"content\": expected_user_prompt}\n        ],\n        temperature=0.5,\n        max_tokens=100,\n    )\n\n    assert summary == \"This is an Anthropic function summary.\"",
        "file": "tests/test_summaries.py"
      },
      {
        "name": "test_summarize_function_google",
        "type": "function",
        "start_line": 519,
        "end_line": 562,
        "code": "def test_summarize_function_google(mock_google_client_constructor, mock_repo):\n    \"\"\"Test summarize_function with GoogleConfig.\"\"\"\n    if kit_s_genai is None:\n        pytest.skip(\"google.genai not available to kit.summaries\")\n\n    mock_func_code = \"def calculate_sum(numbers: list[int]) -> int:\\n    return sum(numbers)\"\n    mock_repo.extract_symbols.return_value = [{ \n        \"name\": \"calculate_sum\",\n        \"type\": \"FUNCTION\",\n        \"code\": mock_func_code\n    }]\n\n    mock_google_client_instance = MagicMock()\n    mock_response = MagicMock()\n    mock_response.text = \"This is a Google function summary.\"\n    mock_response.prompt_feedback = None\n    mock_google_client_instance.models.generate_content.return_value = mock_response\n    mock_google_client_constructor.return_value = mock_google_client_instance\n\n    config = GoogleConfig(api_key=\"test_google_key\", model=\"gemini-func-test\", temperature=0.3, max_output_tokens=100)\n    summarizer = Summarizer(repo=mock_repo, config=config)\n\n    file_path = \"src/calculations.py\"\n    function_name = \"calculate_sum\"\n    summary = summarizer.summarize_function(file_path, function_name)\n\n    mock_repo.extract_symbols.assert_called_once_with(file_path)\n    mock_google_client_constructor.assert_called_once_with(api_key=\"test_google_key\")\n\n    # The actual implementation only uses the user prompt for Google client\n    expected_user_prompt = f\"Summarize the following function named '{function_name}' from the file '{file_path}'. Describe its purpose, parameters, and return value. The function definition is:\\n\\n```\\n{mock_func_code}\\n```\"\n\n    expected_generation_params = {\n        'temperature': 0.3,\n        'max_output_tokens': 100\n    }\n\n    mock_google_client_instance.models.generate_content.assert_called_once_with(\n        model=\"gemini-func-test\",\n        contents=expected_user_prompt,\n        generation_config=expected_generation_params\n    )\n\n    assert summary == \"This is a Google function summary.\"",
        "file": "tests/test_summaries.py"
      },
      {
        "name": "test_summarize_class_openai",
        "type": "function",
        "start_line": 568,
        "end_line": 606,
        "code": "def test_summarize_class_openai(mock_openai_constructor, mock_repo):\n    \"\"\"Test summarize_class with OpenAIConfig.\"\"\"\n    mock_class_code = \"class MyClass:\\n    def __init__(self, x):\\n        self.x = x\\n\\n    def get_x(self):\\n        return self.x\"\n    mock_repo.extract_symbols.return_value = [{ \n        \"name\": \"MyClass\",\n        \"type\": \"CLASS\",\n        \"code\": mock_class_code\n    }]\n\n    # Mock the OpenAI client and its response\n    mock_openai_client = MagicMock()\n    mock_response = MagicMock()\n    mock_response.choices[0].message.content = \"This is an OpenAI class summary.\"\n    mock_openai_client.chat.completions.create.return_value = mock_response\n    mock_openai_constructor.return_value = mock_openai_client\n\n    config = OpenAIConfig(api_key=\"test_openai_key\", model=\"gpt-class-test\", temperature=0.3, max_tokens=110)\n    summarizer = Summarizer(repo=mock_repo, config=config)\n\n    file_path = \"src/data_model.py\"\n    class_name = \"MyClass\"\n    summary = summarizer.summarize_class(file_path, class_name)\n\n    mock_repo.extract_symbols.assert_called_once_with(file_path)\n\n    expected_system_prompt = \"You are an expert assistant skilled in creating concise code summaries for classes.\"\n    expected_user_prompt = f\"Summarize the following class named '{class_name}' from the file '{file_path}'. Describe its purpose, key attributes, and main methods. The class definition is:\\n\\n```\\n{mock_class_code}\\n```\"\n\n    mock_openai_client.chat.completions.create.assert_called_once_with(\n        model=\"gpt-class-test\",\n        messages=[\n            {\"role\": \"system\", \"content\": expected_system_prompt},\n            {\"role\": \"user\", \"content\": expected_user_prompt}\n        ],\n        temperature=0.3,\n        max_tokens=110,\n    )\n\n    assert summary == \"This is an OpenAI class summary.\"",
        "file": "tests/test_summaries.py"
      },
      {
        "name": "test_summarize_class_not_found",
        "type": "function",
        "start_line": 608,
        "end_line": 614,
        "code": "def test_summarize_class_not_found(mock_repo):\n    \"\"\"Test summarize_class raises ValueError if class symbol is not found.\"\"\"\n    mock_repo.extract_symbols.return_value = [] # Simulate symbol not found\n    config = OpenAIConfig(api_key=\"test_key\") # Can use any config\n    summarizer = Summarizer(repo=mock_repo, config=config)\n    with pytest.raises(ValueError, match=\"Could not find class 'NonExistentClass' in 'another_file.py'.\"):\n        summarizer.summarize_class(\"another_file.py\", \"NonExistentClass\")",
        "file": "tests/test_summaries.py"
      },
      {
        "name": "test_summarize_class_llm_error_empty_summary",
        "type": "function",
        "start_line": 617,
        "end_line": 633,
        "code": "def test_summarize_class_llm_error_empty_summary(mock_openai_constructor, mock_repo):\n    \"\"\"Test summarize_class raises LLMError if LLM returns an empty summary.\"\"\"\n    mock_repo.extract_symbols.return_value = [{ \n        \"name\": \"MyClass_empty\",\n        \"type\": \"CLASS\",\n        \"code\": \"class C: pass\"\n    }]\n    mock_openai_client = MagicMock()\n    mock_response = MagicMock()\n    mock_response.choices[0].message.content = \"\" # Empty summary\n    mock_openai_client.chat.completions.create.return_value = mock_response\n    mock_openai_constructor.return_value = mock_openai_client\n\n    config = OpenAIConfig(api_key=\"test_key\")\n    summarizer = Summarizer(repo=mock_repo, config=config)\n    with pytest.raises(LLMError, match=\"LLM returned an empty summary for class MyClass_empty.\"):\n        summarizer.summarize_class(\"file.py\", \"MyClass_empty\")",
        "file": "tests/test_summaries.py"
      },
      {
        "name": "test_summarize_class_llm_api_error",
        "type": "function",
        "start_line": 636,
        "end_line": 650,
        "code": "def test_summarize_class_llm_api_error(mock_openai_constructor, mock_repo):\n    \"\"\"Test summarize_class raises LLMError on API communication failure.\"\"\"\n    mock_repo.extract_symbols.return_value = [{ \n        \"name\": \"MyClass_api_err\",\n        \"type\": \"CLASS\",\n        \"code\": \"class C: pass\"\n    }]\n    mock_openai_client = MagicMock()\n    mock_openai_client.chat.completions.create.side_effect = Exception(\"API Crash\")\n    mock_openai_constructor.return_value = mock_openai_client\n\n    config = OpenAIConfig(api_key=\"test_key\")\n    summarizer = Summarizer(repo=mock_repo, config=config)\n    with pytest.raises(LLMError, match=\"Error communicating with LLM API for class MyClass_api_err: API Crash\"):\n        summarizer.summarize_class(\"file.py\", \"MyClass_api_err\")",
        "file": "tests/test_summaries.py"
      },
      {
        "name": "test_summarize_class_anthropic",
        "type": "function",
        "start_line": 653,
        "end_line": 690,
        "code": "def test_summarize_class_anthropic(mock_anthropic_constructor, mock_repo):\n    \"\"\"Test summarize_class with AnthropicConfig.\"\"\"\n    mock_class_code = \"class DataProcessor:\\n    def __init__(self, data):\\n        self.data = data\\n\\n    def process(self):\\n        return len(self.data)\"\n    mock_repo.extract_symbols.return_value = [{ \n        \"name\": \"DataProcessor\",\n        \"type\": \"CLASS\",\n        \"code\": mock_class_code\n    }]\n\n    mock_anthropic_client = MagicMock()\n    mock_response = MagicMock()\n    mock_response.content[0].text = \"This is an Anthropic class summary.\"\n    mock_anthropic_client.messages.create.return_value = mock_response\n    mock_anthropic_constructor.return_value = mock_anthropic_client\n\n    config = AnthropicConfig(api_key=\"test_anthropic_key\", model=\"claude-class-test\", temperature=0.4, max_tokens=120)\n    summarizer = Summarizer(repo=mock_repo, config=config)\n\n    file_path = \"src/processing.py\"\n    class_name = \"DataProcessor\"\n    summary = summarizer.summarize_class(file_path, class_name)\n\n    mock_repo.extract_symbols.assert_called_once_with(file_path)\n\n    expected_system_prompt = \"You are an expert assistant skilled in creating concise code summaries for classes.\"\n    expected_user_prompt = f\"Summarize the following class named '{class_name}' from the file '{file_path}'. Describe its purpose, key attributes, and main methods. The class definition is:\\n\\n```\\n{mock_class_code}\\n```\"\n\n    mock_anthropic_client.messages.create.assert_called_once_with(\n        model=\"claude-class-test\",\n        system=expected_system_prompt,\n        messages=[\n            {\"role\": \"user\", \"content\": expected_user_prompt}\n        ],\n        temperature=0.4,\n        max_tokens=120,\n    )\n\n    assert summary == \"This is an Anthropic class summary.\"",
        "file": "tests/test_summaries.py"
      },
      {
        "name": "test_summarize_class_google",
        "type": "function",
        "start_line": 693,
        "end_line": 735,
        "code": "def test_summarize_class_google(mock_google_client_constructor, mock_repo):\n    \"\"\"Test summarize_class with GoogleConfig.\"\"\"\n    if kit_s_genai is None:\n        pytest.skip(\"google.genai not available to kit.summaries\")\n    mock_class_code = \"class Logger:\\n    def __init__(self, level='INFO'):\\n        self.level = level\\n\\n    def log(self, message):\\n        print(f'[{self.level}] {message}')\"\n    mock_repo.extract_symbols.return_value = [{ \n        \"name\": \"Logger\",\n        \"type\": \"CLASS\",\n        \"code\": mock_class_code\n    }]\n\n    mock_google_client_instance = MagicMock()\n    mock_response = MagicMock()\n    mock_response.text = \"This is a Google class summary.\"\n    mock_response.prompt_feedback = None\n    mock_google_client_instance.models.generate_content.return_value = mock_response\n    mock_google_client_constructor.return_value = mock_google_client_instance\n\n    config = GoogleConfig(api_key=\"test_google_key\", model=\"gemini-class-test\", temperature=0.5, max_output_tokens=130)\n    summarizer = Summarizer(repo=mock_repo, config=config)\n\n    file_path = \"src/utils.py\"\n    class_name = \"Logger\"\n    summary = summarizer.summarize_class(file_path, class_name)\n\n    mock_repo.extract_symbols.assert_called_once_with(file_path)\n    mock_google_client_constructor.assert_called_once_with(api_key=\"test_google_key\")\n\n    # The actual implementation only uses the user prompt for Google client\n    expected_user_prompt = f\"Summarize the following class named '{class_name}' from the file '{file_path}'. Describe its purpose, key attributes, and main methods. The class definition is:\\n\\n```\\n{mock_class_code}\\n```\"\n\n    expected_generation_params = {\n        'temperature': 0.5,\n        'max_output_tokens': 130\n    }\n\n    mock_google_client_instance.models.generate_content.assert_called_once_with(\n        model=\"gemini-class-test\",\n        contents=expected_user_prompt,\n        generation_config=expected_generation_params\n    )\n\n    assert summary == \"This is a Google class summary.\"",
        "file": "tests/test_summaries.py"
      }
    ],
    "tests/test_ruby_c_symbols.py": [
      {
        "name": "_extract",
        "type": "function",
        "start_line": 7,
        "end_line": 11,
        "code": "def _extract(tmpdir: str, filename: str, content: str):\n    path = os.path.join(tmpdir, filename)\n    with open(path, \"w\", encoding=\"utf-8\") as f:\n        f.write(content)\n    return Repository(tmpdir).extract_symbols(filename)",
        "file": "tests/test_ruby_c_symbols.py"
      },
      {
        "name": "test_ruby_symbols",
        "type": "function",
        "start_line": 14,
        "end_line": 27,
        "code": "def test_ruby_symbols():\n    code = \"\"\"\nclass Foo; end\nmodule Bar; end\n\ndef baz; end\nclass Foo\n  def qux; end\nend\n\"\"\"\n    with tempfile.TemporaryDirectory() as tmpdir:\n        symbols = _extract(tmpdir, \"main.rb\", code)\n        names = {s[\"name\"] for s in symbols}\n        assert {\"Foo\", \"Bar\", \"baz\", \"qux\"}.issubset(names)",
        "file": "tests/test_ruby_c_symbols.py"
      },
      {
        "name": "test_c_symbols",
        "type": "function",
        "start_line": 30,
        "end_line": 41,
        "code": "def test_c_symbols():\n    code = \"\"\"\nstruct Person { int age; };\n\nenum Color { RED, GREEN };\n\nint add(int a,int b){ return a+b; }\n\"\"\"\n    with tempfile.TemporaryDirectory() as tmpdir:\n        symbols = _extract(tmpdir, \"main.c\", code)\n        names = {s[\"name\"] for s in symbols}\n        assert {\"Person\", \"Color\", \"add\"}.issubset(names)",
        "file": "tests/test_ruby_c_symbols.py"
      }
    ],
    "tests/test_summarizer.py": [
      {
        "name": "FakeRepo",
        "type": "class",
        "start_line": 5,
        "end_line": 19,
        "code": "class FakeRepo:\n    \"\"\"Minimal fake Repository with in-memory file storage.\"\"\"\n\n    def __init__(self, files, local_path_str: str = \".\"):\n        self._files = files\n        self.local_path = Path(local_path_str)\n\n    def get_file_content(self, path: str) -> str:\n        if path not in self._files:\n            raise FileNotFoundError(path)\n        return self._files[path]\n\n    def get_abs_path(self, path: str) -> str:\n        # For FakeRepo, paths are already \"absolute\" in its context or not relevant.\n        return path",
        "file": "tests/test_summarizer.py"
      },
      {
        "name": "__init__",
        "type": "method",
        "start_line": 8,
        "end_line": 10,
        "code": "def __init__(self, files, local_path_str: str = \".\"):\n        self._files = files\n        self.local_path = Path(local_path_str)",
        "file": "tests/test_summarizer.py"
      },
      {
        "name": "get_file_content",
        "type": "method",
        "start_line": 12,
        "end_line": 15,
        "code": "def get_file_content(self, path: str) -> str:\n        if path not in self._files:\n            raise FileNotFoundError(path)\n        return self._files[path]",
        "file": "tests/test_summarizer.py"
      },
      {
        "name": "get_abs_path",
        "type": "method",
        "start_line": 17,
        "end_line": 19,
        "code": "def get_abs_path(self, path: str) -> str:\n        # For FakeRepo, paths are already \"absolute\" in its context or not relevant.\n        return path",
        "file": "tests/test_summarizer.py"
      },
      {
        "name": "_FakeCompletion",
        "type": "class",
        "start_line": 24,
        "end_line": 28,
        "code": "class _FakeCompletion:\n    def __init__(self, content: str):\n        # Mimic OpenAI response object shape we access in Summarizer\n        self.choices = [type(\"_Choice\", (), {\"message\": type(\"_Msg\", (), {\"content\": content})()})]\n        self.usage = None",
        "file": "tests/test_summarizer.py"
      },
      {
        "name": "__init__",
        "type": "method",
        "start_line": 25,
        "end_line": 28,
        "code": "def __init__(self, content: str):\n        # Mimic OpenAI response object shape we access in Summarizer\n        self.choices = [type(\"_Choice\", (), {\"message\": type(\"_Msg\", (), {\"content\": content})()})]\n        self.usage = None",
        "file": "tests/test_summarizer.py"
      },
      {
        "name": "_FakeChatCompletions",
        "type": "class",
        "start_line": 30,
        "end_line": 38,
        "code": "class _FakeChatCompletions:\n    def __init__(self, content: str, raise_exc: bool = False):\n        self._content = content\n        self._raise = raise_exc\n\n    def create(self, *args, **kwargs):  # noqa: D401\n        if self._raise:\n            raise RuntimeError(\"API down\")\n        return _FakeCompletion(self._content)",
        "file": "tests/test_summarizer.py"
      },
      {
        "name": "__init__",
        "type": "method",
        "start_line": 31,
        "end_line": 33,
        "code": "def __init__(self, content: str, raise_exc: bool = False):\n        self._content = content\n        self._raise = raise_exc",
        "file": "tests/test_summarizer.py"
      },
      {
        "name": "create",
        "type": "method",
        "start_line": 35,
        "end_line": 38,
        "code": "def create(self, *args, **kwargs):  # noqa: D401\n        if self._raise:\n            raise RuntimeError(\"API down\")\n        return _FakeCompletion(self._content)",
        "file": "tests/test_summarizer.py"
      },
      {
        "name": "FakeOpenAI",
        "type": "class",
        "start_line": 41,
        "end_line": 45,
        "code": "class FakeOpenAI:\n    \"\"\"Mimics the parts of openai.OpenAI used by Summarizer.\"\"\"\n\n    def __init__(self, summary: str = \"Fake summary\", raise_exc: bool = False):\n        self.chat = type(\"_Chat\", (), {\"completions\": _FakeChatCompletions(summary, raise_exc)})()",
        "file": "tests/test_summarizer.py"
      },
      {
        "name": "__init__",
        "type": "method",
        "start_line": 44,
        "end_line": 45,
        "code": "def __init__(self, summary: str = \"Fake summary\", raise_exc: bool = False):\n        self.chat = type(\"_Chat\", (), {\"completions\": _FakeChatCompletions(summary, raise_exc)})()",
        "file": "tests/test_summarizer.py"
      },
      {
        "name": "test_summarize_file_happy",
        "type": "function",
        "start_line": 51,
        "end_line": 56,
        "code": "def test_summarize_file_happy():\n    repo = FakeRepo({\"foo.py\": \"print('hello')\"})\n    client = FakeOpenAI(\"This file prints hello\")\n    summarizer = Summarizer(repo, llm_client=client)\n    summary = summarizer.summarize_file(\"foo.py\")\n    assert summary == \"This file prints hello\"",
        "file": "tests/test_summarizer.py"
      },
      {
        "name": "test_summarize_file_not_found",
        "type": "function",
        "start_line": 59,
        "end_line": 63,
        "code": "def test_summarize_file_not_found():\n    repo = FakeRepo({})\n    summarizer = Summarizer(repo, llm_client=FakeOpenAI())\n    with pytest.raises(FileNotFoundError):\n        summarizer.summarize_file(\"missing.py\")",
        "file": "tests/test_summarizer.py"
      },
      {
        "name": "test_summarize_llm_error",
        "type": "function",
        "start_line": 66,
        "end_line": 71,
        "code": "def test_summarize_llm_error():\n    repo = FakeRepo({\"bar.py\": \"print('x')\"})\n    error_client = FakeOpenAI(raise_exc=True)\n    summarizer = Summarizer(repo, llm_client=error_client)\n    with pytest.raises(LLMError):\n        summarizer.summarize_file(\"bar.py\")",
        "file": "tests/test_summarizer.py"
      }
    ],
    "tests/test_hcl_symbols.py": [
      {
        "name": "test_hcl_symbol_extraction",
        "type": "function",
        "start_line": 6,
        "end_line": 78,
        "code": "def test_hcl_symbol_extraction():\n    hcl_content = '''\nprovider \"aws\" {\n  region = \"us-west-2\"\n}\n\nresource \"aws_instance\" \"web\" {\n  ami           = \"ami-0c55b159cbfa1f0\"\n  instance_type = \"t2.micro\"\n  tags = {\n    Name = \"WebServer\"\n  }\n}\n\nresource \"aws_s3_bucket\" \"bucket\" {\n  bucket = \"my-example-bucket-123456\"\n  acl    = \"private\"\n}\n\nvariable \"instance_count\" {\n  description = \"Number of EC2 instances to launch\"\n  type        = number\n  default     = 2\n}\n\noutput \"instance_id\" {\n  value = aws_instance.web.id\n}\n\nlocals {\n  environment = \"dev\"\n  owner       = \"test-user\"\n}\n\nmodule \"vpc\" {\n  source = \"terraform-aws-modules/vpc/aws\"\n  name   = \"example-vpc\"\n  cidr   = \"10.0.0.0/16\"\n}\n'''\n    with tempfile.TemporaryDirectory() as tmpdir:\n        hcl_path = os.path.join(tmpdir, \"main.tf\")\n        with open(hcl_path, \"w\") as f:\n            f.write(hcl_content)\n        repository = Repository(tmpdir)\n        symbols = repository.extract_symbols(\"main.tf\")\n        types = {s[\"type\"] for s in symbols}\n        names = {s[\"name\"] for s in symbols if \"name\" in s}\n\n        # Expected symbols based on HCL query and updated extractor logic\n        expected = {\n            \"aws\",                      # provider \"aws\"\n            \"aws_instance.web\",         # resource \"aws_instance\" \"web\"\n            \"aws_s3_bucket.bucket\",     # resource \"aws_s3_bucket\" \"bucket\"\n            \"instance_count\",           # variable \"instance_count\"\n            \"instance_id\",             # output \"instance_id\"\n            \"vpc\",                     # module \"vpc\"\n            \"locals\",                  # locals block\n            # Note: no terraform block in this fixture\n        }\n\n        # Assert individual expected symbols exist\n        for name in expected:\n            assert name in names, f\"Expected name {name} not found in {names}\"\n\n        # Check types for resource blocks (should be unquoted resource type)\n        resource_types = {s[\"subtype\"] for s in symbols if s[\"type\"] == \"resource\" and \"subtype\" in s}\n        assert \"aws_instance\" in resource_types\n        assert \"aws_s3_bucket\" in resource_types\n\n        # Check for provider and locals types\n        assert \"provider\" in types\n        assert \"locals\" in types",
        "file": "tests/test_hcl_symbols.py"
      },
      {
        "name": "test_hcl_symbol_edge_cases",
        "type": "function",
        "start_line": 80,
        "end_line": 114,
        "code": "def test_hcl_symbol_edge_cases():\n    hcl_content = '''\nresource \"aws_security_group\" \"sg\" {\n  name        = \"allow_tls\"\n  description = \"Allow TLS inbound traffic\"\n}\n\nresource \"aws_lb_listener\" \"listener\" {\n  port     = 443\n  protocol = \"HTTPS\"\n}\n\nterraform {\n  required_providers {\n    aws = {\n      source  = \"hashicorp/aws\"\n      version = \"~> 2.0\"\n    }\n  }\n}\n'''\n    with tempfile.TemporaryDirectory() as tmpdir:\n        hcl_path = os.path.join(tmpdir, \"main.tf\")\n        with open(hcl_path, \"w\") as f:\n            f.write(hcl_content)\n        repository = Repository(tmpdir)\n        symbols = repository.extract_symbols(\"main.tf\")\n        types = {s[\"type\"] for s in symbols}\n        subtypes = {s[\"subtype\"] for s in symbols if \"subtype\" in s}\n        names = {s[\"name\"] for s in symbols if \"name\" in s}\n        # Should include the unnamed terraform block\n        assert \"terraform\" in types or \"block\" in types\n        # Should include specific resource subtypes (unquoted)\n        assert \"aws_security_group\" in subtypes\n        assert \"aws_lb_listener\" in subtypes",
        "file": "tests/test_hcl_symbols.py"
      }
    ],
    "tests/test_cross_file_impact.py": [
      {
        "name": "setup_test_repo",
        "type": "function",
        "start_line": 21,
        "end_line": 26,
        "code": "def setup_test_repo():\n    tmpdir = tempfile.mkdtemp()\n    for fname, content in TEST_FILES.items():\n        with open(os.path.join(tmpdir, fname), \"w\") as f:\n            f.write(content)\n    return tmpdir",
        "file": "tests/test_cross_file_impact.py"
      },
      {
        "name": "test_find_symbol_usages",
        "type": "function",
        "start_line": 28,
        "end_line": 42,
        "code": "def test_find_symbol_usages():\n    repo_dir = setup_test_repo()\n    try:\n        repository = Repository(repo_dir)\n        usages = repository.find_symbol_usages(\"foo\", symbol_type=\"function\")\n        usage_files = sorted(set(u[\"file\"].split(os.sep)[-1] for u in usages))\n        assert \"a.py\" in usage_files\n        assert \"b.py\" in usage_files\n        # Should find both the definition and calls/imports\n        found_types = set(u.get(\"type\") for u in usages if u.get(\"type\"))\n        assert \"function\" in found_types\n        # Should find at least one usage with context containing 'foo()'\n        assert any(\"foo()\" in (u.get(\"context\") or \"\") for u in usages)\n    finally:\n        shutil.rmtree(repo_dir)",
        "file": "tests/test_cross_file_impact.py"
      }
    ],
    "tests/test_vector_searcher.py": [
      {
        "name": "_reset_chroma_system",
        "type": "function",
        "start_line": 11,
        "end_line": 13,
        "code": "def _reset_chroma_system():\n    yield\n    _ssc.SharedSystemClient._identifier_to_system.clear()",
        "file": "tests/test_vector_searcher.py"
      },
      {
        "name": "dummy_embed",
        "type": "function",
        "start_line": 15,
        "end_line": 17,
        "code": "def dummy_embed(text):\n    # Simple deterministic embedding for testing (sum of char codes)\n    return [sum(ord(c) for c in text) % 1000]",
        "file": "tests/test_vector_searcher.py"
      },
      {
        "name": "test_vector_searcher_build_and_query",
        "type": "function",
        "start_line": 19,
        "end_line": 36,
        "code": "def test_vector_searcher_build_and_query():\n    with tempfile.TemporaryDirectory() as tmpdir:\n        # Create a simple file\n        fpath = os.path.join(tmpdir, \"a.py\")\n        with open(fpath, \"w\") as f:\n            f.write(\"\"\"\ndef foo(): pass\nclass Bar: pass\n\"\"\")\n        repository = Repository(tmpdir)\n        vs = VectorSearcher(repository, embed_fn=dummy_embed)\n        vs.build_index(chunk_by=\"symbols\")\n        results = vs.search(\"foo\", top_k=2)\n        assert isinstance(results, list)\n        assert any(\"foo\" in (r.get(\"name\") or \"\") for r in results)\n        # Test search_semantic via Repository\n        results2 = repository.search_semantic(\"Bar\", embed_fn=dummy_embed)\n        assert any(\"Bar\" in (r.get(\"name\") or \"\") for r in results2)",
        "file": "tests/test_vector_searcher.py"
      },
      {
        "name": "test_vector_searcher_multiple_files",
        "type": "function",
        "start_line": 38,
        "end_line": 60,
        "code": "def test_vector_searcher_multiple_files():\n    with tempfile.TemporaryDirectory() as tmpdir:\n        files = [\n            (\"a.py\", \"def foo(): pass\\nclass Bar: pass\\n\"),\n            (\"b.py\", \"def baz(): pass\\n# just a comment\\n\"),\n            (\"empty.py\", \"\\n\"),\n            (\"unicode.py\", \"def \u00fcnicode(): pass\\n\"),\n        ]\n        for fname, content in files:\n            with open(os.path.join(tmpdir, fname), \"w\", encoding=\"utf-8\") as f:\n                f.write(content)\n        repository = Repository(tmpdir)\n        vs = VectorSearcher(repository, embed_fn=dummy_embed)\n        vs.build_index(chunk_by=\"symbols\")\n        # Should find foo, Bar, baz, \u00fcnicode\n        results = vs.search(\"foo\", top_k=10)\n        assert any(\"foo\" in (r.get(\"name\") or \"\") for r in results)\n        results = vs.search(\"baz\", top_k=10)\n        assert any(\"baz\" in (r.get(\"name\") or \"\") for r in results)\n        results = vs.search(\"Bar\", top_k=10)\n        assert any(\"Bar\" in (r.get(\"name\") or \"\") for r in results)\n        results = vs.search(\"\u00fcnicode\", top_k=10)\n        assert any(\"\u00fcnicode\" in (r.get(\"name\") or \"\") for r in results)",
        "file": "tests/test_vector_searcher.py"
      },
      {
        "name": "test_vector_searcher_empty_and_comment_files",
        "type": "function",
        "start_line": 62,
        "end_line": 73,
        "code": "def test_vector_searcher_empty_and_comment_files():\n    with tempfile.TemporaryDirectory() as tmpdir:\n        with open(os.path.join(tmpdir, \"c.py\"), \"w\") as f:\n            f.write(\"# just a comment\\n\\n\")\n        with open(os.path.join(tmpdir, \"d.py\"), \"w\") as f:\n            f.write(\"\")\n        repository = Repository(tmpdir)\n        vs = VectorSearcher(repository, embed_fn=dummy_embed)\n        vs.build_index(chunk_by=\"symbols\")\n        # Should not crash or index anything meaningful\n        results = vs.search(\"anything\", top_k=5)\n        assert isinstance(results, list)",
        "file": "tests/test_vector_searcher.py"
      },
      {
        "name": "test_vector_searcher_chunk_by_lines",
        "type": "function",
        "start_line": 75,
        "end_line": 83,
        "code": "def test_vector_searcher_chunk_by_lines():\n    with tempfile.TemporaryDirectory() as tmpdir:\n        with open(os.path.join(tmpdir, \"e.py\"), \"w\") as f:\n            f.write(\"\\n\".join([f\"def f{i}(): pass\" for i in range(100)]))\n        repository = Repository(tmpdir)\n        vs = VectorSearcher(repository, embed_fn=dummy_embed)\n        vs.build_index(chunk_by=\"lines\")\n        results = vs.search(\"f42\", top_k=10)\n        assert isinstance(results, list)",
        "file": "tests/test_vector_searcher.py"
      },
      {
        "name": "test_vector_searcher_search_nonexistent",
        "type": "function",
        "start_line": 85,
        "end_line": 94,
        "code": "def test_vector_searcher_search_nonexistent():\n    with tempfile.TemporaryDirectory() as tmpdir:\n        with open(os.path.join(tmpdir, \"f.py\"), \"w\") as f:\n            f.write(\"def hello(): pass\\n\")\n        repository = Repository(tmpdir)\n        vs = VectorSearcher(repository, embed_fn=dummy_embed)\n        vs.build_index()\n        results = vs.search(\"nonexistent\", top_k=5)\n        assert isinstance(results, list)\n        assert all(\"nonexistent\" not in (r.get(\"name\") or \"\") for r in results)",
        "file": "tests/test_vector_searcher.py"
      },
      {
        "name": "test_vector_searcher_top_k_bounds",
        "type": "function",
        "start_line": 96,
        "end_line": 106,
        "code": "def test_vector_searcher_top_k_bounds():\n    with tempfile.TemporaryDirectory() as tmpdir:\n        with open(os.path.join(tmpdir, \"g.py\"), \"w\") as f:\n            f.write(\"def a(): pass\\ndef b(): pass\\n\")\n        repository = Repository(tmpdir)\n        vs = VectorSearcher(repository, embed_fn=dummy_embed)\n        vs.build_index()\n        results = vs.search(\"a\", top_k=10)\n        assert len(results) <= 10\n        results_zero = vs.search(\"a\", top_k=0)\n        assert results_zero == [] or len(results_zero) == 0",
        "file": "tests/test_vector_searcher.py"
      },
      {
        "name": "test_vector_searcher_edge_case_queries",
        "type": "function",
        "start_line": 108,
        "end_line": 116,
        "code": "def test_vector_searcher_edge_case_queries():\n    with tempfile.TemporaryDirectory() as tmpdir:\n        with open(os.path.join(tmpdir, \"h.py\"), \"w\") as f:\n            f.write(\"def edgecase(): pass\\n\")\n        repository = Repository(tmpdir)\n        vs = VectorSearcher(repository, embed_fn=dummy_embed)\n        vs.build_index()\n        assert vs.search(\"\", top_k=5) == [] or isinstance(vs.search(\"\", top_k=5), list)\n        assert isinstance(vs.search(\"$%^&*\", top_k=5), list)",
        "file": "tests/test_vector_searcher.py"
      },
      {
        "name": "test_vector_searcher_identical_embeddings",
        "type": "function",
        "start_line": 118,
        "end_line": 129,
        "code": "def test_vector_searcher_identical_embeddings():\n    def constant_embed(text):\n        return [42]\n    with tempfile.TemporaryDirectory() as tmpdir:\n        for i in range(3):\n            with open(os.path.join(tmpdir, f\"i{i}.py\"), \"w\") as f:\n                f.write(f\"def func{i}(): pass\\n\")\n        repository = Repository(tmpdir)\n        vs = VectorSearcher(repository, embed_fn=constant_embed)\n        vs.build_index()\n        results = vs.search(\"anything\", top_k=5)\n        assert len(results) == 3",
        "file": "tests/test_vector_searcher.py"
      },
      {
        "name": "test_vector_searcher_missing_embed_fn",
        "type": "function",
        "start_line": 131,
        "end_line": 137,
        "code": "def test_vector_searcher_missing_embed_fn():\n    with tempfile.TemporaryDirectory() as tmpdir:\n        with open(os.path.join(tmpdir, \"j.py\"), \"w\") as f:\n            f.write(\"def missing(): pass\\n\")\n        repository = Repository(tmpdir)\n        with pytest.raises(ValueError):\n            repository.get_vector_searcher()",
        "file": "tests/test_vector_searcher.py"
      },
      {
        "name": "test_vector_searcher_persistency",
        "type": "function",
        "start_line": 139,
        "end_line": 150,
        "code": "def test_vector_searcher_persistency():\n    with tempfile.TemporaryDirectory() as tmpdir:\n        fpath = os.path.join(tmpdir, \"k.py\")\n        with open(fpath, \"w\") as f:\n            f.write(\"def persist(): pass\\n\")\n        repository = Repository(tmpdir)\n        vs = VectorSearcher(repository, embed_fn=dummy_embed)\n        vs.build_index()\n        # Simulate restart by creating new VectorSearcher with same persist_dir and backend\n        new_vs = VectorSearcher(repository, embed_fn=dummy_embed, persist_dir=vs.persist_dir, backend=vs.backend)\n        results = new_vs.search(\"persist\", top_k=2)\n        assert any(\"persist\" in (r.get(\"name\") or \"\") for r in results)",
        "file": "tests/test_vector_searcher.py"
      },
      {
        "name": "test_vector_searcher_overwrite_index",
        "type": "function",
        "start_line": 152,
        "end_line": 164,
        "code": "def test_vector_searcher_overwrite_index():\n    with tempfile.TemporaryDirectory() as tmpdir:\n        fpath = os.path.join(tmpdir, \"l.py\")\n        with open(fpath, \"w\") as f:\n            f.write(\"def first(): pass\\n\")\n        repository = Repository(tmpdir)\n        vs = VectorSearcher(repository, embed_fn=dummy_embed)\n        vs.build_index()\n        with open(fpath, \"a\") as f:\n            f.write(\"def second(): pass\\n\")\n        vs.build_index()\n        results = vs.search(\"second\", top_k=2)\n        assert any(\"second\" in (r.get(\"name\") or \"\") for r in results)",
        "file": "tests/test_vector_searcher.py"
      },
      {
        "name": "test_vector_searcher_similar_queries",
        "type": "function",
        "start_line": 166,
        "end_line": 175,
        "code": "def test_vector_searcher_similar_queries():\n    with tempfile.TemporaryDirectory() as tmpdir:\n        with open(os.path.join(tmpdir, \"m.py\"), \"w\") as f:\n            f.write(\"def hello(): pass\\ndef hell(): pass\\n\")\n        repository = Repository(tmpdir)\n        vs = VectorSearcher(repository, embed_fn=dummy_embed)\n        vs.build_index()\n        results = vs.search(\"hell\", top_k=2)\n        assert any(\"hell\" in (r.get(\"name\") or \"\") for r in results)\n        assert any(\"hello\" in (r.get(\"name\") or \"\") for r in results)",
        "file": "tests/test_vector_searcher.py"
      },
      {
        "name": "is_sentence_transformer_unavailable",
        "type": "function",
        "start_line": 181,
        "end_line": 194,
        "code": "def is_sentence_transformer_unavailable() -> bool:  # helper for skipif\n    \"\"\"Return True if SentenceTransformer or model cannot be imported/loaded.\"\"\"\n    try:\n        from sentence_transformers import SentenceTransformer  # noqa: WPS433 (third-party import inside function is fine)\n        print(\"DEBUG: sentence_transformers imported OK\")\n        # Don't attempt to download model here; just check import.\n        # Actual load will happen inside the test body where we can handle exceptions.\n        return False\n    except ImportError as err:\n        print(f\"DEBUG: SentenceTransformer ImportError \u2192 skipping test: {err}\")\n        return True\n    except Exception as err:  # pragma: no cover \u2013 other unexpected issues\n        print(f\"DEBUG: Unexpected error during SentenceTransformer check \u2192 skipping: {err}\")\n        return True",
        "file": "tests/test_vector_searcher.py"
      },
      {
        "name": "test_vector_searcher_with_sentence_transformer",
        "type": "function",
        "start_line": 200,
        "end_line": 261,
        "code": "def test_vector_searcher_with_sentence_transformer():\n    \"\"\"End-to-end semantic search using a real embedding model (if available).\"\"\"\n    from sentence_transformers import SentenceTransformer  # type: ignore\n\n    model = SentenceTransformer(MODEL_NAME)\n\n    def st_embed_fn(text: str) -> list[float]:\n        return model.encode([text])[0].tolist()\n\n    with tempfile.TemporaryDirectory() as tmpdir_st:\n        repo_path = Path(tmpdir_st)\n        file1_content = \"\"\"\n        def calculate_area_of_circle(radius):\n            pi = 3.14159\n            return pi * (radius ** 2)\n        \"\"\"\n        file2_content = \"\"\"\n        class UserLogin:\n            def __init__(self, username, password):\n                self.username = username\n                self.password = password\n\n            def authenticate(self):\n                # Complex authentication logic here\n                print(f\"Authenticating {self.username}\")\n                return True\n        \"\"\"\n        (repo_path / \"geometry.py\").write_text(file1_content)\n        (repo_path / \"auth.py\").write_text(file2_content)\n\n        repository = Repository(str(repo_path))\n        # Use a unique persist_dir for this test to avoid conflicts\n        persist_path = repo_path / \".kit_test_st_index\"\n        vs = VectorSearcher(repository, embed_fn=st_embed_fn, persist_dir=str(persist_path))\n        vs.build_index(chunk_by=\"symbols\") # Chunking by symbols is often good for semantic code search\n\n        # Query for something related to \"circle area calculation\"\n        query1 = \"mathematical function for disk size\"\n        results1 = vs.search(query1, top_k=1)\n\n        assert len(results1) >= 1, \"Should find at least one result for query 1\"\n        # Check if the top result's metadata (which includes the code) contains relevant terms\n        # The 'text' field in metadata should be the chunk of code\n        top_result1_text = results1[0].get('code', '')\n        assert \"calculate_area_of_circle\" in top_result1_text or \"radius\" in top_result1_text, \\\n            f\"Top result for '{query1}' did not contain expected geometry code. Got: {top_result1_text}\"\n\n        # Query for something related to \"user sign-in\"\n        query2 = \"process for verifying user credentials\"\n        results2 = vs.search(query2, top_k=1)\n\n        assert len(results2) >= 1, \"Should find at least one result for query 2\"\n        top_result2_text = results2[0].get('code', '')\n        assert \"UserLogin\" in top_result2_text or \"authenticate\" in top_result2_text, \\\n            f\"Top result for '{query2}' did not contain expected auth code. Got: {top_result2_text}\"\n\n        # Test persistence: create a new searcher instance pointing to the same directory\n        vs_persistent = VectorSearcher(repository, embed_fn=st_embed_fn, persist_dir=str(persist_path))\n        results_persistent = vs_persistent.search(query1, top_k=1) # embed_fn might be needed if query embedding is not part of backend state\n        assert len(results_persistent) >= 1\n        top_result_persistent_text = results_persistent[0].get('code', '')\n        assert \"calculate_area_of_circle\" in top_result_persistent_text or \"radius\" in top_result_persistent_text",
        "file": "tests/test_vector_searcher.py"
      }
    ],
    "tests/golden_python.py": [
      {
        "name": "top_level_function",
        "type": "function",
        "start_line": 2,
        "end_line": 4,
        "code": "def top_level_function(arg1, arg2):\n    \"\"\"A regular function.\"\"\"\n    pass",
        "file": "tests/golden_python.py"
      },
      {
        "name": "MyClass",
        "type": "class",
        "start_line": 6,
        "end_line": 13,
        "code": "class MyClass:\n    \"\"\"A sample class.\"\"\"\n    def __init__(self, value):\n        self.value = value\n\n    def method_one(self, param):\n        \"\"\"A method within the class.\"\"\"\n        return self.value + param",
        "file": "tests/golden_python.py"
      },
      {
        "name": "__init__",
        "type": "method",
        "start_line": 8,
        "end_line": 9,
        "code": "def __init__(self, value):\n        self.value = value",
        "file": "tests/golden_python.py"
      },
      {
        "name": "method_one",
        "type": "method",
        "start_line": 11,
        "end_line": 13,
        "code": "def method_one(self, param):\n        \"\"\"A method within the class.\"\"\"\n        return self.value + param",
        "file": "tests/golden_python.py"
      },
      {
        "name": "async_function",
        "type": "function",
        "start_line": 15,
        "end_line": 17,
        "code": "async def async_function():\n    \"\"\"An asynchronous function.\"\"\"\n    await asyncio.sleep(1)",
        "file": "tests/golden_python.py"
      },
      {
        "name": "async_function",
        "type": "function",
        "start_line": 15,
        "end_line": 17,
        "code": "async def async_function():\n    \"\"\"An asynchronous function.\"\"\"\n    await asyncio.sleep(1)",
        "file": "tests/golden_python.py"
      }
    ],
    "tests/test_golden_symbols.py": [
      {
        "name": "run_extraction",
        "type": "function",
        "start_line": 7,
        "end_line": 12,
        "code": "def run_extraction(tmpdir, filename, content):\n    path = os.path.join(tmpdir, filename)\n    with open(path, \"w\") as f:\n        f.write(content)\n    repository = Repository(tmpdir)\n    return repository.extract_symbols(filename)",
        "file": "tests/test_golden_symbols.py"
      },
      {
        "name": "test_typescript_symbol_extraction",
        "type": "function",
        "start_line": 16,
        "end_line": 34,
        "code": "def test_typescript_symbol_extraction():\n    with tempfile.TemporaryDirectory() as tmpdir:\n        ts_path = os.path.join(tmpdir, \"golden_typescript.ts\")\n        # Read content from the actual golden file\n        golden_content = open(os.path.join(os.path.dirname(__file__), \"golden_typescript.ts\")).read()\n        symbols = run_extraction(tmpdir, \"golden_typescript.ts\", golden_content)\n        names_types = {(s[\"name\"], s[\"type\"]) for s in symbols}\n\n        expected = {\n            (\"MyClass\", \"class\"),\n            (\"MyInterface\", \"interface\"),\n            (\"MyEnum\", \"enum\"),\n            (\"helper\", \"function\")\n        }\n\n        assert (\"MyClass\", \"class\") in names_types\n        assert (\"MyInterface\", \"interface\") in names_types\n        assert (\"MyEnum\", \"enum\") in names_types\n        assert (\"helper\", \"function\") in names_types",
        "file": "tests/test_golden_symbols.py"
      },
      {
        "name": "test_python_symbol_extraction",
        "type": "function",
        "start_line": 37,
        "end_line": 63,
        "code": "def test_python_symbol_extraction():\n    with tempfile.TemporaryDirectory() as tmpdir:\n        py_path = os.path.join(tmpdir, \"golden_python.py\")\n        # Read content from the actual golden file\n        golden_content = open(os.path.join(os.path.dirname(__file__), \"golden_python.py\")).read()\n        symbols = run_extraction(tmpdir, \"golden_python.py\", golden_content)\n        # Convert to set of tuples for easier assertion.\n        # Note: The current basic query likely won't capture methods or async correctly.\n        names_types = {(s[\"name\"], s[\"type\"]) for s in symbols}\n\n        # Expected symbols based on the *improved* query\n        expected = {\n            (\"top_level_function\", \"function\"),\n            (\"MyClass\", \"class\"),\n            (\"__init__\", \"method\"), \n            (\"method_one\", \"method\"),\n            (\"async_function\", \"function\"),\n        }\n\n        # We'll refine the assertions as we improve the query\n        assert (\"top_level_function\", \"function\") in names_types\n        assert (\"MyClass\", \"class\") in names_types\n        assert (\"method_one\", \"method\") in names_types\n        assert (\"async_function\", \"function\") in names_types\n\n        # Example of more precise assertion (use once query is improved)\n        assert names_types == expected",
        "file": "tests/test_golden_symbols.py"
      },
      {
        "name": "test_python_complex_symbol_extraction",
        "type": "function",
        "start_line": 67,
        "end_line": 97,
        "code": "def test_python_complex_symbol_extraction():\n    with tempfile.TemporaryDirectory() as tmpdir:\n        golden_content = open(os.path.join(os.path.dirname(__file__), \"golden_python_complex.py\")).read()\n        symbols = run_extraction(tmpdir, \"golden_python_complex.py\", golden_content)\n        names_types = {(s[\"name\"], s[\"type\"]) for s in symbols}\n\n        # Expected symbols based on current Python query\n        # NOTE: Current query doesn't capture decorators well, nested functions, lambdas, or generators explicitly\n        expected = {\n            (\"decorator\", \"function\"),          # Decorator function itself\n            (\"decorated_function\", \"function\"),# The decorated function\n            (\"OuterClass\", \"class\"),\n            (\"outer_method\", \"method\"),\n            (\"InnerClass\", \"class\"),          # Nested class\n            (\"__init__\", \"method\"),           # Inner class method\n            (\"inner_method\", \"method\"),        # Inner class method\n            (\"static_inner\", \"method\"),       # Inner class static method\n            (\"nested_function_in_method\", \"method\"), # Method containing nested func\n            # (\"deeply_nested\", \"function\"),   # NOT CAPTURED - function defined inside method\n            (\"generator_function\", \"function\"),# Generator (captured as function)\n            (\"async_generator\", \"function\"),   # Async Generator (captured as function)\n            # lambda_func is not captured by name\n            (\"another_top_level\", \"function\")\n        }\n\n        # Assert individual expected symbols exist\n        for item in expected:\n            assert item in names_types, f\"Expected symbol {item} not found in {names_types}\"\n\n        # Assert the exact set matches (allows for debugging extra captures)\n        assert names_types == expected, f\"Mismatch: Got {names_types}, Expected {expected}\"",
        "file": "tests/test_golden_symbols.py"
      },
      {
        "name": "test_typescript_complex_symbol_extraction",
        "type": "function",
        "start_line": 100,
        "end_line": 134,
        "code": "def test_typescript_complex_symbol_extraction():\n    with tempfile.TemporaryDirectory() as tmpdir:\n        golden_content = open(os.path.join(os.path.dirname(__file__), \"golden_typescript_complex.ts\")).read()\n        symbols = run_extraction(tmpdir, \"golden_typescript_complex.ts\", golden_content)\n        names_types = {(s[\"name\"], s[\"type\"]) for s in symbols}\n\n        # Expected symbols based on current TypeScript query\n        # NOTE: Current query might not capture all nuances (e.g. arrow funcs, namespaces well)\n        expected = {\n            (\"UserProfile\", \"interface\"),\n            (\"Status\", \"enum\"),\n            (\"Utilities\", \"namespace\"),       # Namespace itself\n            (\"log\", \"function\"),             # Function inside namespace\n            (\"StringHelper\", \"class\"),        # Class inside namespace\n            (\"capitalize\", \"method\"),       # Static method inside namespace class\n            (\"identity\", \"function\"),         # Generic function\n            (\"GenericRepo\", \"class\"),         # Generic class\n            (\"add\", \"method\"),              # Method in generic class\n            (\"getAll\", \"method\"),           # Method in generic class\n            (\"constructor\", \"method\"),     # Constructor is captured by method query\n            # (\"addNumbers\", \"function\"),    # NOT CAPTURED - Arrow function assigned to const\n            (\"DecoratedClass\", \"class\"),\n            (\"greet\", \"method\"),\n            (\"calculateArea\", \"function\"),    # Exported function\n            (\"fetchData\", \"function\"),         # Async function\n            (\"SimpleLogger\", \"class\"),\n            (\"log\", \"method\")               # Method in SimpleLogger (duplicate name, diff class)\n        }\n\n        # Assert individual expected symbols exist\n        for item in expected:\n            assert item in names_types, f\"Expected symbol {item} not found in {names_types}\"\n\n        # Assert the exact set matches (allows for debugging extra captures)\n        assert names_types == expected, f\"Mismatch: Got {names_types}, Expected {expected}\"",
        "file": "tests/test_golden_symbols.py"
      },
      {
        "name": "test_hcl_symbol_extraction",
        "type": "function",
        "start_line": 138,
        "end_line": 162,
        "code": "def test_hcl_symbol_extraction():\n    with tempfile.TemporaryDirectory() as tmpdir:\n        golden_content = open(os.path.join(os.path.dirname(__file__), \"golden_hcl.tf\")).read()\n        symbols = run_extraction(tmpdir, \"golden_hcl.tf\", golden_content)\n        names_types = {(s[\"name\"], s[\"type\"]) for s in symbols}\n\n        # Expected symbols based on HCL query and updated extractor logic\n        expected = {\n            (\"aws\", \"provider\"),               # provider \"aws\"\n            (\"aws_instance.web_server\", \"resource\"), # resource \"aws_instance\" \"web_server\"\n            (\"aws_s3_bucket.data_bucket\", \"resource\"),# resource \"aws_s3_bucket\" \"data_bucket\"\n            (\"aws_ami.ubuntu\", \"data\"),         # data \"aws_ami\" \"ubuntu\"\n            (\"server_port\", \"variable\"),         # variable \"server_port\"\n            (\"instance_ip_addr\", \"output\"),      # output \"instance_ip_addr\"\n            (\"vpc\", \"module\"),                 # module \"vpc\"\n            (\"locals\", \"locals\"),               # locals block\n            (\"terraform\", \"terraform\")        # terraform block\n        }\n\n        # Assert individual expected symbols exist\n        for item in expected:\n            assert item in names_types, f\"Expected symbol {item} not found in {names_types}\"\n\n        # Assert the exact set matches\n        assert names_types == expected, f\"Mismatch: Got {names_types}, Expected {expected}\"",
        "file": "tests/test_golden_symbols.py"
      },
      {
        "name": "test_go_symbol_extraction",
        "type": "function",
        "start_line": 166,
        "end_line": 187,
        "code": "def test_go_symbol_extraction():\n    with tempfile.TemporaryDirectory() as tmpdir:\n        golden_content = open(os.path.join(os.path.dirname(__file__), \"golden_go.go\")).read()\n        symbols = run_extraction(tmpdir, \"golden_go.go\", golden_content)\n        names_types = {(s[\"name\"], s[\"type\"]) for s in symbols}\n\n        # Expected symbols based on Go query\n        expected = {\n            (\"User\", \"struct\"),         # type User struct {...}\n            (\"Greeter\", \"interface\"),   # type Greeter interface {...}\n            (\"Greet\", \"method\"),        # func (u User) Greet() string {...}\n            (\"Add\", \"function\"),        # func Add(a, b int) int {...}\n            (\"HelperFunction\", \"function\"), # func HelperFunction() {...}\n            (\"main\", \"function\"),       # func main() {...}\n        }\n\n        # Assert individual expected symbols exist\n        for item in expected:\n            assert item in names_types, f\"Expected symbol {item} not found in {names_types}\"\n\n        # Assert the exact set matches\n        assert names_types == expected, f\"Mismatch: Got {names_types}, Expected {expected}\"",
        "file": "tests/test_golden_symbols.py"
      }
    ],
    "tests/test_code_searcher.py": [
      {
        "name": "test_search_text_basic",
        "type": "function",
        "start_line": 4,
        "end_line": 17,
        "code": "def test_search_text_basic():\n    with tempfile.TemporaryDirectory() as tmpdir:\n        pyfile = os.path.join(tmpdir, \"foo.py\")\n        with open(pyfile, \"w\") as f:\n            f.write(\"\"\"\ndef foo(): pass\n\ndef bar(): pass\n\"\"\")\n        searcher = CodeSearcher(tmpdir)\n        matches = searcher.search_text(\"def foo\")\n        assert any(\"foo\" in m[\"line\"] for m in matches)\n        matches_bar = searcher.search_text(\"bar\")\n        assert any(\"bar\" in m[\"line\"] for m in matches_bar)",
        "file": "tests/test_code_searcher.py"
      },
      {
        "name": "test_search_text_multiple_files",
        "type": "function",
        "start_line": 19,
        "end_line": 28,
        "code": "def test_search_text_multiple_files():\n    with tempfile.TemporaryDirectory() as tmpdir:\n        files = [\"a.py\", \"b.py\", \"c.txt\"]\n        for fname in files:\n            with open(os.path.join(tmpdir, fname), \"w\") as f:\n                f.write(f\"def {fname[:-3]}(): pass\\n\")\n        searcher = CodeSearcher(tmpdir)\n        matches = searcher.search_text(\"def \", file_pattern=\"*.py\")\n        assert len(matches) == 2\n        assert all(m[\"file\"].endswith(\".py\") for m in matches)",
        "file": "tests/test_code_searcher.py"
      },
      {
        "name": "test_search_text_regex",
        "type": "function",
        "start_line": 30,
        "end_line": 38,
        "code": "def test_search_text_regex():\n    with tempfile.TemporaryDirectory() as tmpdir:\n        pyfile = os.path.join(tmpdir, \"foo.py\")\n        with open(pyfile, \"w\") as f:\n            f.write(\"def foo(): pass\\ndef bar(): pass\\n\")\n        searcher = CodeSearcher(tmpdir)\n        matches = searcher.search_text(r\"def [fb]oo\")\n        assert any(\"foo\" in m[\"line\"] for m in matches)\n        assert not any(\"bar\" in m[\"line\"] for m in matches)",
        "file": "tests/test_code_searcher.py"
      }
    ],
    "tests/test_repo_mapper.py": [
      {
        "name": "test_get_file_tree",
        "type": "function",
        "start_line": 3,
        "end_line": 13,
        "code": "def test_get_file_tree():\n    with tempfile.TemporaryDirectory() as tmpdir:\n        # Create some files and dirs\n        import os\n        os.makedirs(f\"{tmpdir}/foo/bar\")\n        with open(f\"{tmpdir}/foo/bar/baz.py\", \"w\") as f:\n            f.write(\"def test(): pass\\n\")\n        mapper = RepoMapper(tmpdir)\n        tree = mapper.get_file_tree()\n        assert any(item[\"path\"].endswith(\"baz.py\") for item in tree)\n        assert any(item[\"is_dir\"] and item[\"path\"].endswith(\"foo/bar\") for item in tree)",
        "file": "tests/test_repo_mapper.py"
      },
      {
        "name": "test_extract_symbols",
        "type": "function",
        "start_line": 15,
        "end_line": 32,
        "code": "def test_extract_symbols():\n    with tempfile.TemporaryDirectory() as tmpdir:\n        pyfile = f\"{tmpdir}/a.py\"\n        with open(pyfile, \"w\") as f:\n            f.write(\"\"\"\nclass Foo:\n    def bar(self): pass\n\ndef baz(): pass\n\"\"\")\n        mapper = RepoMapper(tmpdir)\n        symbols = mapper.extract_symbols(\"a.py\")\n        names = {s[\"name\"] for s in symbols}\n        assert \"Foo\" in names\n        assert \"baz\" in names\n        types = {s[\"type\"] for s in symbols}\n        assert \"class\" in types\n        assert \"function\" in types",
        "file": "tests/test_repo_mapper.py"
      }
    ],
    "tests/test_repo.py": [
      {
        "name": "test_repo_get_file_tree_and_symbols",
        "type": "function",
        "start_line": 5,
        "end_line": 25,
        "code": "def test_repo_get_file_tree_and_symbols():\n    with tempfile.TemporaryDirectory() as tmpdir:\n        os.makedirs(f\"{tmpdir}/foo/bar\")\n        with open(f\"{tmpdir}/foo/bar/baz.py\", \"w\") as f:\n            f.write(\"\"\"\nclass Foo:\n    def bar(self): pass\n\ndef baz(): pass\n\"\"\")\n        repository = Repository(tmpdir)\n        tree = repository.get_file_tree()\n        assert any(item[\"path\"].endswith(\"baz.py\") for item in tree)\n        assert any(item[\"is_dir\"] and item[\"path\"].endswith(\"foo/bar\") for item in tree)\n        symbols = repository.extract_symbols(\"foo/bar/baz.py\")\n        names = {s[\"name\"] for s in symbols}\n        assert \"Foo\" in names\n        assert \"baz\" in names\n        types = {s[\"type\"] for s in symbols}\n        assert \"class\" in types\n        assert \"function\" in types",
        "file": "tests/test_repo.py"
      },
      {
        "name": "test_repo_file_tree_various",
        "type": "function",
        "start_line": 31,
        "end_line": 41,
        "code": "def test_repo_file_tree_various(structure):\n    with tempfile.TemporaryDirectory() as tmpdir:\n        for relpath in structure:\n            path = os.path.join(tmpdir, relpath)\n            os.makedirs(os.path.dirname(path), exist_ok=True)\n            with open(path, \"w\") as f:\n                f.write(\"pass\\n\")\n        repository = Repository(tmpdir)\n        tree = repository.get_file_tree()\n        for relpath in structure:\n            assert any(item[\"path\"].endswith(relpath) for item in tree)",
        "file": "tests/test_repo.py"
      },
      {
        "name": "test_repo_get_file_content",
        "type": "function",
        "start_line": 43,
        "end_line": 75,
        "code": "def test_repo_get_file_content():\n    with tempfile.TemporaryDirectory() as tmpdir:\n        # Setup: Create some test files\n        content1 = \"Hello, world!\\nThis is a test file.\"\n        file1_path = \"dir1/file1.txt\"\n        full_file1_path = os.path.join(tmpdir, file1_path)\n        os.makedirs(os.path.dirname(full_file1_path), exist_ok=True)\n        with open(full_file1_path, \"w\") as f:\n            f.write(content1)\n\n        empty_file_path = \"empty.txt\"\n        full_empty_file_path = os.path.join(tmpdir, empty_file_path)\n        with open(full_empty_file_path, \"w\") as f:\n            pass # Create an empty file\n\n        repository = Repository(tmpdir)\n\n        # Test 1: Read content from an existing file\n        retrieved_content1 = repository.get_file_content(file1_path)\n        assert retrieved_content1 == content1\n\n        # Test 2: Read content from an empty file\n        retrieved_empty_content = repository.get_file_content(empty_file_path)\n        assert retrieved_empty_content == \"\"\n\n        # Test 3: Attempt to read content from a non-existent file\n        non_existent_file_path = \"non_existent.txt\"\n        with pytest.raises(FileNotFoundError):\n            repository.get_file_content(non_existent_file_path)\n\n        # Test 4: Attempt to read content from a directory (should also fail)\n        with pytest.raises(IOError): # Or perhaps FileNotFoundError or IsADirectoryError, adjust as per actual behavior\n            repository.get_file_content(\"dir1\")",
        "file": "tests/test_repo.py"
      }
    ],
    "tests/test_docstring_indexer.py": [
      {
        "name": "DummyBackend",
        "type": "class",
        "start_line": 10,
        "end_line": 36,
        "code": "class DummyBackend(VectorDBBackend):\n    \"\"\"In-memory VectorDB backend for testing purposes.\"\"\"\n\n    def __init__(self):\n        self.embeddings = []\n        self.metadatas = []\n        self.ids = [] # Add storage for IDs\n\n    # --- VectorDBBackend interface -------------------------------------\n    def add(self, embeddings, metadatas, ids=None): # Add ids parameter\n        self.embeddings.extend(embeddings)\n        self.metadatas.extend(metadatas)\n        if ids:\n            self.ids.extend(ids)\n        else: # Maintain old behavior if ids not provided by test\n            self.ids.extend([str(i) for i in range(len(metadatas))])\n\n    def query(self, embedding, top_k):  # noqa: D401\n        \"\"\"Return first *top_k* stored metadatas (distance ignored).\"\"\"\n        return self.metadatas[: top_k]\n\n    def persist(self):\n        # No-op for the in-memory backend\n        pass\n\n    def count(self): # Add count method\n        return len(self.metadatas)",
        "file": "tests/test_docstring_indexer.py"
      },
      {
        "name": "__init__",
        "type": "method",
        "start_line": 13,
        "end_line": 16,
        "code": "def __init__(self):\n        self.embeddings = []\n        self.metadatas = []\n        self.ids = [] # Add storage for IDs",
        "file": "tests/test_docstring_indexer.py"
      },
      {
        "name": "add",
        "type": "method",
        "start_line": 19,
        "end_line": 25,
        "code": "def add(self, embeddings, metadatas, ids=None): # Add ids parameter\n        self.embeddings.extend(embeddings)\n        self.metadatas.extend(metadatas)\n        if ids:\n            self.ids.extend(ids)\n        else: # Maintain old behavior if ids not provided by test\n            self.ids.extend([str(i) for i in range(len(metadatas))])",
        "file": "tests/test_docstring_indexer.py"
      },
      {
        "name": "query",
        "type": "method",
        "start_line": 27,
        "end_line": 29,
        "code": "def query(self, embedding, top_k):  # noqa: D401\n        \"\"\"Return first *top_k* stored metadatas (distance ignored).\"\"\"\n        return self.metadatas[: top_k]",
        "file": "tests/test_docstring_indexer.py"
      },
      {
        "name": "persist",
        "type": "method",
        "start_line": 31,
        "end_line": 33,
        "code": "def persist(self):\n        # No-op for the in-memory backend\n        pass",
        "file": "tests/test_docstring_indexer.py"
      },
      {
        "name": "count",
        "type": "method",
        "start_line": 35,
        "end_line": 36,
        "code": "def count(self): # Add count method\n        return len(self.metadatas)",
        "file": "tests/test_docstring_indexer.py"
      },
      {
        "name": "dummy_repo",
        "type": "function",
        "start_line": 40,
        "end_line": 45,
        "code": "def dummy_repo(tmp_path):\n    \"\"\"Create a temporary repository with a single Python file.\"\"\"\n    repo_root = tmp_path / \"repo\"\n    repo_root.mkdir()\n    (repo_root / \"hello.py\").write_text(\"\"\"def hello():\\n    return 'hi'\\n\"\"\")\n    return Repository(str(repo_root))",
        "file": "tests/test_docstring_indexer.py"
      },
      {
        "name": "repo_with_symbols",
        "type": "function",
        "start_line": 49,
        "end_line": 62,
        "code": "def repo_with_symbols(tmp_path):\n    \"\"\"Create a temporary repository with a Python file containing symbols.\"\"\"\n    repo_root = tmp_path / \"repo_symbols\"\n    repo_root.mkdir()\n    file_content = \"\"\"\nclass MyClass:\n    def method_one(self):\n        return 'method one'\n\ndef my_function():\n    return 'function one'\n\"\"\"\n    (repo_root / \"symbols.py\").write_text(file_content)\n    return Repository(str(repo_root)), repo_root / \"symbols.py\"",
        "file": "tests/test_docstring_indexer.py"
      },
      {
        "name": "test_index_and_search",
        "type": "function",
        "start_line": 65,
        "end_line": 99,
        "code": "def test_index_and_search(dummy_repo):\n    # --- Arrange --------------------------------------------------------\n    summarizer = MagicMock()\n    summarizer.summarize_file.side_effect = lambda p: f\"Summary of {p}\"\n    # Mock summarize_function as it's called by DocstringIndexer for symbol-level indexing\n    summarizer.summarize_function.side_effect = lambda path_str, func_name: f\"Summary of function {func_name} in {path_str}\"\n    # Add for completeness, though not strictly needed for the 'hello.py' (function only) test case\n    summarizer.summarize_class.side_effect = lambda path_str, class_name: f\"Summary of class {class_name} in {path_str}\"\n\n    embed_fn = lambda text: [float(len(text))]  # very simple embedding\n\n    backend = DummyBackend()\n\n    indexer = DocstringIndexer(dummy_repo, summarizer, embed_fn, backend=backend)\n\n    # --- Act ------------------------------------------------------------\n    indexer.build()\n\n    # --- Assert build() -------------------------------------------------\n    # The repo contains exactly one file -> one embedding & metadata\n    assert len(backend.embeddings) == 1\n    assert len(backend.metadatas) == 1\n\n    meta = backend.metadatas[0]\n    assert meta[\"file_path\"].endswith(\"hello.py\") # Changed \"file\" to \"file_path\"\n    assert meta[\"summary\"].startswith(\"Summary of\")\n\n    summarizer.summarize_function.assert_called_once() # For symbol-level on 'hello' function\n\n    # --- Act & Assert search() -----------------------------------------\n    searcher = SummarySearcher(indexer)\n    hits = searcher.search(\"hello\", top_k=5)\n    assert hits\n    assert hits[0][\"file_path\"].endswith(\"hello.py\") # Changed \"file\" to \"file_path\", and adjusted for direct metadata access if SummarySearcher returns it directly\n    assert \"summary\" in hits[0]",
        "file": "tests/test_docstring_indexer.py"
      },
      {
        "name": "test_index_and_search_symbol_level",
        "type": "function",
        "start_line": 102,
        "end_line": 187,
        "code": "def test_index_and_search_symbol_level(repo_with_symbols):\n    dummy_repo, file_path = repo_with_symbols\n    relative_file_path = str(file_path.relative_to(dummy_repo.repo_path)) # Corrected to repo_path\n\n    # --- Arrange --------------------------------------------------------\n    mock_summarizer = MagicMock(spec=Summarizer)\n    mock_summarizer.summarize_class.return_value = \"Summary of MyClass\"\n    \n    # Define a side_effect function for summarize_function\n    def mock_summarize_func_side_effect(file_path_arg, symbol_name_or_node_path_arg, **kwargs):\n        if symbol_name_or_node_path_arg == \"MyClass.method_one\":\n            return \"Summary of MyClass.method_one\"\n        elif symbol_name_or_node_path_arg == \"my_function\":\n            return \"Summary of my_function\"\n        return \"Unknown function summary\" # Fallback, should not be hit in this test\n\n    mock_summarizer.summarize_function.side_effect = mock_summarize_func_side_effect\n\n    # Mock Repository's extract_symbols method\n    # Ensure dummy_repo itself is not a mock, but its methods can be\n    dummy_repo.extract_symbols = MagicMock(return_value=[\n        {\"name\": \"MyClass\", \"type\": \"CLASS\", \"node_path\": \"MyClass\", \"code\": \"class MyClass: pass\"},\n        {\"name\": \"method_one\", \"type\": \"METHOD\", \"node_path\": \"MyClass.method_one\", \"code\": \"def method_one(self): pass\"}, # Assuming extract_symbols gives qualified name\n        {\"name\": \"my_function\", \"type\": \"FUNCTION\", \"node_path\": \"my_function\", \"code\": \"def my_function(): pass\"},\n    ])\n\n    embed_fn = lambda text: [float(len(text))]  # very simple embedding\n    backend = DummyBackend()\n    indexer = DocstringIndexer(dummy_repo, mock_summarizer, embed_fn, backend=backend)\n\n    # --- Act ------------------------------------------------------------\n    indexer.build(level=\"symbol\", file_extensions=[\".py\"], force=True)\n\n    # --- Assert build() -------------------------------------------------\n    dummy_repo.extract_symbols.assert_called_once_with(relative_file_path)\n    \n    # Check calls to summarizer\n    # Order of symbol extraction might vary, so check calls without specific order if needed\n    # or ensure mock_extract_symbols returns in a fixed order.\n    mock_summarizer.summarize_class.assert_called_once_with(relative_file_path, \"MyClass\")\n    assert mock_summarizer.summarize_function.call_count == 2\n    mock_summarizer.summarize_function.assert_any_call(relative_file_path, \"MyClass.method_one\")\n    mock_summarizer.summarize_function.assert_any_call(relative_file_path, \"my_function\")\n\n    assert len(backend.embeddings) == 3\n    assert len(backend.metadatas) == 3\n    assert len(backend.ids) == 3\n\n    expected_ids = [\n        f\"{relative_file_path}::MyClass\",\n        f\"{relative_file_path}::MyClass.method_one\",\n        f\"{relative_file_path}::my_function\",\n    ]\n    assert sorted(backend.ids) == sorted(expected_ids)\n\n    for meta in backend.metadatas:\n        assert meta[\"level\"] == \"symbol\"\n        assert meta[\"file_path\"] == relative_file_path\n        assert \"symbol_name\" in meta\n        assert \"symbol_type\" in meta\n        assert \"summary\" in meta\n        if meta[\"symbol_name\"] == \"MyClass\":\n            assert meta[\"summary\"] == \"Summary of MyClass\"\n            assert meta[\"symbol_type\"] == \"CLASS\"\n        elif meta[\"symbol_name\"] == \"MyClass.method_one\":\n            assert meta[\"summary\"] == \"Summary of MyClass.method_one\"\n            assert meta[\"symbol_type\"] == \"METHOD\"\n        elif meta[\"symbol_name\"] == \"my_function\":\n            assert meta[\"summary\"] == \"Summary of my_function\"\n            assert meta[\"symbol_type\"] == \"FUNCTION\"\n\n    # --- Act & Assert search() -----------------------------------------\n    searcher = SummarySearcher(indexer)\n    hits = searcher.search(\"query for MyClass\", top_k=3)\n    assert len(hits) == 3 # DummyBackend query returns all in order\n\n    # Assuming 'Summary of MyClass' is most similar due to simple embed_fn\n    # or that the query function in DummyBackend just returns metadatas in order\n    found_myclass = False\n    for hit in hits:\n        assert hit[\"level\"] == \"symbol\"\n        if hit[\"symbol_name\"] == \"MyClass\":\n            found_myclass = True\n            assert hit[\"file_path\"] == relative_file_path\n            assert hit[\"summary\"] == \"Summary of MyClass\"\n    assert found_myclass, \"MyClass symbol not found in search results\"",
        "file": "tests/test_docstring_indexer.py"
      }
    ],
    "tests/test_context_assembler.py": [
      {
        "name": "test_context_assembler_basic",
        "type": "function",
        "start_line": 2,
        "end_line": 13,
        "code": "def test_context_assembler_basic(tmp_path):\n    # Create a simple repo with one file\n    file_path = tmp_path / \"foo.py\"\n    file_path.write_text(\"print('hi')\\n\")\n\n    repo = Repository(str(tmp_path))\n    assembler = ContextAssembler(repo)\n    assembler.add_file(\"foo.py\")\n    ctx = assembler.format_context()\n\n    assert \"foo.py\" in ctx\n    assert \"print('hi')\" in ctx",
        "file": "tests/test_context_assembler.py"
      }
    ],
    "tests/test_context_extractor.py": [
      {
        "name": "test_chunk_file_by_lines",
        "type": "function",
        "start_line": 4,
        "end_line": 12,
        "code": "def test_chunk_file_by_lines():\n    with tempfile.TemporaryDirectory() as tmpdir:\n        fpath = f\"{tmpdir}/test.py\"\n        with open(fpath, \"w\") as f:\n            f.write((\"def foo():\\n    pass\\ndef bar():\\n    pass\\n\") * 10)\n        extractor = ContextExtractor(tmpdir)\n        chunks = extractor.chunk_file_by_lines(\"test.py\", max_lines=3)\n        assert len(chunks) > 0\n        assert all(isinstance(chunk, str) for chunk in chunks)",
        "file": "tests/test_context_extractor.py"
      },
      {
        "name": "test_chunk_file_by_symbols",
        "type": "function",
        "start_line": 14,
        "end_line": 26,
        "code": "def test_chunk_file_by_symbols():\n    with tempfile.TemporaryDirectory() as tmpdir:\n        fpath = f\"{tmpdir}/test.py\"\n        with open(fpath, \"w\") as f:\n            f.write(\"\"\"class Foo:\\n    def bar(self): pass\\ndef baz(): pass\\n\"\"\")\n        extractor = ContextExtractor(tmpdir)\n        chunks = extractor.chunk_file_by_symbols(\"test.py\")\n        names = {c[\"name\"] for c in chunks}\n        assert \"Foo\" in names\n        assert \"baz\" in names\n        types = {c[\"type\"] for c in chunks}\n        assert \"class\" in types\n        assert \"function\" in types",
        "file": "tests/test_context_extractor.py"
      },
      {
        "name": "test_extract_context_around_line",
        "type": "function",
        "start_line": 28,
        "end_line": 184,
        "code": "def test_extract_context_around_line():\n    with tempfile.TemporaryDirectory() as tmpdir_str:\n        tmpdir = Path(tmpdir_str)\n        extractor = ContextExtractor(tmpdir_str)\n\n        # --- Test 1: Python file, line within a function (existing test, adapted) ---\n        py_file_func = tmpdir / \"py_func.py\"\n        py_file_func_content = \"\"\"def foo():\n    x = 1 # Target line for foo\n    y = 2\n    return x + y\n\"\"\"\n        with open(py_file_func, \"w\") as f:\n            f.write(py_file_func_content)\n        ctx_func = extractor.extract_context_around_line(\"py_func.py\", 2)\n        assert ctx_func is not None\n        assert ctx_func[\"type\"] == \"function\"\n        assert ctx_func[\"name\"] == \"foo\"\n        assert ctx_func[\"code\"] == py_file_func_content\n\n        # --- Test 2: Python file, line within a class definition ---\n        py_file_class = tmpdir / \"py_class.py\"\n        py_file_class_content = \"\"\"class MyClass:\n    class_var = 10 # Target line for MyClass\n\n    def __init__(self):\n        self.instance_var = 20\n\ndef another_func():\n    pass\n\"\"\"\n        with open(py_file_class, \"w\") as f:\n            f.write(py_file_class_content)\n        ctx_class = extractor.extract_context_around_line(\"py_class.py\", 2)\n        assert ctx_class is not None\n        assert ctx_class[\"type\"] == \"class\"\n        assert ctx_class[\"name\"] == \"MyClass\"\n        assert \"class_var = 10\" in ctx_class[\"code\"]\n        assert \"def __init__\" in ctx_class[\"code\"]\n        assert \"another_func\" not in ctx_class[\"code\"] # Ensure it doesn't grab unrelated parts\n\n        # --- Test 3: Python file, line is top-level (should use fallback) ---\n        py_file_toplevel = tmpdir / \"py_toplevel.py\"\n        py_file_toplevel_content = \"\"\"import os\nMY_GLOBAL = 100 # Target line for top-level\nprint(MY_GLOBAL)\ndef helper():\n    pass\n\"\"\"\n        with open(py_file_toplevel, \"w\") as f:\n            f.write(py_file_toplevel_content)\n        ctx_toplevel = extractor.extract_context_around_line(\"py_toplevel.py\", 2)\n        assert ctx_toplevel is not None\n        assert ctx_toplevel[\"type\"] == \"code_chunk\"\n        assert ctx_toplevel[\"name\"] == \"py_toplevel.py:2\"\n        assert \"MY_GLOBAL = 100\" in ctx_toplevel[\"code\"]\n        # Check if it captured the surrounding lines (default 10 up/down)\n        # Since this file is short, it should capture most/all of it.\n        assert \"import os\" in ctx_toplevel[\"code\"]\n        assert \"print(MY_GLOBAL)\" in ctx_toplevel[\"code\"]\n\n        # --- Test 4: Python file with syntax error (should use fallback) ---\n        py_file_syntax_error = tmpdir / \"py_syntax_error.py\"\n        py_file_syntax_error_content = \"\"\"def valid_func():\n    print(\"ok\")\nthis_is_a_syntax_error x = # Target line for syntax error\ndef another_valid_func():\n    print(\"still ok\")\n\"\"\"\n        with open(py_file_syntax_error, \"w\") as f:\n            f.write(py_file_syntax_error_content)\n        ctx_syntax_error = extractor.extract_context_around_line(\"py_syntax_error.py\", 3)\n        assert ctx_syntax_error is not None\n        assert ctx_syntax_error[\"type\"] == \"code_chunk\"\n        assert ctx_syntax_error[\"name\"] == \"py_syntax_error.py:3\"\n        assert \"this_is_a_syntax_error x =\" in ctx_syntax_error[\"code\"]\n        assert \"def valid_func()\" in ctx_syntax_error[\"code\"] # Part of the chunk\n        assert \"def another_valid_func()\" in ctx_syntax_error[\"code\"] # Part of the chunk\n\n        # --- Test 5: Non-Python file (e.g., .txt) ---\n        txt_file = tmpdir / \"test.txt\"\n        txt_file_lines = [f\"Line {i+1}\\n\" for i in range(30)]\n        txt_file_content = \"\".join(txt_file_lines)\n        with open(txt_file, \"w\") as f:\n            f.write(txt_file_content)\n        \n        # Target line 15 (0-indexed 14)\n        # Expect lines 5 to 25 (0-indexed 4 to 24)\n        ctx_txt = extractor.extract_context_around_line(\"test.txt\", 15)\n        assert ctx_txt is not None\n        assert ctx_txt[\"type\"] == \"code_chunk\"\n        assert ctx_txt[\"name\"] == \"test.txt:15\"\n        expected_txt_chunk_lines = txt_file_lines[15-1-10 : 15-1+10+1]\n        assert ctx_txt[\"code\"] == \"\".join(expected_txt_chunk_lines)\n        assert \"Line 5\" in ctx_txt[\"code\"]\n        assert \"Line 25\" in ctx_txt[\"code\"]\n        assert \"Line 4\" not in ctx_txt[\"code\"]\n        assert \"Line 26\" not in ctx_txt[\"code\"]\n\n        # --- Test 6: Line-chunking, near start of file ---\n        ctx_txt_start = extractor.extract_context_around_line(\"test.txt\", 2) # Target line 2\n        assert ctx_txt_start is not None\n        assert ctx_txt_start[\"type\"] == \"code_chunk\"\n        assert ctx_txt_start[\"name\"] == \"test.txt:2\"\n        # Expect lines 1 to 12 (0-indexed 0 to 11)\n        expected_txt_start_chunk_lines = txt_file_lines[0 : 2-1+10+1]\n        assert ctx_txt_start[\"code\"] == \"\".join(expected_txt_start_chunk_lines)\n        assert \"Line 1\" in ctx_txt_start[\"code\"]\n        assert \"Line 12\" in ctx_txt_start[\"code\"]\n        assert \"Line 13\" not in ctx_txt_start[\"code\"]\n\n        # --- Test 7: Line-chunking, near end of file ---\n        ctx_txt_end = extractor.extract_context_around_line(\"test.txt\", 29) # Target line 29 in 30 line file\n        assert ctx_txt_end is not None\n        assert ctx_txt_end[\"type\"] == \"code_chunk\"\n        assert ctx_txt_end[\"name\"] == \"test.txt:29\"\n        # Expect lines 19 to 30 (0-indexed 18 to 29)\n        expected_txt_end_chunk_lines = txt_file_lines[29-1-10 : 30]\n        assert ctx_txt_end[\"code\"] == \"\".join(expected_txt_end_chunk_lines)\n        assert \"Line 19\" in ctx_txt_end[\"code\"]\n        assert \"Line 30\" in ctx_txt_end[\"code\"]\n        assert \"Line 18\" not in ctx_txt_end[\"code\"]\n\n        # --- Test 8: Line-chunking, target line out of bounds ---\n        ctx_out_of_bounds_high = extractor.extract_context_around_line(\"test.txt\", 50)\n        assert ctx_out_of_bounds_high is None\n        ctx_out_of_bounds_low = extractor.extract_context_around_line(\"test.txt\", 0)\n        assert ctx_out_of_bounds_low is None\n        ctx_out_of_bounds_negative = extractor.extract_context_around_line(\"test.txt\", -5)\n        assert ctx_out_of_bounds_negative is None\n\n        # --- Test 9: Line-chunking, file with fewer than context_delta * 2 + 1 lines ---\n        short_txt_file = tmpdir / \"short_test.txt\"\n        short_txt_content = \"Line 1\\nLine 2\\nLine 3\\n\"\n        with open(short_txt_file, \"w\") as f:\n            f.write(short_txt_content)\n        ctx_short_txt = extractor.extract_context_around_line(\"short_test.txt\", 2)\n        assert ctx_short_txt is not None\n        assert ctx_short_txt[\"type\"] == \"code_chunk\"\n        assert ctx_short_txt[\"name\"] == \"short_test.txt:2\"\n        assert ctx_short_txt[\"code\"] == short_txt_content # Should be the whole file\n\n        # --- Test 10: Python file, line in class method ---\n        py_file_method = tmpdir / \"py_method.py\"\n        py_method_content = \"\"\"class AnotherClass:\n    def a_method(self):\n        print(\"inside method\") # Target line\n        return True\n\"\"\"\n        with open(py_file_method, \"w\") as f:\n            f.write(py_method_content)\n        ctx_method = extractor.extract_context_around_line(\"py_method.py\", 3)\n        assert ctx_method is not None\n        assert ctx_method[\"type\"] == \"function\" # AST considers methods as FunctionDef\n        assert ctx_method[\"name\"] == \"a_method\"\n        assert \"print(\\\"inside method\\\")\" in ctx_method[\"code\"]\n        assert \"class AnotherClass:\" not in ctx_method[\"code\"] # Should be just the method",
        "file": "tests/test_context_extractor.py"
      }
    ],
    "tests/test_java_symbols.py": [
      {
        "name": "_extract",
        "type": "function",
        "start_line": 3,
        "end_line": 7,
        "code": "def _extract(tmpdir: str, filename: str, content: str):\n    path = os.path.join(tmpdir, filename)\n    with open(path, \"w\") as f:\n        f.write(content)\n    return Repository(tmpdir).extract_symbols(filename)",
        "file": "tests/test_java_symbols.py"
      },
      {
        "name": "test_java_symbols",
        "type": "function",
        "start_line": 9,
        "end_line": 24,
        "code": "def test_java_symbols():\n    code = \"\"\"\npublic class Foo {\n    public int x;\n    public Foo() {}\n    public void bar() {}\n}\n\ninterface Baz {}\n\nenum Color { RED, GREEN }\n\"\"\"\n    with tempfile.TemporaryDirectory() as tmpdir:\n        symbols = _extract(tmpdir, \"Foo.java\", code)\n        names = {s[\"name\"] for s in symbols}\n        assert {\"Foo\", \"bar\", \"Baz\", \"Color\"}.issubset(names)",
        "file": "tests/test_java_symbols.py"
      }
    ],
    "tests/golden_python_complex.py": [
      {
        "name": "decorator",
        "type": "function",
        "start_line": 6,
        "end_line": 12,
        "code": "def decorator(func):\n    def wrapper(*args, **kwargs):\n        print(\"Before call\")\n        result = func(*args, **kwargs)\n        print(\"After call\")\n        return result\n    return wrapper",
        "file": "tests/golden_python_complex.py"
      },
      {
        "name": "decorated_function",
        "type": "function",
        "start_line": 15,
        "end_line": 17,
        "code": "def decorated_function(x: int) -> int:\n    \"\"\"A decorated function.\"\"\"\n    return x * 2",
        "file": "tests/golden_python_complex.py"
      },
      {
        "name": "OuterClass",
        "type": "class",
        "start_line": 19,
        "end_line": 44,
        "code": "class OuterClass:\n    OUTER_CONST = \"outer\"\n\n    def outer_method(self):\n        print(\"Outer method\")\n\n    class InnerClass:\n        INNER_CONST = \"inner\"\n\n        def __init__(self, name: str):\n            self.name = name\n\n        def inner_method(self):\n            print(f\"Inner method called by {self.name}\")\n\n        @staticmethod\n        def static_inner():\n            print(\"Static inner method\")\n            \n        def nested_function_in_method(self):\n            \n            def deeply_nested():\n                print(\"Deeply nested function\")\n            \n            deeply_nested()\n            return \"nested_func_ran\"",
        "file": "tests/golden_python_complex.py"
      },
      {
        "name": "outer_method",
        "type": "method",
        "start_line": 22,
        "end_line": 23,
        "code": "def outer_method(self):\n        print(\"Outer method\")",
        "file": "tests/golden_python_complex.py"
      },
      {
        "name": "InnerClass",
        "type": "class",
        "start_line": 25,
        "end_line": 44,
        "code": "class InnerClass:\n        INNER_CONST = \"inner\"\n\n        def __init__(self, name: str):\n            self.name = name\n\n        def inner_method(self):\n            print(f\"Inner method called by {self.name}\")\n\n        @staticmethod\n        def static_inner():\n            print(\"Static inner method\")\n            \n        def nested_function_in_method(self):\n            \n            def deeply_nested():\n                print(\"Deeply nested function\")\n            \n            deeply_nested()\n            return \"nested_func_ran\"",
        "file": "tests/golden_python_complex.py"
      },
      {
        "name": "__init__",
        "type": "method",
        "start_line": 28,
        "end_line": 29,
        "code": "def __init__(self, name: str):\n            self.name = name",
        "file": "tests/golden_python_complex.py"
      },
      {
        "name": "inner_method",
        "type": "method",
        "start_line": 31,
        "end_line": 32,
        "code": "def inner_method(self):\n            print(f\"Inner method called by {self.name}\")",
        "file": "tests/golden_python_complex.py"
      },
      {
        "name": "static_inner",
        "type": "method",
        "start_line": 35,
        "end_line": 36,
        "code": "def static_inner():\n            print(\"Static inner method\")",
        "file": "tests/golden_python_complex.py"
      },
      {
        "name": "nested_function_in_method",
        "type": "method",
        "start_line": 38,
        "end_line": 44,
        "code": "def nested_function_in_method(self):\n            \n            def deeply_nested():\n                print(\"Deeply nested function\")\n            \n            deeply_nested()\n            return \"nested_func_ran\"",
        "file": "tests/golden_python_complex.py"
      },
      {
        "name": "generator_function",
        "type": "function",
        "start_line": 46,
        "end_line": 51,
        "code": "def generator_function(n):\n    \"\"\"A generator function.\"\"\"\n    i = 0\n    while i < n:\n        yield i\n        i += 1",
        "file": "tests/golden_python_complex.py"
      },
      {
        "name": "async_generator",
        "type": "function",
        "start_line": 53,
        "end_line": 57,
        "code": "async def async_generator(n):\n    i = 0\n    while i < n:\n        yield i\n        i += 1",
        "file": "tests/golden_python_complex.py"
      },
      {
        "name": "async_generator",
        "type": "function",
        "start_line": 53,
        "end_line": 57,
        "code": "async def async_generator(n):\n    i = 0\n    while i < n:\n        yield i\n        i += 1",
        "file": "tests/golden_python_complex.py"
      },
      {
        "name": "another_top_level",
        "type": "function",
        "start_line": 62,
        "end_line": 63,
        "code": "def another_top_level():\n    pass",
        "file": "tests/golden_python_complex.py"
      }
    ],
    "tests/test_typescript_symbol_extraction.py": [
      {
        "name": "test_typescript_symbol_extraction",
        "type": "function",
        "start_line": 5,
        "end_line": 20,
        "code": "def test_typescript_symbol_extraction(tmp_path: Path):\n    # Minimal TypeScript code with a function and a class\n    ts_code = '''\nfunction foo() {}\nclass Bar {}\n'''\n    ts_file = tmp_path / \"example.ts\"\n    ts_file.write_text(ts_code)\n    repository = Repository(str(tmp_path))\n    try:\n        symbols = repository.extract_symbols(\"example.ts\")\n    except Exception as e:\n        pytest.fail(f\"Symbol extraction failed: {e}\")\n    names_types = {(s.get(\"name\"), s.get(\"type\")) for s in symbols}\n    assert (\"foo\", \"function\") in names_types\n    assert (\"Bar\", \"class\") in names_types",
        "file": "tests/test_typescript_symbol_extraction.py"
      }
    ],
    "tests/test_docstring_incremental.py": [
      {
        "name": "DummyBackend",
        "type": "class",
        "start_line": 15,
        "end_line": 43,
        "code": "class DummyBackend(VectorDBBackend):\n    \"\"\"Minimal in-memory backend with delete support for tests.\"\"\"\n\n    def __init__(self):\n        self.embeddings = []\n        self.metadatas = []\n        self.ids = []\n\n    def add(self, embeddings, metadatas, ids=None):\n        self.embeddings.extend(embeddings)\n        self.metadatas.extend(metadatas)\n        self.ids.extend(ids or [str(i) for i in range(len(metadatas))])\n\n    def query(self, embedding, top_k):\n        return self.metadatas[:top_k]\n\n    def persist(self):\n        pass\n\n    def count(self):\n        return len(self.metadatas)\n\n    def delete(self, ids):\n        for _id in ids:\n            if _id in self.ids:\n                idx = self.ids.index(_id)\n                self.ids.pop(idx)\n                self.embeddings.pop(idx)\n                self.metadatas.pop(idx)",
        "file": "tests/test_docstring_incremental.py"
      },
      {
        "name": "__init__",
        "type": "method",
        "start_line": 18,
        "end_line": 21,
        "code": "def __init__(self):\n        self.embeddings = []\n        self.metadatas = []\n        self.ids = []",
        "file": "tests/test_docstring_incremental.py"
      },
      {
        "name": "add",
        "type": "method",
        "start_line": 23,
        "end_line": 26,
        "code": "def add(self, embeddings, metadatas, ids=None):\n        self.embeddings.extend(embeddings)\n        self.metadatas.extend(metadatas)\n        self.ids.extend(ids or [str(i) for i in range(len(metadatas))])",
        "file": "tests/test_docstring_incremental.py"
      },
      {
        "name": "query",
        "type": "method",
        "start_line": 28,
        "end_line": 29,
        "code": "def query(self, embedding, top_k):\n        return self.metadatas[:top_k]",
        "file": "tests/test_docstring_incremental.py"
      },
      {
        "name": "persist",
        "type": "method",
        "start_line": 31,
        "end_line": 32,
        "code": "def persist(self):\n        pass",
        "file": "tests/test_docstring_incremental.py"
      },
      {
        "name": "count",
        "type": "method",
        "start_line": 34,
        "end_line": 35,
        "code": "def count(self):\n        return len(self.metadatas)",
        "file": "tests/test_docstring_incremental.py"
      },
      {
        "name": "delete",
        "type": "method",
        "start_line": 37,
        "end_line": 43,
        "code": "def delete(self, ids):\n        for _id in ids:\n            if _id in self.ids:\n                idx = self.ids.index(_id)\n                self.ids.pop(idx)\n                self.embeddings.pop(idx)\n                self.metadatas.pop(idx)",
        "file": "tests/test_docstring_incremental.py"
      },
      {
        "name": "realistic_repo",
        "type": "function",
        "start_line": 46,
        "end_line": 50,
        "code": "def realistic_repo(tmp_path):\n    # Copy fixture repo to tmp so we can mutate files safely\n    workdir = tmp_path / \"repo\"\n    shutil.copytree(FIXTURE_REPO, workdir)\n    return Repository(str(workdir))",
        "file": "tests/test_docstring_incremental.py"
      },
      {
        "name": "_hash_file",
        "type": "function",
        "start_line": 53,
        "end_line": 54,
        "code": "def _hash_file(path: Path) -> str:\n    return hashlib.sha1(path.read_bytes()).hexdigest()",
        "file": "tests/test_docstring_incremental.py"
      },
      {
        "name": "test_incremental_indexing",
        "type": "function",
        "start_line": 57,
        "end_line": 90,
        "code": "def test_incremental_indexing(realistic_repo):\n    \"\"\"Initial build -> modify one file -> rebuild should only upsert that file's symbols.\"\"\"\n\n    summarizer = MagicMock()\n    summarizer.summarize_function.side_effect = lambda p, s: f\"F-{s}\"\n    summarizer.summarize_class.side_effect = lambda p, s: f\"C-{s}\"\n\n    embed_fn = lambda t: [float(len(t))]\n    backend = DummyBackend()\n    indexer = DocstringIndexer(realistic_repo, summarizer, embed_fn, backend=backend)\n\n    # 1. initial build\n    indexer.build(level=\"symbol\", force=True)\n    initial_count = backend.count()\n\n    # 2. mutate utils.py (append a comment)\n    utils_file = Path(realistic_repo.repo_path) / \"utils.py\"\n    utils_file.write_text(utils_file.read_text() + \"\\n# change\\n\")\n\n    indexer.build(level=\"symbol\")  # incremental\n\n    # summarizer should have been called for symbols in utils.py only\n    assert summarizer.summarize_function.call_count > 0\n    # naive check: count unchanged (upsert not duplicate)\n    assert backend.count() == initial_count\n\n    # 3. delete models/user.py -> rebuild\n    user_file = Path(realistic_repo.repo_path) / \"models\" / \"user.py\"\n    user_file.unlink()\n\n    indexer.build(level=\"symbol\")\n\n    # count should now be reduced (symbols from user.py removed)\n    assert backend.count() < initial_count",
        "file": "tests/test_docstring_incremental.py"
      }
    ],
    "examples/test_remote_repo.py": [
      {
        "name": "format_output",
        "type": "function",
        "start_line": 16,
        "end_line": 21,
        "code": "def format_output(title, content):\n    \"\"\"Helper function to format and print output\"\"\"\n    print(f\"\\n{'=' * 80}\")\n    print(f\"=== {title} ===\")\n    print(f\"{'=' * 80}\")\n    print(content)",
        "file": "examples/test_remote_repo.py"
      },
      {
        "name": "test_remote_repository",
        "type": "function",
        "start_line": 23,
        "end_line": 143,
        "code": "def test_remote_repository(repo_url):\n    \"\"\"Test Kit's ability to clone and analyze a remote GitHub repository\"\"\"\n    start_time = time.time()\n\n    # 1. Initialize Repository with GitHub URL (should auto-clone)\n    print(f\"Initializing Repository with GitHub URL: {repo_url}\")\n    print(\"This will automatically clone the repository to a local cache directory...\")\n    repo = Repository(repo_url)\n\n    print(f\"Repository initialized: {repo}\")\n    clone_time = time.time() - start_time\n    print(f\"Time to clone and initialize: {clone_time:.2f} seconds\")\n\n    # 2. Test File Structure Analysis\n    print(\"\\nAnalyzing repository structure...\")\n    file_tree = repo.get_file_tree()\n    print(f\"Found {len(file_tree)} files/directories\")\n\n    py_files = [f for f in file_tree if not f.get(\"is_dir\", False) and f[\"path\"].endswith(\".py\")]\n    print(f\"Python files: {len(py_files)}\")\n    if py_files:\n        format_output(\"Sample Python Files\", \"\\n\".join([f['path'] for f in py_files[:5]]))\n\n    # 3. Test Symbol Extraction\n    if py_files:\n        sample_file = py_files[0][\"path\"]\n        print(f\"\\nExtracting symbols from: {sample_file}\")\n        symbols = repo.extract_symbols(sample_file)\n        print(f\"Found {len(symbols)} symbols\")\n        if symbols:\n            format_output(\"Sample Symbols\", \"\\n\".join([f\"{s['type']}: {s['name']}\" for s in symbols[:5]]))\n\n    # 4. Test Text Search\n    print(\"\\nPerforming text search...\")\n    search_term = \"def\"\n    results = repo.search_text(search_term, file_pattern=\"*.py\")\n    print(f\"Found {len(results)} matches for '{search_term}'\")\n    if results:\n        format_output(\"Sample Search Results\",\n                     \"\\n\".join([f\"{r['file']}:{r['line_number']} - {r['line'].strip()}\" for r in results[:5]]))\n\n    # 5. Test LLM Integration (if API key is available)\n    api_key = os.environ.get(\"OPENAI_API_KEY\")\n    if api_key:\n        try:\n            print(\"\\nTesting LLM integration...\")\n            # Try to find a working model\n            available_models = [\"gpt-4o\", \"gpt-4\", \"gpt-3.5-turbo-0125\", \"gpt-3.5-turbo\"]\n            model_used = None\n\n            for model in available_models:\n                try:\n                    print(f\"Trying OpenAI model: {model}\")\n                    config = OpenAIConfig(\n                        api_key=api_key,\n                        model=model,\n                        temperature=0.3,\n                        max_tokens=500\n                    )\n\n                    # Test constructor only\n                    summarizer = repo.get_summarizer(config)\n                    model_used = model\n                    print(f\"Success! Using model: {model}\")\n                    break\n                except Exception as e:\n                    print(f\"  Error with model {model}: {str(e)}\")\n                    continue\n\n            if model_used:\n                # Find a small Python file to summarize\n                small_py_files = [f for f in py_files if f.get(\"size\", 100000) < 5000]\n                if small_py_files:\n                    sample_small_file = small_py_files[0][\"path\"]\n                    print(f\"\\nSummarizing file: {sample_small_file}\")\n                    file_summary = summarizer.summarize_file(sample_small_file)\n                    format_output(\"File Summary\", file_summary)\n\n                    # If there are symbols in this file, try to summarize one\n                    symbols = repo.extract_symbols(sample_small_file)\n                    functions = [s for s in symbols if s.get(\"type\").lower() in [\"function\", \"method\"]]\n                    if functions:\n                        func = functions[0]\n                        func_name = func[\"name\"]\n                        print(f\"\\nSummarizing function: {func_name}\")\n                        func_summary = summarizer.summarize_function(sample_small_file, func_name)\n                        format_output(\"Function Summary\", func_summary)\n        except Exception as e:\n            print(f\"Error during LLM integration: {str(e)}\")\n    else:\n        print(\"\\nSkipping LLM integration (no OpenAI API key found)\")\n\n    # 6. Test Dependency Analysis\n    print(\"\\nAnalyzing dependencies...\")\n    try:\n        analyzer = repo.get_dependency_analyzer()\n        dep_graph = analyzer.build_dependency_graph()\n\n        if hasattr(dep_graph, 'nodes') and callable(getattr(dep_graph, 'nodes')):\n            # NetworkX-like graph interface\n            node_count = len(dep_graph.nodes())\n            edge_count = len(dep_graph.edges())\n            print(f\"Dependency graph has {node_count} nodes and {edge_count} edges\")\n        else:\n            # Dictionary-based graph structure\n            nodes = list(dep_graph.keys())\n            edges = sum(len(deps) for deps in dep_graph.values())\n            print(f\"Dependency graph has {len(nodes)} nodes and approximately {edges} edges\")\n\n        cycles = analyzer.find_cycles()\n        if cycles:\n            print(f\"Found {len(cycles)} import cycles\")\n        else:\n            print(\"No import cycles found\")\n    except Exception as e:\n        print(f\"Error during dependency analysis: {str(e)}\")\n\n    # Done!\n    total_time = time.time() - start_time\n    print(f\"\\nTotal analysis time: {total_time:.2f} seconds\")\n    print(\"Remote repository analysis complete!\")",
        "file": "examples/test_remote_repo.py"
      }
    ],
    "examples/map_repository.py": [
      {
        "name": "map_repository",
        "type": "function",
        "start_line": 15,
        "end_line": 79,
        "code": "def map_repository(repo_path_or_url):\n    \"\"\"Map the repository and output its structure as JSON\"\"\"\n    start_time = time.time()\n\n    print(f\"Initializing repository from: {repo_path_or_url}\")\n    # Repository handles both local paths and GitHub URLs\n    repo = Repository(repo_path_or_url)\n\n    print(f\"Repository initialized in {time.time() - start_time:.2f} seconds\")\n    print(f\"Repository info: {repo}\")\n\n    # Get the file tree\n    print(\"\\nExtracting file tree...\")\n    file_tree = repo.get_file_tree()\n\n    # Extract symbols from all Python files\n    print(\"\\nExtracting symbols from Python files...\")\n    symbols_by_file = {}\n    py_files = [f[\"path\"] for f in file_tree if not f.get(\"is_dir\", False) and f[\"path\"].endswith(\".py\")]\n\n    for py_file in py_files:\n        print(f\"  Processing {py_file}\")\n        try:\n            symbols = repo.extract_symbols(py_file)\n            symbols_by_file[py_file] = symbols\n        except Exception as e:\n            print(f\"  Error extracting symbols from {py_file}: {str(e)}\")\n\n    # Create the complete repo map\n    repo_map = {\n        \"repository\": {\n            \"path\": repo_path_or_url,\n            \"is_remote\": repo_path_or_url.startswith(\"http\"),\n            \"file_count\": len([f for f in file_tree if not f.get(\"is_dir\", False)]),\n            \"directory_count\": len([f for f in file_tree if f.get(\"is_dir\", False)]),\n            \"python_file_count\": len(py_files)\n        },\n        \"file_tree\": [f[\"path\"] for f in file_tree],\n        \"symbols\": symbols_by_file\n    }\n\n    # Add dependency graph if we have Python files\n    if py_files:\n        try:\n            print(\"\\nAnalyzing dependencies...\")\n            analyzer = repo.get_dependency_analyzer()\n            graph = analyzer.build_dependency_graph()\n\n            # Add simplified dependency representation (source file \u2192 imported files)\n            dependencies = {}\n            for node, edges in graph.items():\n                if node in py_files:\n                    dependencies[node] = list(edges)\n\n            repo_map[\"dependencies\"] = dependencies\n        except Exception as e:\n            print(f\"Error analyzing dependencies: {str(e)}\")\n\n    total_time = time.time() - start_time\n    repo_map[\"metadata\"] = {\n        \"analysis_time\": total_time,\n        \"timestamp\": time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    }\n\n    return repo_map",
        "file": "examples/map_repository.py"
      }
    ],
    "examples/semantic_code_search.py": [
      {
        "name": "semantic_search",
        "type": "function",
        "start_line": 11,
        "end_line": 153,
        "code": "def semantic_search(repo_path: str, query: str, limit: int = 10) -> list:\n    \"\"\"\n    Perform semantic search on repository code.\n\n    Args:\n        repo_path: Path to the repository to search\n        query: Natural language query or code snippet\n        limit: Maximum number of results to return\n\n    Returns:\n        List of search results with file, relevance score, and code context\n    \"\"\"\n    print(f\"Initializing repository at {repo_path}...\")\n    repo = Repository(repo_path)\n\n    print(f\"Performing semantic search for: \\\"{query}\\\"\")\n    # Use the built-in semantic search capability of Kit\n    try:\n        # Try different parameter options (API might have changed)\n        try:\n            results = repo.search_semantic(query)\n        except TypeError:\n            try:\n                # Try with different parameter name\n                results = repo.search_semantic(query=query)\n            except:\n                # Try with no parameters (just use default behavior)\n                results = repo.search_semantic()\n\n        # Limit results manually\n        results = results[:limit]\n\n        # Add additional context to results\n        enhanced_results = []\n        for result in results:\n            file_path = result.get(\"file\")\n\n            # Get full context for search results\n            if \"symbol\" in result:\n                symbol_name = result[\"symbol\"]\n                symbol_info = repo.extract_symbols(file_path)\n                symbol_details = next((s for s in symbol_info if s[\"name\"] == symbol_name), None)\n                if symbol_details:\n                    result[\"code\"] = symbol_details.get(\"code\", \"Code not available\")\n                    result[\"type\"] = symbol_details.get(\"type\", \"Unknown\")\n\n            # If just a file result, get first few lines\n            elif not result.get(\"code\") and file_path:\n                try:\n                    content = repo.get_file_content(file_path)\n                    result[\"code\"] = \"\\n\".join(content.split(\"\\n\")[:10]) + \"\\n...\"\n                except Exception:\n                    result[\"code\"] = \"Unable to read file content\"\n\n            enhanced_results.append(result)\n\n        return enhanced_results\n    except Exception as e:\n        print(f\"Error during semantic search: {str(e)}\")\n        print(\"Falling back to keyword search...\")\n\n        # Use standard text search as fallback\n        try:\n            print(f\"Performing keyword search for: \\\"{query}\\\"\")\n            text_results = repo.search_text(query)\n\n            if text_results:\n                enhanced_results = []\n                for result in text_results[:limit]:\n                    file_path = result.get(\"file\")\n                    context = []\n\n                    if \"context_before\" in result:\n                        context.extend(result[\"context_before\"])\n\n                    if \"line\" in result:\n                        context.append(result[\"line\"])\n\n                    if \"context_after\" in result:\n                        context.extend(result[\"context_after\"])\n\n                    result[\"code\"] = \"\\n\".join(context)\n                    result[\"score\"] = 0.5  # Arbitrary score for text match\n                    enhanced_results.append(result)\n\n                return enhanced_results\n        except Exception as search_err:\n            print(f\"Text search also failed: {str(search_err)}\")\n            print(\"Falling back to simple file scan...\")\n\n        # Last resort - manual search\n        results = []\n\n        # Get all Python files\n        for file in repo.get_file_tree():\n            if file.get(\"is_dir\", False) or not file[\"path\"].endswith((\".py\", \".md\")):\n                continue\n\n            file_path = file[\"path\"]\n            try:\n                content = repo.get_file_content(file_path)\n\n                # Skip files that are too large\n                if len(content) > 100000:  # Skip files larger than ~100KB\n                    continue\n\n                # Check if query appears in content\n                if query.lower() in content.lower():\n                    # Find the relevant line with the query\n                    lines = content.split(\"\\n\")\n                    for i, line in enumerate(lines):\n                        if query.lower() in line.lower():\n                            context_start = max(0, i-2)\n                            context_end = min(len(lines), i+3)\n                            context = \"\\n\".join(lines[context_start:context_end])\n                            results.append({\n                                \"file\": file_path,\n                                \"line_number\": i+1,\n                                \"code\": context,\n                                \"score\": 0.5  # Arbitrary score for text match\n                            })\n                            break\n\n                # Check symbols if it's a Python file\n                if file_path.endswith(\".py\"):\n                    try:\n                        symbols = repo.extract_symbols(file_path)\n                        for symbol in symbols:\n                            if (query.lower() in symbol[\"name\"].lower() or\n                                (symbol.get(\"code\") and query.lower() in symbol[\"code\"].lower())):\n                                results.append({\n                                    \"file\": file_path,\n                                    \"symbol\": symbol[\"name\"],\n                                    \"type\": symbol[\"type\"],\n                                    \"code\": symbol.get(\"code\", \"No code available\"),\n                                    \"score\": 0.7  # Slightly higher score for symbol match\n                                })\n                    except Exception:\n                        pass  # Skip symbol extraction if it fails\n            except Exception:\n                pass  # Skip files that can't be read\n\n        return results[:limit]",
        "file": "examples/semantic_code_search.py"
      },
      {
        "name": "main",
        "type": "function",
        "start_line": 155,
        "end_line": 192,
        "code": "def main() -> None:\n    parser = argparse.ArgumentParser(description=\"Semantic code search using Kit.\")\n    parser.add_argument(\"--repo\", required=True, help=\"Path to the code repository\")\n    parser.add_argument(\"--query\", required=True, help=\"Search query (keyword or phrase)\")\n    parser.add_argument(\"--limit\", type=int, default=10, help=\"Maximum number of results\")\n    parser.add_argument(\"--output\", help=\"Output file for results (JSON)\")\n    args = parser.parse_args()\n\n    results = semantic_search(args.repo, args.query, args.limit)\n\n    if not results:\n        print(\"No matches found for your query.\")\n        sys.exit(0)\n\n    print(f\"\\nFound {len(results)} results:\")\n\n    # Print results in a readable format\n    for i, result in enumerate(results, 1):\n        print(f\"\\n--- Result {i} ---\")\n        print(f\"File: {result.get('file', 'Unknown')}\")\n        if \"symbol\" in result:\n            print(f\"Symbol: {result['symbol']} ({result.get('type', 'unknown type')})\")\n        if \"line_number\" in result:\n            print(f\"Line: {result['line_number']}\")\n        elif \"line\" in result and isinstance(result[\"line\"], int):\n            print(f\"Line: {result['line']}\")\n        if \"score\" in result:\n            print(f\"Relevance: {result['score']:.2f}\")\n        print(\"\\nCode:\")\n        print(\"```\")\n        print(result.get(\"code\", \"No code available\"))\n        print(\"```\")\n\n    # Save to file if requested\n    if args.output:\n        with open(args.output, \"w\") as f:\n            json.dump(results, f, indent=2)\n        print(f\"\\nResults also saved to {args.output}\")",
        "file": "examples/semantic_code_search.py"
      }
    ],
    "examples/test_llm_summarization.py": [
      {
        "name": "format_output",
        "type": "function",
        "start_line": 15,
        "end_line": 20,
        "code": "def format_output(title, content):\n    \"\"\"Helper function to format and print output\"\"\"\n    print(f\"\\n{'=' * 80}\")\n    print(f\"=== {title} ===\")\n    print(f\"{'=' * 80}\")\n    print(content)",
        "file": "examples/test_llm_summarization.py"
      },
      {
        "name": "test_summarization",
        "type": "function",
        "start_line": 22,
        "end_line": 109,
        "code": "def test_summarization(repo_path):\n    \"\"\"Test Kit's LLM integration with OpenAI summarization\"\"\"\n\n    # Ensure API key is set\n    api_key = os.environ.get(\"OPENAI_API_KEY\")\n    if not api_key:\n        print(\"Error: OPENAI_API_KEY environment variable not set.\")\n        print(\"Please set your OpenAI API key with:\")\n        print(\"  export OPENAI_API_KEY='your-api-key'\")\n        sys.exit(1)\n\n    # Load the repository\n    print(f\"Loading repository from: {repo_path}\")\n    repo = Repository(repo_path)\n\n    try:\n        # Try different models that might be available\n        available_models = [\"gpt-4o\", \"gpt-4\", \"gpt-3.5-turbo-0125\", \"gpt-3.5-turbo\"]\n        model_used = None\n\n        for model in available_models:\n            try:\n                print(f\"Trying OpenAI model: {model}\")\n                config = OpenAIConfig(\n                    api_key=api_key,\n                    model=model,\n                    temperature=0.3,\n                    max_tokens=500\n                )\n\n                # Test constructor only\n                summarizer = repo.get_summarizer(config)\n                model_used = model\n                print(f\"Success! Using model: {model}\")\n                break\n            except Exception as e:\n                print(f\"  Error with model {model}: {str(e)}\")\n                continue\n\n        if not model_used:\n            print(\"Failed to find a working OpenAI model. Please check your API key and access.\")\n            sys.exit(1)\n\n        # Start with a smaller file to test\n        small_file = \"src/kit/code_searcher.py\"\n        print(f\"\\nSummarizing small file: {small_file}\")\n        file_summary = summarizer.summarize_file(small_file)\n        format_output(\"File Summary\", file_summary)\n\n        # 2. Summarize a function (also try a smaller one)\n        function_file = \"src/kit/code_searcher.py\"\n        function_name = \"search_text\"\n        print(f\"\\nSummarizing function: {function_name} in {function_file}\")\n        function_summary = summarizer.summarize_function(function_file, function_name)\n        format_output(\"Function Summary\", function_summary)\n\n        # 3. Summarize a class (stick with smaller file)\n        class_file = \"src/kit/code_searcher.py\"\n        class_name = \"CodeSearcher\"\n        print(f\"\\nSummarizing class: {class_name} in {class_file}\")\n        class_summary = summarizer.summarize_class(class_file, class_name)\n        format_output(\"Class Summary\", class_summary)\n\n        print(\"\\nBasic tests completed successfully!\")\n\n        # Now try the larger file if the initial tests were successful\n        print(\"\\nTrying larger file - Repository class...\")\n        try:\n            # Try with Repository class - but this might hit token limits\n            large_file = \"src/kit/repository.py\"\n            repo_summary = summarizer.summarize_file(large_file)\n            format_output(\"Repository File Summary\", repo_summary)\n        except Exception as e:\n            print(f\"Warning: Could not summarize large file: {str(e)}\")\n            print(\"This is likely due to token limits and doesn't indicate an issue with Kit's functionality.\")\n\n        print(\"\\nAll LLM integration tests completed!\")\n\n    except ModuleNotFoundError:\n        print(\"\\nError: Required packages for LLM integration are not installed.\")\n        print(\"Please install kit with OpenAI support:\")\n        print(\"  uv pip install -e '.[openai]'\")\n        print(\"\\nOr install openai separately:\")\n        print(\"  uv pip install openai\")\n    except Exception as e:\n        print(f\"\\nError during summarization: {str(e)}\")\n        import traceback\n        traceback.print_exc()",
        "file": "examples/test_llm_summarization.py"
      }
    ],
    "examples/test_kit_capabilities.py": [
      {
        "name": "format_output",
        "type": "function",
        "start_line": 14,
        "end_line": 34,
        "code": "def format_output(title, content, limit=5):\n    \"\"\"Helper function to format and print output\"\"\"\n    print(f\"\\n{'=' * 80}\")\n    print(f\"=== {title} ===\")\n    print(f\"{'=' * 80}\")\n\n    if isinstance(content, list):\n        # Print list items with limit\n        for i, item in enumerate(content[:limit]):\n            if isinstance(item, dict):\n                print(f\"{i+1}. {json.dumps(item, indent=2)}\")\n            else:\n                print(f\"{i+1}. {item}\")\n        if len(content) > limit:\n            print(f\"... and {len(content) - limit} more items\")\n    elif isinstance(content, dict):\n        # Print dictionary items\n        print(json.dumps(content, indent=2))\n    else:\n        # Print string or other types\n        print(content)",
        "file": "examples/test_kit_capabilities.py"
      },
      {
        "name": "main",
        "type": "function",
        "start_line": 36,
        "end_line": 244,
        "code": "def main():\n    # Load the current repository\n    repo_path = os.path.dirname(os.path.abspath(__file__))\n    print(f\"Loading repository: {repo_path}\")\n    repo = Repository(repo_path)\n    print(f\"Repository: {repo}\")\n\n    # ===============================================================\n    # Capability 1: Code Structure Analysis\n    # ===============================================================\n\n    # Get file tree\n    file_tree = repo.get_file_tree()\n    py_files = [f for f in file_tree if f['path'].endswith('.py')][:5]\n    format_output(\"Capability 1: Code Structure Analysis - File Tree\", py_files)\n\n    # Extract symbols from repository.py\n    repo_file = \"src/kit/repository.py\"\n    symbols = repo.extract_symbols(repo_file)\n    format_output(\"Capability 1: Code Structure Analysis - Symbols\", symbols)\n\n    # ===============================================================\n    # Capability 2: Intelligent Code Search\n    # ===============================================================\n\n    # Basic text search\n    search_term = \"extract_symbols\"\n    results = repo.search_text(search_term, file_pattern=\"*.py\")\n    format_output(f\"Capability 2: Intelligent Code Search - Text Search for '{search_term}'\", results)\n\n    # Find symbol usages\n    symbol_usages = repo.find_symbol_usages(\"Repository\", symbol_type=\"class\")\n    format_output(\"Capability 2: Intelligent Code Search - Symbol Usages\", symbol_usages)\n\n    # Try semantic search if embeddings are available\n    try:\n        from openai import OpenAI\n        client = OpenAI()\n\n        # Create an embedding function that uses OpenAI\n        def embed_fn(text):\n            response = client.embeddings.create(\n                input=text,\n                model=\"text-embedding-ada-002\"\n            )\n            return response.data[0].embedding\n\n        # Perform semantic search\n        semantic_results = repo.search_semantic(\"how to extract code symbols\", embed_fn=embed_fn)\n        format_output(\"Capability 2: Intelligent Code Search - Semantic Search\", semantic_results)\n    except Exception as e:\n        print(f\"\\nSkipping semantic search (requires OpenAI API key): {e}\")\n\n    # ===============================================================\n    # Capability 3: Context Extraction\n    # ===============================================================\n\n    # Extract context around line\n    line_context = repo.extract_context_around_line(repo_file, 30)  # Line in Repository class\n    format_output(\"Capability 3: Context Extraction - Around Line\", line_context)\n\n    # Chunk file by lines\n    line_chunks = repo.chunk_file_by_lines(repo_file, max_lines=50)\n    format_output(\"Capability 3: Context Extraction - Chunk by Lines\", [f\"Chunk {i+1}: {len(chunk.split('\\\\n'))} lines\" for i, chunk in enumerate(line_chunks)])\n\n    # Chunk file by symbols\n    symbol_chunks = repo.chunk_file_by_symbols(repo_file)\n    format_output(\"Capability 3: Context Extraction - Chunk by Symbols\", symbol_chunks)\n\n    # Use context assembler\n    try:\n        assembler = repo.get_context_assembler()\n        # Get context (use the actual methods available)\n        context = {}\n        # Placeholder for actual context assembly\n        # The method used was incorrect - ContextAssembler doesn't have assemble_context\n        # It likely has other methods for building context\n        sample_content = repo.get_file_content(\"src/kit/repo_mapper.py\")[:500] + \"...[truncated]\"\n        context = {\n            \"query\": \"How does Kit extract symbols from code?\",\n            \"content\": sample_content,\n            \"source\": \"src/kit/repo_mapper.py\",\n            \"note\": \"Context assembly shown with sample content (method names were incorrect)\"\n        }\n        format_output(\"Capability 3: Context Extraction - Context Example\", context)\n    except Exception as e:\n        print(f\"\\nError with context assembler: {e}\")\n\n    # ===============================================================\n    # Capability 4: LLM Integration (Summaries)\n    # ===============================================================\n\n    print(\"\\n\" + \"=\" * 80)\n    print(\"=== Capability 4: LLM Integration - Summaries ===\")\n    print(\"=\" * 80)\n    print(\"This capability requires LLM API access (OpenAI, Anthropic, or Google).\")\n    print(\"To use this feature:\")\n    print(\"1. Install Kit with LLM extras: `pip install kit[openai]` or `kit[anthropic]` or `kit[google]`\")\n    print(\"2. Set appropriate API keys as environment variables\")\n    print(\"3. Create a configuration object:\")\n    print(\"   ```python\")\n    print(\"   from kit.summaries import OpenAIConfig\")\n    print(\"   config = OpenAIConfig(api_key='your_key_here')  # Or use env vars\")\n    print(\"   summarizer = repo.get_summarizer(config)\")\n    print(\"   ```\")\n    print(\"4. Use summarization methods:\")\n    print(\"   - summarizer.summarize_file(file_path)\")\n    print(\"   - summarizer.summarize_function(file_path, function_name)\")\n    print(\"   - summarizer.summarize_repo()\")\n\n    # ===============================================================\n    # Capability 5: Dependency Analysis\n    # ===============================================================\n\n    print(\"\\n\" + \"=\" * 80)\n    print(\"=== Capability 5: Dependency Analysis ===\")\n    print(\"=\" * 80)\n\n    try:\n        # Get dependency analyzer\n        analyzer = repo.get_dependency_analyzer()\n\n        # Build dependency graph\n        dep_graph = analyzer.build_dependency_graph()\n\n        # Handle the dependency graph based on its actual structure\n        if hasattr(dep_graph, 'nodes') and callable(getattr(dep_graph, 'nodes')):\n            # NetworkX-like graph interface\n            node_count = len(dep_graph.nodes())\n            edge_count = len(dep_graph.edges())\n\n            format_output(\"Capability 5: Dependency Analysis - Graph Stats\",\n                         f\"Dependency graph has {node_count} nodes and {edge_count} edges\")\n\n            # Find modules with most dependencies\n            modules_with_imports = {}\n            for node in dep_graph.nodes():\n                num_imports = len(list(dep_graph.successors(node)))\n                if num_imports > 0:\n                    modules_with_imports[node] = num_imports\n\n            # Sort by number of imports\n            most_dependencies = sorted(modules_with_imports.items(),\n                                     key=lambda x: x[1], reverse=True)[:5]\n            format_output(\"Capability 5: Dependency Analysis - Modules with Most Dependencies\",\n                         most_dependencies)\n\n            # Find most imported modules\n            modules_imported_by = {}\n            for node in dep_graph.nodes():\n                imported_by_count = len(list(dep_graph.predecessors(node)))\n                if imported_by_count > 0:\n                    modules_imported_by[node] = imported_by_count\n\n            # Sort by number of times imported\n            most_imported = sorted(modules_imported_by.items(),\n                                 key=lambda x: x[1], reverse=True)[:5]\n            format_output(\"Capability 5: Dependency Analysis - Most Imported Modules\",\n                         most_imported)\n        else:\n            # Dictionary-based graph structure\n            # Assume structure is {module: [dependencies]}\n            nodes = list(dep_graph.keys())\n            edges = sum(len(deps) for deps in dep_graph.values())\n\n            format_output(\"Capability 5: Dependency Analysis - Graph Stats\",\n                         f\"Dependency graph has {len(nodes)} nodes and approximately {edges} edges\")\n\n            # Find modules with most dependencies (outgoing)\n            modules_with_imports = {module: len(deps) for module, deps in dep_graph.items() if deps}\n            most_dependencies = sorted(modules_with_imports.items(),\n                                      key=lambda x: x[1], reverse=True)[:5]\n            format_output(\"Capability 5: Dependency Analysis - Modules with Most Dependencies\",\n                         most_dependencies)\n\n            # Find most imported modules (incoming)\n            modules_imported_by = {}\n            for module, deps in dep_graph.items():\n                for dep in deps:\n                    if dep in modules_imported_by:\n                        modules_imported_by[dep] += 1\n                    else:\n                        modules_imported_by[dep] = 1\n\n            most_imported = sorted(modules_imported_by.items(),\n                                  key=lambda x: x[1], reverse=True)[:5]\n            format_output(\"Capability 5: Dependency Analysis - Most Imported Modules\",\n                         most_imported)\n\n        # Find cycles - this method should work regardless of graph structure\n        cycles = analyzer.find_cycles()\n        if cycles:\n            format_output(\"Capability 5: Dependency Analysis - Import Cycles\", cycles)\n        else:\n            print(\"\\nNo import cycles found in the codebase.\")\n\n        print(\"\\nTo visualize the dependency graph, you could use:\")\n        print(\"analyzer.export_dependency_graph(output_format=\\\"dot\\\", output_path=\\\"dependencies.dot\\\")\")\n        print(\"Then convert the dot file to an image: dot -Tpng dependencies.dot -o dependencies.png\")\n\n    except Exception as e:\n        print(f\"\\nError with dependency analysis: {e}\")\n        print(f\"Type of dep_graph: {type(dep_graph)}\")\n        if isinstance(dep_graph, dict):\n            print(f\"Keys in dep_graph: {list(dep_graph.keys())[:5]} (showing first 5)\")\n\n    # Export graph (commented out as it would create files)\n    # analyzer.export_dependency_graph(output_format=\"dot\", output_path=\"dependencies.dot\")\n    # print(\"\\nDependency graph exported to dependencies.dot\")",
        "file": "examples/test_kit_capabilities.py"
      }
    ],
    "scripts/benchmark.py": [
      {
        "name": "main",
        "type": "function",
        "start_line": 3,
        "end_line": 16,
        "code": "def main():\n    import argparse\n    parser = argparse.ArgumentParser(description=\"Benchmark kit repo indexing.\")\n    parser.add_argument(\"repo\", nargs=\"?\", default=\".\", help=\"Path to repo root (default: .)\")\n    args = parser.parse_args()\n    repo = Repo(args.repo)\n\n    print(f\"Indexing repo at {args.repo} ...\")\n    start = time.time()\n    idx = repo.index()\n    elapsed = time.time() - start\n    num_files = len(idx[\"file_tree\"])\n    num_symbols = sum(len(syms) for syms in idx[\"symbols\"].values())\n    print(f\"Indexed {num_files} files, {num_symbols} symbols in {elapsed:.2f} seconds.\")",
        "file": "scripts/benchmark.py"
      }
    ],
    "scripts/index.py": [],
    "src/kit/llm_context.py": [
      {
        "name": "ContextAssembler",
        "type": "class",
        "start_line": 16,
        "end_line": 68,
        "code": "class ContextAssembler:\n    \"\"\"Collects pieces of context and spits out a prompt blob.\n\n    Parameters\n    ----------\n    repo\n        A :class:`kit.repository.Repository` object representing the codebase\n        we want to reason about. The assembler uses it to fetch file content\n        and (in the future) symbol relationships.\n    title\n        Optional global title prepended to the context (not used by default).\n    \"\"\"\n\n    def __init__(self, repo: Repository, *, title: Optional[str] = None) -> None:\n        self.repo = repo\n        self._sections: List[str] = []\n        if title:\n            self._sections.append(f\"# {title}\\n\")\n\n    def add_diff(self, diff: str) -> None:\n        \"\"\"Add a raw git diff section.\"\"\"\n        if not diff.strip():\n            return\n        self._sections.append(\"## Diff\\n```diff\\n\" + diff.strip() + \"\\n```\")\n\n    def add_file(self, file_path: str, *, highlight_changes: bool = False) -> None:\n        \"\"\"Embed full file content.\n\n        If *highlight_changes* is true we still just inline raw content \u2013\n        markup is left to the caller/LLM.\n        \"\"\"\n        try:\n            code = self.repo.get_file_content(file_path)\n        except FileNotFoundError:\n            return\n        lang = Path(file_path).suffix.lstrip(\".\") or \"text\"\n        header = f\"## {file_path} (full)\" if not highlight_changes else f\"## {file_path} (with changes highlighted)\"\n        self._sections.append(f\"{header}\\n```{lang}\\n{code}\\n```\")\n\n    def add_search_results(self, results: Sequence[Dict[str, Any]], *, query: str) -> None:\n        \"\"\"Append semantic search matches to the context.\"\"\"\n        if not results:\n            return\n        blob = [f\"## Semantic search for: {query}\"]\n        for i, res in enumerate(results, 1):\n            code = res.get(\"code\") or res.get(\"snippet\") or \"\"\n            file = res.get(\"file\", f\"result_{i}\")\n            blob.append(f\"### {file}\\n```\\n{code}\\n```\")\n        self._sections.append(\"\\n\".join(blob))\n\n    def format_context(self) -> str:\n        \"\"\"Return the accumulated context.\"\"\"\n        return \"\\n\\n\".join(self._sections)",
        "file": "src/kit/llm_context.py"
      },
      {
        "name": "__init__",
        "type": "method",
        "start_line": 29,
        "end_line": 33,
        "code": "def __init__(self, repo: Repository, *, title: Optional[str] = None) -> None:\n        self.repo = repo\n        self._sections: List[str] = []\n        if title:\n            self._sections.append(f\"# {title}\\n\")",
        "file": "src/kit/llm_context.py"
      },
      {
        "name": "add_diff",
        "type": "method",
        "start_line": 35,
        "end_line": 39,
        "code": "def add_diff(self, diff: str) -> None:\n        \"\"\"Add a raw git diff section.\"\"\"\n        if not diff.strip():\n            return\n        self._sections.append(\"## Diff\\n```diff\\n\" + diff.strip() + \"\\n```\")",
        "file": "src/kit/llm_context.py"
      },
      {
        "name": "add_file",
        "type": "method",
        "start_line": 41,
        "end_line": 53,
        "code": "def add_file(self, file_path: str, *, highlight_changes: bool = False) -> None:\n        \"\"\"Embed full file content.\n\n        If *highlight_changes* is true we still just inline raw content \u2013\n        markup is left to the caller/LLM.\n        \"\"\"\n        try:\n            code = self.repo.get_file_content(file_path)\n        except FileNotFoundError:\n            return\n        lang = Path(file_path).suffix.lstrip(\".\") or \"text\"\n        header = f\"## {file_path} (full)\" if not highlight_changes else f\"## {file_path} (with changes highlighted)\"\n        self._sections.append(f\"{header}\\n```{lang}\\n{code}\\n```\")",
        "file": "src/kit/llm_context.py"
      },
      {
        "name": "add_search_results",
        "type": "method",
        "start_line": 55,
        "end_line": 64,
        "code": "def add_search_results(self, results: Sequence[Dict[str, Any]], *, query: str) -> None:\n        \"\"\"Append semantic search matches to the context.\"\"\"\n        if not results:\n            return\n        blob = [f\"## Semantic search for: {query}\"]\n        for i, res in enumerate(results, 1):\n            code = res.get(\"code\") or res.get(\"snippet\") or \"\"\n            file = res.get(\"file\", f\"result_{i}\")\n            blob.append(f\"### {file}\\n```\\n{code}\\n```\")\n        self._sections.append(\"\\n\".join(blob))",
        "file": "src/kit/llm_context.py"
      },
      {
        "name": "format_context",
        "type": "method",
        "start_line": 66,
        "end_line": 68,
        "code": "def format_context(self) -> str:\n        \"\"\"Return the accumulated context.\"\"\"\n        return \"\\n\\n\".join(self._sections)",
        "file": "src/kit/llm_context.py"
      }
    ],
    "src/kit/docstring_indexer.py": [
      {
        "name": "_process_symbol_task",
        "type": "function",
        "start_line": 35,
        "end_line": 107,
        "code": "def _process_symbol_task(\n    path_str: str,\n    symbol_info: Dict[str, Any],\n    summarizer_instance: Summarizer,\n    embed_fn_instance: Callable[[str], List[float]],\n    current_cache: Dict[str, Dict[str, str]] # Pass for reading cache state\n) -> Dict[str, Any]:\n    \"\"\"Processes a single symbol: summarize, embed, handle caching and errors.\"\"\"\n    symbol_name = symbol_info.get(\"name\")\n    symbol_type = symbol_info.get(\"type\")\n\n    display_name = symbol_info.get(\"node_path\", symbol_name)\n    # Ensure doc_id is formed even if symbol_name or display_name is None initially for error reporting\n    doc_id_prefix = f\"{path_str}::{display_name if display_name else 'unknown_symbol'}\"\n\n    if not symbol_name or not symbol_type:\n        logger.debug(f\"Symbol in {path_str} missing name or type, node_path: {display_name}. Skipping.\")\n        return {\"status\": \"skipped_no_name_type\", \"doc_id\": doc_id_prefix}\n\n    doc_id = f\"{path_str}::{display_name}\" # Re-assign with definite display_name\n    symbol_code = symbol_info.get(\"code\", \"\")\n\n    if not symbol_code:\n        logger.warning(f\"Could not retrieve code for symbol {display_name} in {path_str}, skipping.\")\n        return {\"status\": \"skipped_no_code\", \"doc_id\": doc_id}\n\n    try:\n        symbol_hash = hashlib.sha1(symbol_code.encode(\"utf-8\", \"ignore\")).hexdigest()\n        if current_cache.get(doc_id, {}).get(\"hash\") == symbol_hash:\n            logger.debug(f\"Symbol {doc_id} unchanged (hash: {symbol_hash}), skipping.\")\n            return {\"status\": \"cached\", \"doc_id\": doc_id}\n\n        summary_text = None # Initialize summary_text\n        if symbol_type.upper() == \"FUNCTION\" or symbol_type.upper() == \"METHOD\":\n            summary_text = summarizer_instance.summarize_function(path_str, display_name)\n        elif symbol_type.upper() == \"CLASS\":\n            summary_text = summarizer_instance.summarize_class(path_str, display_name)\n        else:\n            logger.debug(f\"Symbol {doc_id} has unsupported type '{symbol_type}'. Skipping summarization.\")\n            return {\"status\": \"skipped_unsupported_type\", \"doc_id\": doc_id, \"symbol_type\": symbol_type}\n\n        # Check if summarizer returned a valid string\n        if not isinstance(summary_text, str):\n            logger.warning(f\"Summarizer did not return a valid string for symbol {display_name} in {path_str} (type: {symbol_type}). Received: {type(summary_text)}. Skipping.\")\n            return {\"status\": \"skipped_invalid_summary_type\", \"doc_id\": doc_id}\n\n        if not summary_text.strip():\n            logger.warning(f\"Empty summary for symbol {display_name} in {path_str} (type: {symbol_type}), skipping.\")\n            return {\"status\": \"skipped_empty_summary\", \"doc_id\": doc_id}\n\n        emb = embed_fn_instance(summary_text)\n        meta = {\n            \"file_path\": path_str,\n            \"symbol_name\": display_name,\n            \"symbol_type\": symbol_type,\n            \"summary\": summary_text,\n            \"level\": \"symbol\"\n        }\n        return {\n            \"status\": \"processed\",\n            \"doc_id\": doc_id,\n            \"embedding\": emb,\n            \"metadata\": meta,\n            \"hash\": symbol_hash,\n            \"summary_for_embedding\": summary_text\n        }\n    except ValueError as ve:\n        logger.warning(f\"Skipping symbol {display_name} in {path_str} due to ValueError: {ve}\")\n        return {\"status\": \"error\", \"doc_id\": doc_id, \"error_type\": \"ValueError\", \"message\": str(ve)}\n    except Exception as exc:\n        # Log less info from worker to keep main logs cleaner, but still indicate error\n        logger.error(f\"Failed to process symbol {display_name} in {path_str}: {type(exc).__name__} - {exc}\") \n        return {\"status\": \"error\", \"doc_id\": doc_id, \"error_type\": str(type(exc).__name__), \"message\": str(exc)}",
        "file": "src/kit/docstring_indexer.py"
      },
      {
        "name": "DocstringIndexer",
        "type": "class",
        "start_line": 110,
        "end_line": 376,
        "code": "class DocstringIndexer:\n    \"\"\"Builds a vector index of LLM-generated docstrings (summaries).\n\n    A thin wrapper around an existing VectorDB backend.  On ``build()``, it:\n    1. walks the repository file tree,\n    2. calls :py:meth:`Summarizer.summarize_file` to obtain a concise summary,\n    3. embeds that summary via *embed_fn*, and\n    4. stores the embedding + metadata in *backend*.\n\n    Parameters\n    ----------\n    repo\n        Active :py:class:`kit.repository.Repository`.\n    summarizer\n        Configured :py:class:`kit.summaries.Summarizer`.\n    embed_fn\n        Callable that converts text \u2192 embedding vector (list[float]).\n    backend\n        Optional vector-DB backend, defaults to :class:`ChromaDBBackend`.\n    persist_dir\n        Where on disk to store backend data (if backend honors persistence).\n    \"\"\"\n\n    def __init__(\n        self,\n        repo: Repository,\n        summarizer: Summarizer,\n        embed_fn: Optional[Callable[[str], List[float]]] = None, \n        *,\n        backend: Optional[VectorDBBackend] = None,\n        persist_dir: Optional[str] = None,\n    ) -> None:\n        self.repo = repo\n        self.summarizer = summarizer\n\n        if embed_fn:\n            self.embed_fn = embed_fn\n        else:\n            try:\n                from sentence_transformers import SentenceTransformer \n                \n                _st_model = SentenceTransformer(\"all-MiniLM-L6-v2\") \n                \n                def default_embed_fn(text_to_embed: str) -> List[float]:\n                    embedding_vector = _st_model.encode(text_to_embed)\n                    return embedding_vector.tolist()\n\n                self.embed_fn = default_embed_fn\n                # Example logging:\n                # logger.info(\"No embed_fn provided. Using default SentenceTransformer ('all-MiniLM-L6-v2').\")\n            except ImportError:\n                raise ImportError(\n                    \"The 'sentence-transformers' library is required to use the default embedding function. \"\n                    \"Please install it (e.g., 'pip install kit[default-embeddings]' or 'pip install sentence-transformers') \"\n                    \"or provide a custom 'embed_fn' to DocstringIndexer.\"\n                )\n            except Exception as e:\n                raise RuntimeError(\n                    f\"Failed to initialize the default SentenceTransformer embedding function: {e}. \"\n                    \"Please check your internet connection, ensure the model name is correct, \"\n                    \"or provide a custom 'embed_fn'.\"\n                )\n\n        # Updated persist_dir logic:\n        # Default to a .kit_cache directory within the repository path if persist_dir is not given.\n        if persist_dir:\n            self.persist_dir = persist_dir\n        else:\n            # Check if repo.repo_path is available and valid, otherwise default to a generic location or raise error\n            if hasattr(self.repo, 'repo_path') and self.repo.repo_path:\n                 self.persist_dir = os.path.join(self.repo.repo_path, \".kit_cache\", \"docstring_db\")\n            else:\n                # Fallback if repo_path is not available (should ideally not happen with a valid Repo object)\n                self.persist_dir = os.path.join(os.getcwd(), \".kit_cache\", \"docstring_db_generic\") \n                # logger.warning(f\"Repository path not found. Defaulting persist_dir to {self.persist_dir}\")\n\n        # Ensure the persist_dir exists before ChromaDBBackend tries to use it\n        if not os.path.exists(self.persist_dir):\n            os.makedirs(self.persist_dir, exist_ok=True)\n            \n        # Updated backend instantiation: explicitly pass path and a clearer collection_name\n        str_persist_dir = str(self.persist_dir)\n        self.backend: VectorDBBackend = backend or ChromaDBBackend(\n            persist_dir=str_persist_dir,\n            collection_name=\"kit_docstring_index\"\n        )\n\n    def build(self, force: bool = False, level: str = \"symbol\", file_extensions: Optional[List[str]] = None) -> None:\n        \"\"\"(Re)build the docstring index.\n\n        If *force* is ``False`` and the backend already contains data we do\n        nothing.\n\n        Parameters\n        ----------\n        force\n            If ``True``, rebuild even if data exists.\n        level\n            Granularity of indexing: \"file\" or \"symbol\".\n        file_extensions\n            Optional list of file extensions to include (e.g., [\".py\", \".js\"]).\n            If None, all files are considered (respecting .gitignore).\n        \"\"\"\n        meta_path = os.path.join(self.persist_dir, \"meta.json\")\n        os.makedirs(self.persist_dir, exist_ok=True)\n        if os.path.exists(meta_path):\n            with open(meta_path, \"r\", encoding=\"utf-8\") as fp:\n                cache: Dict[str, Dict[str, str]] = json.load(fp)\n        else:\n            cache = {}\n\n        if force and cache:\n            try:\n                self.backend.delete(ids=list(cache.keys()))\n            except Exception:\n                pass\n            cache = {}\n\n        all_files = [f[\"path\"] for f in self.repo.get_file_tree() if not f.get(\"is_dir\", False)]\n\n        if file_extensions:\n            files_to_process = [\n                fp for fp in all_files\n                if any(fp.endswith(ext) for ext in file_extensions)\n            ]\n        else:\n            files_to_process = all_files\n\n        if not files_to_process:\n            logger.warning(\"No files found to index after extension filtering.\")\n            return\n\n        embeddings: List[List[float]] = []\n        metadatas: List[Dict[str, Any]] = []\n        ids: List[str] = []\n        seen_ids: set[str] = set()\n\n        from concurrent.futures import ThreadPoolExecutor, as_completed, Future\n\n        try:\n            max_workers = int(os.environ.get(\"KIT_INDEXER_MAX_WORKERS\", os.cpu_count() or 4))\n        except TypeError: # os.cpu_count() can return None\n            max_workers = 4 \n        logger.info(f\"Using up to {max_workers} workers for symbol processing.\")\n\n        for path in tqdm(files_to_process, desc=f\"Indexing ({level} level)\"):\n            if level == \"file\":\n                abs_file_path = Path(self.repo.repo_path) / path # Create absolute path\n                try:\n                    summary = self.summarizer.summarize_file(path)\n                    if not summary.strip():\n                        logger.warning(f\"Empty summary for file {path}, skipping.\")\n                        continue\n                    doc_id = path\n                    file_hash = hashlib.sha1(abs_file_path.read_bytes()).hexdigest()\n                    if cache.get(doc_id, {}).get(\"hash\") == file_hash:\n                        seen_ids.add(doc_id)\n                        continue\n                    meta = {\"file_path\": path, \"summary\": summary, \"level\": \"file\"}\n                    emb = self.embed_fn(summary)\n                    embeddings.append(emb)\n                    metadatas.append(meta)\n                    ids.append(doc_id)\n                    cache[doc_id] = {\"hash\": file_hash}\n                    seen_ids.add(doc_id)\n                except Exception as exc:\n                    logger.error(f\"Failed to summarize file {path}: {exc}\", exc_info=True)\n                    continue\n            elif level == \"symbol\":\n                try:\n                    symbols_in_file = self.repo.extract_symbols(path)\n                except Exception as exc:\n                    logger.error(f\"Failed to extract symbols from {path}: {exc}\", exc_info=True)\n                    continue\n\n                if not symbols_in_file:\n                    logger.debug(f\"No symbols found in {path}, skipping.\")\n                    continue\n\n                # Lists to store results for the current file\n                file_embeddings: List[List[float]] = []\n                file_metadatas: List[Dict[str, Any]] = []\n                file_ids: List[str] = []\n                file_cache_updates: Dict[str, Dict[str, str]] = {}\n                file_seen_ids: set[str] = set()\n\n                with ThreadPoolExecutor(max_workers=max_workers) as executor:\n                    futures: List[Future] = []\n                    for s_info in symbols_in_file:\n                        futures.append(executor.submit(\n                            _process_symbol_task,\n                            path_str=path,\n                            symbol_info=s_info,\n                            summarizer_instance=self.summarizer,\n                            embed_fn_instance=self.embed_fn,\n                            current_cache=cache # Pass read-only view of cache\n                        ))\n\n                    for future in as_completed(futures):\n                        try:\n                            result = future.result()\n                            status = result.get(\"status\")\n                            doc_id_from_result = result.get(\"doc_id\", \"unknown_doc_id_in_future\")\n\n                            if status == \"processed\":\n                                file_embeddings.append(result[\"embedding\"])\n                                file_metadatas.append(result[\"metadata\"])\n                                file_ids.append(doc_id_from_result)\n                                file_cache_updates[doc_id_from_result] = {\"hash\": result[\"hash\"]}\n                                file_seen_ids.add(doc_id_from_result)\n                            elif status == \"cached\":\n                                file_seen_ids.add(doc_id_from_result)\n                            elif status == \"error\":\n                                logger.debug(f\"Symbol processing for {doc_id_from_result} resulted in status '{status}': {result.get('message')}\")\n                            elif status in [\"skipped_no_name_type\", \"skipped_no_code\", \"skipped_unsupported_type\", \"skipped_empty_summary\", \"skipped_invalid_summary_type\"]:\n                                logger.debug(f\"Symbol {doc_id_from_result} was skipped: {status}\")\n                            else:\n                                logger.warning(f\"Unknown status '{status}' from _process_symbol_task for {doc_id_from_result}\")\n                        except Exception as exc_in_future:\n                            logger.error(f\"Exception retrieving result from future for a symbol in {path}: {exc_in_future}\", exc_info=True)\n                \n                # After processing all symbols for the current file, extend the main lists\n                if file_embeddings: # Only extend if there's something to add\n                    embeddings.extend(file_embeddings)\n                    metadatas.extend(file_metadatas)\n                    ids.extend(file_ids)\n                    cache.update(file_cache_updates) # Update the main cache\n                    seen_ids.update(file_seen_ids)    # Update main seen_ids\n                elif file_seen_ids: # Still update seen_ids if items were cached\n                    seen_ids.update(file_seen_ids)\n\n            else:\n                raise ValueError(f\"Invalid indexing level: {level}. Choose 'file' or 'symbol'.\")\n\n        # remove orphans\n        orphan_ids = set(cache.keys()) - seen_ids\n        if orphan_ids:\n            try:\n                self.backend.delete(ids=list(orphan_ids))\n            except Exception:\n                pass\n            for oid in orphan_ids:\n                cache.pop(oid, None)\n\n        if embeddings:\n            logger.info(f\"Adding {len(embeddings)} embeddings to the index.\")\n            self.backend.add(embeddings=embeddings, metadatas=metadatas, ids=ids) # Chroma needs 'ids'\n            self.backend.persist()\n            logger.info(\"Index build complete and persisted.\")\n        else:\n            logger.warning(\"No embeddings were generated during indexing.\")\n\n        # persist cache\n        with open(meta_path, \"w\", encoding=\"utf-8\") as fp:\n            json.dump(cache, fp)\n\n    def get_searcher(self) -> 'SummarySearcher':\n        \"\"\"\n        Returns a SummarySearcher instance configured with this indexer.\n\n        This provides a convenient way to get a query interface after the\n        indexer has been built or loaded.\n\n        Returns:\n            SummarySearcher: An instance of SummarySearcher ready to query this indexer.\n        \"\"\"\n        return SummarySearcher(indexer=self)",
        "file": "src/kit/docstring_indexer.py"
      },
      {
        "name": "__init__",
        "type": "method",
        "start_line": 133,
        "end_line": 195,
        "code": "def __init__(\n        self,\n        repo: Repository,\n        summarizer: Summarizer,\n        embed_fn: Optional[Callable[[str], List[float]]] = None, \n        *,\n        backend: Optional[VectorDBBackend] = None,\n        persist_dir: Optional[str] = None,\n    ) -> None:\n        self.repo = repo\n        self.summarizer = summarizer\n\n        if embed_fn:\n            self.embed_fn = embed_fn\n        else:\n            try:\n                from sentence_transformers import SentenceTransformer \n                \n                _st_model = SentenceTransformer(\"all-MiniLM-L6-v2\") \n                \n                def default_embed_fn(text_to_embed: str) -> List[float]:\n                    embedding_vector = _st_model.encode(text_to_embed)\n                    return embedding_vector.tolist()\n\n                self.embed_fn = default_embed_fn\n                # Example logging:\n                # logger.info(\"No embed_fn provided. Using default SentenceTransformer ('all-MiniLM-L6-v2').\")\n            except ImportError:\n                raise ImportError(\n                    \"The 'sentence-transformers' library is required to use the default embedding function. \"\n                    \"Please install it (e.g., 'pip install kit[default-embeddings]' or 'pip install sentence-transformers') \"\n                    \"or provide a custom 'embed_fn' to DocstringIndexer.\"\n                )\n            except Exception as e:\n                raise RuntimeError(\n                    f\"Failed to initialize the default SentenceTransformer embedding function: {e}. \"\n                    \"Please check your internet connection, ensure the model name is correct, \"\n                    \"or provide a custom 'embed_fn'.\"\n                )\n\n        # Updated persist_dir logic:\n        # Default to a .kit_cache directory within the repository path if persist_dir is not given.\n        if persist_dir:\n            self.persist_dir = persist_dir\n        else:\n            # Check if repo.repo_path is available and valid, otherwise default to a generic location or raise error\n            if hasattr(self.repo, 'repo_path') and self.repo.repo_path:\n                 self.persist_dir = os.path.join(self.repo.repo_path, \".kit_cache\", \"docstring_db\")\n            else:\n                # Fallback if repo_path is not available (should ideally not happen with a valid Repo object)\n                self.persist_dir = os.path.join(os.getcwd(), \".kit_cache\", \"docstring_db_generic\") \n                # logger.warning(f\"Repository path not found. Defaulting persist_dir to {self.persist_dir}\")\n\n        # Ensure the persist_dir exists before ChromaDBBackend tries to use it\n        if not os.path.exists(self.persist_dir):\n            os.makedirs(self.persist_dir, exist_ok=True)\n            \n        # Updated backend instantiation: explicitly pass path and a clearer collection_name\n        str_persist_dir = str(self.persist_dir)\n        self.backend: VectorDBBackend = backend or ChromaDBBackend(\n            persist_dir=str_persist_dir,\n            collection_name=\"kit_docstring_index\"\n        )",
        "file": "src/kit/docstring_indexer.py"
      },
      {
        "name": "build",
        "type": "method",
        "start_line": 197,
        "end_line": 364,
        "code": "def build(self, force: bool = False, level: str = \"symbol\", file_extensions: Optional[List[str]] = None) -> None:\n        \"\"\"(Re)build the docstring index.\n\n        If *force* is ``False`` and the backend already contains data we do\n        nothing.\n\n        Parameters\n        ----------\n        force\n            If ``True``, rebuild even if data exists.\n        level\n            Granularity of indexing: \"file\" or \"symbol\".\n        file_extensions\n            Optional list of file extensions to include (e.g., [\".py\", \".js\"]).\n            If None, all files are considered (respecting .gitignore).\n        \"\"\"\n        meta_path = os.path.join(self.persist_dir, \"meta.json\")\n        os.makedirs(self.persist_dir, exist_ok=True)\n        if os.path.exists(meta_path):\n            with open(meta_path, \"r\", encoding=\"utf-8\") as fp:\n                cache: Dict[str, Dict[str, str]] = json.load(fp)\n        else:\n            cache = {}\n\n        if force and cache:\n            try:\n                self.backend.delete(ids=list(cache.keys()))\n            except Exception:\n                pass\n            cache = {}\n\n        all_files = [f[\"path\"] for f in self.repo.get_file_tree() if not f.get(\"is_dir\", False)]\n\n        if file_extensions:\n            files_to_process = [\n                fp for fp in all_files\n                if any(fp.endswith(ext) for ext in file_extensions)\n            ]\n        else:\n            files_to_process = all_files\n\n        if not files_to_process:\n            logger.warning(\"No files found to index after extension filtering.\")\n            return\n\n        embeddings: List[List[float]] = []\n        metadatas: List[Dict[str, Any]] = []\n        ids: List[str] = []\n        seen_ids: set[str] = set()\n\n        from concurrent.futures import ThreadPoolExecutor, as_completed, Future\n\n        try:\n            max_workers = int(os.environ.get(\"KIT_INDEXER_MAX_WORKERS\", os.cpu_count() or 4))\n        except TypeError: # os.cpu_count() can return None\n            max_workers = 4 \n        logger.info(f\"Using up to {max_workers} workers for symbol processing.\")\n\n        for path in tqdm(files_to_process, desc=f\"Indexing ({level} level)\"):\n            if level == \"file\":\n                abs_file_path = Path(self.repo.repo_path) / path # Create absolute path\n                try:\n                    summary = self.summarizer.summarize_file(path)\n                    if not summary.strip():\n                        logger.warning(f\"Empty summary for file {path}, skipping.\")\n                        continue\n                    doc_id = path\n                    file_hash = hashlib.sha1(abs_file_path.read_bytes()).hexdigest()\n                    if cache.get(doc_id, {}).get(\"hash\") == file_hash:\n                        seen_ids.add(doc_id)\n                        continue\n                    meta = {\"file_path\": path, \"summary\": summary, \"level\": \"file\"}\n                    emb = self.embed_fn(summary)\n                    embeddings.append(emb)\n                    metadatas.append(meta)\n                    ids.append(doc_id)\n                    cache[doc_id] = {\"hash\": file_hash}\n                    seen_ids.add(doc_id)\n                except Exception as exc:\n                    logger.error(f\"Failed to summarize file {path}: {exc}\", exc_info=True)\n                    continue\n            elif level == \"symbol\":\n                try:\n                    symbols_in_file = self.repo.extract_symbols(path)\n                except Exception as exc:\n                    logger.error(f\"Failed to extract symbols from {path}: {exc}\", exc_info=True)\n                    continue\n\n                if not symbols_in_file:\n                    logger.debug(f\"No symbols found in {path}, skipping.\")\n                    continue\n\n                # Lists to store results for the current file\n                file_embeddings: List[List[float]] = []\n                file_metadatas: List[Dict[str, Any]] = []\n                file_ids: List[str] = []\n                file_cache_updates: Dict[str, Dict[str, str]] = {}\n                file_seen_ids: set[str] = set()\n\n                with ThreadPoolExecutor(max_workers=max_workers) as executor:\n                    futures: List[Future] = []\n                    for s_info in symbols_in_file:\n                        futures.append(executor.submit(\n                            _process_symbol_task,\n                            path_str=path,\n                            symbol_info=s_info,\n                            summarizer_instance=self.summarizer,\n                            embed_fn_instance=self.embed_fn,\n                            current_cache=cache # Pass read-only view of cache\n                        ))\n\n                    for future in as_completed(futures):\n                        try:\n                            result = future.result()\n                            status = result.get(\"status\")\n                            doc_id_from_result = result.get(\"doc_id\", \"unknown_doc_id_in_future\")\n\n                            if status == \"processed\":\n                                file_embeddings.append(result[\"embedding\"])\n                                file_metadatas.append(result[\"metadata\"])\n                                file_ids.append(doc_id_from_result)\n                                file_cache_updates[doc_id_from_result] = {\"hash\": result[\"hash\"]}\n                                file_seen_ids.add(doc_id_from_result)\n                            elif status == \"cached\":\n                                file_seen_ids.add(doc_id_from_result)\n                            elif status == \"error\":\n                                logger.debug(f\"Symbol processing for {doc_id_from_result} resulted in status '{status}': {result.get('message')}\")\n                            elif status in [\"skipped_no_name_type\", \"skipped_no_code\", \"skipped_unsupported_type\", \"skipped_empty_summary\", \"skipped_invalid_summary_type\"]:\n                                logger.debug(f\"Symbol {doc_id_from_result} was skipped: {status}\")\n                            else:\n                                logger.warning(f\"Unknown status '{status}' from _process_symbol_task for {doc_id_from_result}\")\n                        except Exception as exc_in_future:\n                            logger.error(f\"Exception retrieving result from future for a symbol in {path}: {exc_in_future}\", exc_info=True)\n                \n                # After processing all symbols for the current file, extend the main lists\n                if file_embeddings: # Only extend if there's something to add\n                    embeddings.extend(file_embeddings)\n                    metadatas.extend(file_metadatas)\n                    ids.extend(file_ids)\n                    cache.update(file_cache_updates) # Update the main cache\n                    seen_ids.update(file_seen_ids)    # Update main seen_ids\n                elif file_seen_ids: # Still update seen_ids if items were cached\n                    seen_ids.update(file_seen_ids)\n\n            else:\n                raise ValueError(f\"Invalid indexing level: {level}. Choose 'file' or 'symbol'.\")\n\n        # remove orphans\n        orphan_ids = set(cache.keys()) - seen_ids\n        if orphan_ids:\n            try:\n                self.backend.delete(ids=list(orphan_ids))\n            except Exception:\n                pass\n            for oid in orphan_ids:\n                cache.pop(oid, None)\n\n        if embeddings:\n            logger.info(f\"Adding {len(embeddings)} embeddings to the index.\")\n            self.backend.add(embeddings=embeddings, metadatas=metadatas, ids=ids) # Chroma needs 'ids'\n            self.backend.persist()\n            logger.info(\"Index build complete and persisted.\")\n        else:\n            logger.warning(\"No embeddings were generated during indexing.\")\n\n        # persist cache\n        with open(meta_path, \"w\", encoding=\"utf-8\") as fp:\n            json.dump(cache, fp)",
        "file": "src/kit/docstring_indexer.py"
      },
      {
        "name": "get_searcher",
        "type": "method",
        "start_line": 366,
        "end_line": 376,
        "code": "def get_searcher(self) -> 'SummarySearcher':\n        \"\"\"\n        Returns a SummarySearcher instance configured with this indexer.\n\n        This provides a convenient way to get a query interface after the\n        indexer has been built or loaded.\n\n        Returns:\n            SummarySearcher: An instance of SummarySearcher ready to query this indexer.\n        \"\"\"\n        return SummarySearcher(indexer=self)",
        "file": "src/kit/docstring_indexer.py"
      },
      {
        "name": "SummarySearcher",
        "type": "class",
        "start_line": 379,
        "end_line": 391,
        "code": "class SummarySearcher:\n    \"\"\"Simple wrapper that queries a :class:`DocstringIndexer` backend.\"\"\"\n\n    def __init__(self, indexer: DocstringIndexer):\n        self.indexer = indexer\n        self.embed_fn = indexer.embed_fn\n\n    def search(self, query: str, top_k: int = 5) -> List[Dict[str, Any]]:\n        \"\"\"Return up to *top_k* hits with their metadata and distance score.\"\"\"\n        if top_k <= 0:\n            return []\n        emb = self.embed_fn(query)\n        return self.indexer.backend.query(emb, top_k)",
        "file": "src/kit/docstring_indexer.py"
      },
      {
        "name": "__init__",
        "type": "method",
        "start_line": 382,
        "end_line": 384,
        "code": "def __init__(self, indexer: DocstringIndexer):\n        self.indexer = indexer\n        self.embed_fn = indexer.embed_fn",
        "file": "src/kit/docstring_indexer.py"
      },
      {
        "name": "search",
        "type": "method",
        "start_line": 386,
        "end_line": 391,
        "code": "def search(self, query: str, top_k: int = 5) -> List[Dict[str, Any]]:\n        \"\"\"Return up to *top_k* hits with their metadata and distance score.\"\"\"\n        if top_k <= 0:\n            return []\n        emb = self.embed_fn(query)\n        return self.indexer.backend.query(emb, top_k)",
        "file": "src/kit/docstring_indexer.py"
      }
    ],
    "src/kit/context_extractor.py": [
      {
        "name": "ContextExtractor",
        "type": "class",
        "start_line": 6,
        "end_line": 106,
        "code": "class ContextExtractor:\n    \"\"\"\n    Extracts context from source code files for chunking, search, and LLM workflows.\n    Supports chunking by lines, symbols, and function/class scope.\n    \"\"\"\n    def __init__(self, repo_path: str) -> None:\n        self.repo_path: Path = Path(repo_path)\n\n    def chunk_file_by_lines(self, file_path: str, max_lines: int = 50) -> List[str]:\n        \"\"\"\n        Chunk file into blocks of at most max_lines lines.\n        \"\"\"\n        chunks: List[str] = []\n        with open(self.repo_path / file_path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n            lines: List[str] = []\n            for i, line in enumerate(f, 1):\n                lines.append(line)\n                if i % max_lines == 0:\n                    chunks.append(\"\".join(lines))\n                    lines = []\n            if lines:\n                chunks.append(\"\".join(lines))\n        return chunks\n\n    def chunk_file_by_symbols(self, file_path: str) -> List[Dict[str, Any]]:\n        ext = Path(file_path).suffix.lower()\n        abs_path = self.repo_path / file_path\n        try:\n            with open(abs_path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n                code = f.read()\n        except Exception:\n            return []\n        if ext in TreeSitterSymbolExtractor.LANGUAGES:\n            return TreeSitterSymbolExtractor.extract_symbols(ext, code)\n        return []\n\n    def extract_context_around_line(self, file_path: str, line: int) -> Optional[Dict[str, Any]]:\n        \"\"\"\n        Extracts the function/class (or code block) containing the given line.\n        Returns a dict with type, name, and code.\n        \"\"\"\n        ext = Path(file_path).suffix.lower()\n        abs_path = self.repo_path / file_path\n        try:\n            with open(abs_path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n                all_lines = f.readlines()\n            code = \"\".join(all_lines)\n        except Exception:\n            return None\n        if ext == \".py\":\n            try:\n                tree = ast.parse(code, filename=str(abs_path))\n                best_node = None\n                min_length = float('inf')\n\n                for node in ast.walk(tree):\n                    if isinstance(node, (ast.FunctionDef, ast.ClassDef)):\n                        start_lineno = node.lineno\n                        end_lineno = getattr(node, 'end_lineno', start_lineno)\n\n                        if start_lineno is not None and end_lineno is not None and start_lineno <= line <= end_lineno:\n                            current_length = end_lineno - start_lineno\n                            if current_length < min_length:\n                                min_length = current_length\n                                best_node = node\n                            # If lengths are equal, prefer functions/methods over classes if one contains the other\n                            elif current_length == min_length and isinstance(node, ast.FunctionDef) and isinstance(best_node, ast.ClassDef):\n                                # This heuristic helps if a class and a method start on the same line (unlikely for typical formatting)\n                                # A more robust check would be full containment, but this is simpler.\n                                best_node = node \n\n                if best_node:\n                    start = best_node.lineno\n                    end = getattr(best_node, 'end_lineno', start)\n                    code_block = \"\".join(all_lines[start-1:end])\n                    return {\n                        \"type\": \"function\" if isinstance(best_node, ast.FunctionDef) else \"class\",\n                        \"name\": best_node.name,\n                        \"code\": code_block\n                    }\n            except Exception: # If AST parsing fails, fall through to generic line-based chunking\n                pass \n \n         # For other languages or Python AST failure: fallback to chunk by lines\n        context_delta = 10\n        # `line` is 1-indexed, list `all_lines` is 0-indexed\n        target_line_0_indexed = line - 1\n\n        if not (0 <= target_line_0_indexed < len(all_lines)):\n            return None # Line number out of bounds\n\n        start_chunk_0_indexed = max(0, target_line_0_indexed - context_delta)\n        end_chunk_0_indexed = min(len(all_lines), target_line_0_indexed + context_delta + 1)\n\n        code_block_chunk = \"\".join(all_lines[start_chunk_0_indexed:end_chunk_0_indexed])\n\n        return {\n            \"type\": \"code_chunk\",\n            \"name\": f\"{Path(file_path).name}:{line}\", # Use Path(file_path).name to get filename\n            \"code\": code_block_chunk\n        }",
        "file": "src/kit/context_extractor.py"
      },
      {
        "name": "__init__",
        "type": "method",
        "start_line": 11,
        "end_line": 12,
        "code": "def __init__(self, repo_path: str) -> None:\n        self.repo_path: Path = Path(repo_path)",
        "file": "src/kit/context_extractor.py"
      },
      {
        "name": "chunk_file_by_lines",
        "type": "method",
        "start_line": 14,
        "end_line": 28,
        "code": "def chunk_file_by_lines(self, file_path: str, max_lines: int = 50) -> List[str]:\n        \"\"\"\n        Chunk file into blocks of at most max_lines lines.\n        \"\"\"\n        chunks: List[str] = []\n        with open(self.repo_path / file_path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n            lines: List[str] = []\n            for i, line in enumerate(f, 1):\n                lines.append(line)\n                if i % max_lines == 0:\n                    chunks.append(\"\".join(lines))\n                    lines = []\n            if lines:\n                chunks.append(\"\".join(lines))\n        return chunks",
        "file": "src/kit/context_extractor.py"
      },
      {
        "name": "chunk_file_by_symbols",
        "type": "method",
        "start_line": 30,
        "end_line": 40,
        "code": "def chunk_file_by_symbols(self, file_path: str) -> List[Dict[str, Any]]:\n        ext = Path(file_path).suffix.lower()\n        abs_path = self.repo_path / file_path\n        try:\n            with open(abs_path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n                code = f.read()\n        except Exception:\n            return []\n        if ext in TreeSitterSymbolExtractor.LANGUAGES:\n            return TreeSitterSymbolExtractor.extract_symbols(ext, code)\n        return []",
        "file": "src/kit/context_extractor.py"
      },
      {
        "name": "extract_context_around_line",
        "type": "method",
        "start_line": 42,
        "end_line": 106,
        "code": "def extract_context_around_line(self, file_path: str, line: int) -> Optional[Dict[str, Any]]:\n        \"\"\"\n        Extracts the function/class (or code block) containing the given line.\n        Returns a dict with type, name, and code.\n        \"\"\"\n        ext = Path(file_path).suffix.lower()\n        abs_path = self.repo_path / file_path\n        try:\n            with open(abs_path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n                all_lines = f.readlines()\n            code = \"\".join(all_lines)\n        except Exception:\n            return None\n        if ext == \".py\":\n            try:\n                tree = ast.parse(code, filename=str(abs_path))\n                best_node = None\n                min_length = float('inf')\n\n                for node in ast.walk(tree):\n                    if isinstance(node, (ast.FunctionDef, ast.ClassDef)):\n                        start_lineno = node.lineno\n                        end_lineno = getattr(node, 'end_lineno', start_lineno)\n\n                        if start_lineno is not None and end_lineno is not None and start_lineno <= line <= end_lineno:\n                            current_length = end_lineno - start_lineno\n                            if current_length < min_length:\n                                min_length = current_length\n                                best_node = node\n                            # If lengths are equal, prefer functions/methods over classes if one contains the other\n                            elif current_length == min_length and isinstance(node, ast.FunctionDef) and isinstance(best_node, ast.ClassDef):\n                                # This heuristic helps if a class and a method start on the same line (unlikely for typical formatting)\n                                # A more robust check would be full containment, but this is simpler.\n                                best_node = node \n\n                if best_node:\n                    start = best_node.lineno\n                    end = getattr(best_node, 'end_lineno', start)\n                    code_block = \"\".join(all_lines[start-1:end])\n                    return {\n                        \"type\": \"function\" if isinstance(best_node, ast.FunctionDef) else \"class\",\n                        \"name\": best_node.name,\n                        \"code\": code_block\n                    }\n            except Exception: # If AST parsing fails, fall through to generic line-based chunking\n                pass \n \n         # For other languages or Python AST failure: fallback to chunk by lines\n        context_delta = 10\n        # `line` is 1-indexed, list `all_lines` is 0-indexed\n        target_line_0_indexed = line - 1\n\n        if not (0 <= target_line_0_indexed < len(all_lines)):\n            return None # Line number out of bounds\n\n        start_chunk_0_indexed = max(0, target_line_0_indexed - context_delta)\n        end_chunk_0_indexed = min(len(all_lines), target_line_0_indexed + context_delta + 1)\n\n        code_block_chunk = \"\".join(all_lines[start_chunk_0_indexed:end_chunk_0_indexed])\n\n        return {\n            \"type\": \"code_chunk\",\n            \"name\": f\"{Path(file_path).name}:{line}\", # Use Path(file_path).name to get filename\n            \"code\": code_block_chunk\n        }",
        "file": "src/kit/context_extractor.py"
      }
    ],
    "src/kit/dependency_analyzer.py": [
      {
        "name": "DependencyAnalyzer",
        "type": "class",
        "start_line": 14,
        "end_line": 588,
        "code": "class DependencyAnalyzer:\n    \"\"\"\n    Analyzes internal and external dependencies in a codebase.\n    \n    This class provides functionality to:\n    1. Build a dependency graph of modules within a repository\n    2. Identify import relationships between files\n    3. Export dependency information in various formats\n    4. Detect dependency cycles and other potential issues\n    \"\"\"\n    \n    def __init__(self, repository: 'Repository'):\n        \"\"\"\n        Initialize the analyzer with a Repository instance.\n        \n        Args:\n            repository: A kit.Repository instance\n        \"\"\"\n        self.repo = repository\n        self.dependency_graph: Dict[str, Dict[str, Any]] = {}\n        self._module_map: Dict[str, str] = {}\n        self._initialized = False\n    \n    def build_dependency_graph(self) -> Dict[str, Dict[str, Any]]:\n        \"\"\"\n        Analyzes the entire repository and builds a dependency graph.\n        \n        Returns:\n            A dictionary representing the dependency graph, where:\n            - Keys are module identifiers\n            - Values are dictionaries containing:\n                - 'type': 'internal' or 'external'\n                - 'path': File path (for internal dependencies)\n                - 'dependencies': List of module identifiers this module depends on\n        \"\"\"\n        self.dependency_graph = {}\n        self._module_map = {}\n        \n        self._build_module_map()\n        \n        for file_info in self.repo.get_file_tree():\n            if file_info['path'].endswith('.py'):\n                self._process_file(file_info['path'])\n        \n        self._initialized = True\n        return self.dependency_graph\n    \n    def _build_module_map(self):\n        \"\"\"Maps module names to file paths for internal imports.\"\"\"\n        for file_info in self.repo.get_file_tree():\n            if not file_info['path'].endswith('.py'):\n                continue\n                \n            module_path = os.path.splitext(file_info['path'])[0]\n            module_name = module_path.replace('/', '.').replace('\\\\', '.')\n            \n            if module_name.endswith('.__init__'):\n                package_name = module_name[:-9]\n                self._module_map[package_name] = os.path.dirname(file_info['path'])\n            \n            self._module_map[module_name] = file_info['path']\n            \n            parts = module_name.split('.')\n            for i in range(1, len(parts) + 1):\n                potential_module = '.'.join(parts[:i])\n                if potential_module not in self._module_map:\n                    parent_dir = os.path.dirname(file_info['path'])\n                    for _ in range(len(parts) - i):\n                        parent_dir = os.path.dirname(parent_dir)\n                    self._module_map[potential_module] = parent_dir\n    \n    def _process_file(self, file_path: str):\n        \"\"\"\n        Process a single file to extract its dependencies.\n        \n        Args:\n            file_path: Path to the file to analyze\n        \"\"\"\n        try:\n            file_content = self.repo.get_file_content(file_path)\n            tree = ast.parse(file_content)\n            \n            module_path = os.path.splitext(file_path)[0]\n            module_name = module_path.replace('/', '.').replace('\\\\', '.')\n            if module_name.endswith('.__init__'):\n                module_name = module_name[:-9]\n            \n            if module_name not in self.dependency_graph:\n                self.dependency_graph[module_name] = {\n                    'type': 'internal',\n                    'path': file_path,\n                    'dependencies': set()\n                }\n            \n            for node in ast.walk(tree):\n                if isinstance(node, ast.Import):\n                    for name in node.names:\n                        imported_name = name.name\n                        self._add_dependency(module_name, imported_name)\n                \n                elif isinstance(node, ast.ImportFrom):\n                    if node.module is not None:\n                        module_path = node.module\n                        \n                        for name in node.names:\n                            specific_module = f\"{module_path}.{name.name}\"\n                            \n                            if specific_module in self._module_map:\n                                self._add_dependency(module_name, specific_module)\n                            else:\n                                self._add_dependency(module_name, module_path)\n        \n        except Exception as e:\n            logger.warning(f\"Error processing {file_path}: {e}\")\n    \n    def _add_dependency(self, source: str, target: str):\n        \"\"\"\n        Add a dependency from source to target in the graph.\n        \n        Args:\n            source: Source module name\n            target: Target module/package name\n        \"\"\"\n        self.dependency_graph[source]['dependencies'].add(target)\n        \n        if target not in self.dependency_graph:\n            if target in self._module_map:\n                dependency_type = 'internal'\n                dependency_path = self._module_map[target]\n            else:\n                dependency_type = 'external'\n                dependency_path = None\n                \n            self.dependency_graph[target] = {\n                'type': dependency_type,\n                'path': dependency_path,\n                'dependencies': set()\n            }\n    \n    def export_dependency_graph(self, output_format: str = 'json', output_path: Optional[str] = None) -> Union[Dict, str]:\n        \"\"\"\n        Export the dependency graph in various formats.\n        \n        Args:\n            output_format: Format to export ('json', 'dot', 'graphml', 'adjacency')\n            output_path: Path to save the output file (if None, returns the data)\n        \n        Returns:\n            Depending on format and output_path:\n            - If output_path is provided: Path to the output file\n            - If output_path is None: Formatted dependency data\n        \"\"\"\n        if not self._initialized:\n            self.build_dependency_graph()\n        \n        serializable_graph = {}\n        for module, data in self.dependency_graph.items():\n            serializable_graph[module] = {\n                'type': data['type'],\n                'path': data['path'],\n                'dependencies': list(data['dependencies'])\n            }\n        \n        if output_format == 'json':\n            import json\n            if output_path:\n                with open(output_path, 'w') as f:\n                    json.dump(serializable_graph, f, indent=2)\n                return output_path\n            return serializable_graph\n        \n        elif output_format == 'dot':\n            dot_content = self._generate_dot_file(serializable_graph)\n            if output_path:\n                with open(output_path, 'w') as f:\n                    f.write(dot_content)\n                return output_path\n            return dot_content\n        \n        elif output_format == 'graphml':\n            graphml_content = self._generate_graphml_file(serializable_graph)\n            if output_path:\n                with open(output_path, 'w') as f:\n                    f.write(graphml_content)\n                return output_path\n            return graphml_content\n        \n        elif output_format == 'adjacency':\n            adjacency_list = {}\n            for module, data in serializable_graph.items():\n                adjacency_list[module] = data['dependencies']\n            \n            if output_path:\n                import json\n                with open(output_path, 'w') as f:\n                    json.dump(adjacency_list, f, indent=2)\n                return output_path\n            return adjacency_list\n        \n        else:\n            raise ValueError(f\"Unsupported output format: {output_format}\")\n    \n    def _generate_dot_file(self, graph: Dict[str, Dict[str, Any]]) -> str:\n        \"\"\"Generate a DOT file for visualization with Graphviz.\"\"\"\n        dot_lines = ['digraph G {', '  rankdir=\"LR\";', '  node [shape=box];']\n        \n        # Add nodes\n        for module, data in graph.items():\n            node_color = \"lightblue\" if data['type'] == 'internal' else \"lightgreen\"\n            # Escape quotes in module name\n            safe_module = module.replace('\"', '\\\\\"')\n            dot_lines.append(f'  \"{safe_module}\" [style=filled, fillcolor={node_color}];')\n        \n        # Add edges\n        for module, data in graph.items():\n            safe_module = module.replace('\"', '\\\\\"')\n            for dep in data['dependencies']:\n                if dep in graph:  # Only add edges for modules in the graph\n                    safe_dep = dep.replace('\"', '\\\\\"')\n                    dot_lines.append(f'  \"{safe_module}\" -> \"{safe_dep}\";')\n        \n        dot_lines.append('}')\n        return '\\n'.join(dot_lines)\n    \n    def _generate_graphml_file(self, graph: Dict[str, Dict[str, Any]]) -> str:\n        \"\"\"Generate a GraphML file for visualization with tools like Gephi or yEd.\"\"\"\n        graphml_lines = [\n            '<?xml version=\"1.0\" encoding=\"UTF-8\"?>',\n            '<graphml xmlns=\"http://graphml.graphdrawing.org/xmlns\"',\n            '  xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"',\n            '  xsi:schemaLocation=\"http://graphml.graphdrawing.org/xmlns',\n            '  http://graphml.graphdrawing.org/xmlns/1.0/graphml.xsd\">',\n            '<key id=\"type\" for=\"node\" attr.name=\"type\" attr.type=\"string\"/>',\n            '<key id=\"path\" for=\"node\" attr.name=\"path\" attr.type=\"string\"/>',\n            '<graph id=\"G\" edgedefault=\"directed\">'\n        ]\n        \n        # Add nodes\n        for module, data in graph.items():\n            # XML-escape module name\n            safe_module = module.replace('&', '&amp;').replace('<', '&lt;').replace('>', '&gt;').replace('\"', '&quot;')\n            \n            graphml_lines.append(f'  <node id=\"{safe_module}\">')\n            graphml_lines.append(f'    <data key=\"type\">{data[\"type\"]}</data>')\n            if data['path']:\n                safe_path = data['path'].replace('&', '&amp;').replace('<', '&lt;').replace('>', '&gt;').replace('\"', '&quot;')\n                graphml_lines.append(f'    <data key=\"path\">{safe_path}</data>')\n            graphml_lines.append('  </node>')\n        \n        # Add edges\n        edge_id = 0\n        for module, data in graph.items():\n            safe_module = module.replace('&', '&amp;').replace('<', '&lt;').replace('>', '&gt;').replace('\"', '&quot;')\n            for dep in data['dependencies']:\n                if dep in graph:  # Only add edges for modules in the graph\n                    safe_dep = dep.replace('&', '&amp;').replace('<', '&lt;').replace('>', '&gt;').replace('\"', '&quot;')\n                    graphml_lines.append(f'  <edge id=\"e{edge_id}\" source=\"{safe_module}\" target=\"{safe_dep}\"/>')\n                    edge_id += 1\n        \n        graphml_lines.append('</graph>')\n        graphml_lines.append('</graphml>')\n        return '\\n'.join(graphml_lines)\n    \n    def find_cycles(self) -> List[List[str]]:\n        \"\"\"\n        Find cycles in the dependency graph.\n        \n        Returns:\n            List of cycles, where each cycle is a list of module names\n        \"\"\"\n        if not self._initialized:\n            self.build_dependency_graph()\n        \n        cycles = []\n        \n        for start_module in self.dependency_graph:\n            if self.dependency_graph[start_module]['type'] != 'internal':\n                continue\n                \n            path: List[str] = []\n            visited = set()\n            \n            def dfs(module):\n                if module in path:\n                    cycle_start = path.index(module)\n                    cycle = path[cycle_start:] + [module]\n                    if cycle not in cycles and len(cycle) > 1:\n                        cycles.append(cycle)\n                    return\n                \n                if module in visited or module not in self.dependency_graph:\n                    return\n                \n                visited.add(module)\n                path.append(module)\n                \n                for dep in self.dependency_graph[module]['dependencies']:\n                    if self.dependency_graph.get(dep, {}).get('type') == 'internal':\n                        dfs(dep)\n                \n                path.pop()\n            \n            dfs(start_module)\n        \n        return cycles\n    \n    def get_module_dependencies(self, module_name: str, include_indirect: bool = False) -> List[str]:\n        \"\"\"\n        Get dependencies for a specific module.\n        \n        Args:\n            module_name: Name of the module to check\n            include_indirect: Whether to include indirect dependencies\n            \n        Returns:\n            List of module names this module depends on\n        \"\"\"\n        if not self._initialized:\n            self.build_dependency_graph()\n        \n        if module_name not in self.dependency_graph:\n            return []\n        \n        if include_indirect:\n            all_deps = set()\n            visited = set()\n            \n            def dfs(module):\n                if module in visited or module not in self.dependency_graph:\n                    return\n                \n                visited.add(module)\n                \n                for dep in self.dependency_graph[module]['dependencies']:\n                    if dep in self.dependency_graph:\n                        all_deps.add(dep)\n                        dfs(dep)\n            \n            dfs(module_name)\n            return list(all_deps)\n        else:\n            return [\n                dep for dep in self.dependency_graph[module_name]['dependencies']\n                if dep in self.dependency_graph\n            ]\n    \n    def get_dependents(self, module_name: str, include_indirect: bool = False) -> List[str]:\n        \"\"\"\n        Get modules that depend on the specified module.\n        \n        Args:\n            module_name: Name of the module to check\n            include_indirect: Whether to include indirect dependents\n            \n        Returns:\n            List of module names that depend on this module\n        \"\"\"\n        if not self._initialized:\n            self.build_dependency_graph()\n        \n        if module_name not in self.dependency_graph:\n            return []\n        \n        direct_dependents = [\n            mod for mod, data in self.dependency_graph.items()\n            if module_name in data['dependencies']\n        ]\n        \n        if not include_indirect:\n            return direct_dependents\n        \n        all_dependents = set(direct_dependents)\n        \n        def find_ancestors(module):\n            parents = [\n                mod for mod, data in self.dependency_graph.items()\n                if module in data['dependencies'] and mod not in all_dependents\n            ]\n            \n            for parent in parents:\n                all_dependents.add(parent)\n                find_ancestors(parent)\n        \n        for dep in direct_dependents:\n            find_ancestors(dep)\n        \n        return list(all_dependents)\n    \n    def get_file_dependencies(self, file_path: str) -> Dict[str, Any]:\n        \"\"\"\n        Get detailed dependency information for a specific file.\n        \n        Args:\n            file_path: Path to the file to analyze\n            \n        Returns:\n            Dictionary with dependency information for the file\n        \"\"\"\n        if not self._initialized:\n            self.build_dependency_graph()\n        \n        # Convert file path to module name\n        module_path = os.path.splitext(file_path)[0]\n        module_name = module_path.replace('/', '.').replace('\\\\', '.')\n        \n        # Handle __init__.py files\n        if module_name.endswith('.__init__'):\n            module_name = module_name[:-9]  # Remove .__init__\n        \n        if module_name not in self.dependency_graph:\n            return {\n                'file_path': file_path,\n                'module_name': module_name,\n                'dependencies': [],\n                'dependents': []\n            }\n        \n        # Get direct dependencies and dependents\n        dependencies = self.get_module_dependencies(module_name)\n        dependents = self.get_dependents(module_name)\n        \n        # Map dependencies to their types\n        dependencies_info = []\n        for dep in dependencies:\n            if dep in self.dependency_graph:\n                dependencies_info.append({\n                    'module': dep,\n                    'type': self.dependency_graph[dep]['type'],\n                    'path': self.dependency_graph[dep]['path']\n                })\n            else:\n                dependencies_info.append({\n                    'module': dep,\n                    'type': 'external',\n                    'path': None\n                })\n        \n        # Map dependents to their types\n        dependents_info = []\n        for dep in dependents:\n            dependents_info.append({\n                'module': dep,\n                'type': self.dependency_graph[dep]['type'],\n                'path': self.dependency_graph[dep]['path']\n            })\n        \n        return {\n            'file_path': file_path,\n            'module_name': module_name,\n            'dependencies': dependencies_info,\n            'dependents': dependents_info\n        }\n    \n    def generate_dependency_report(self, output_path: Optional[str] = None) -> Dict[str, Any]:\n        \"\"\"\n        Generate a comprehensive dependency report for the repository.\n        \n        Args:\n            output_path: Optional path to save the report JSON\n            \n        Returns:\n            Dictionary with the complete dependency report\n        \"\"\"\n        if not self._initialized:\n            self.build_dependency_graph()\n        \n        # Count internal and external dependencies\n        internal_modules = [m for m, data in self.dependency_graph.items() if data['type'] == 'internal']\n        external_modules = [m for m, data in self.dependency_graph.items() if data['type'] == 'external']\n        \n        # Find cycles\n        cycles = self.find_cycles()\n        \n        # Find highly connected modules (potential refactoring candidates)\n        high_dependency_modules = []\n        for module, data in self.dependency_graph.items():\n            if data['type'] == 'internal':\n                dependents = self.get_dependents(module)\n                dependencies = self.get_module_dependencies(module)\n                \n                if len(dependents) > 5 or len(dependencies) > 10:\n                    high_dependency_modules.append({\n                        'module': module,\n                        'path': data['path'],\n                        'dependent_count': len(dependents),\n                        'dependency_count': len(dependencies)\n                    })\n        \n        # Generate report\n        report = {\n            'summary': {\n                'total_modules': len(self.dependency_graph),\n                'internal_modules': len(internal_modules),\n                'external_modules': len(external_modules),\n                'dependency_cycles': len(cycles)\n            },\n            'cycles': cycles,\n            'high_dependency_modules': sorted(\n                high_dependency_modules,\n                key=lambda x: x['dependent_count'] + x['dependency_count'],\n                reverse=True\n            ),\n            'external_dependencies': sorted(external_modules)\n        }\n        \n        if output_path:\n            import json\n            with open(output_path, 'w') as f:\n                json.dump(report, f, indent=2)\n        \n        return report\n    \n    def visualize_dependencies(self, output_path: str, format: str = 'png') -> str:\n        \"\"\"\n        Generate a visualization of the dependency graph.\n        \n        Note: Requires Graphviz to be installed.\n        \n        Args:\n            output_path: Path to save the visualization\n            format: Output format ('png', 'svg', 'pdf')\n            \n        Returns:\n            Path to the generated visualization file\n        \"\"\"\n        try:\n            import graphviz\n        except ImportError:\n            raise ImportError(\n                \"Graphviz Python package is required for visualization. \"\n                \"Install with 'pip install graphviz'. \"\n                \"You also need the Graphviz binary installed on your system.\"\n            )\n        \n        if not self._initialized:\n            self.build_dependency_graph()\n            \n        dot = graphviz.Digraph(\n            'dependencies',\n            comment='Module Dependencies',\n            format=format,\n            engine='dot'\n        )\n        dot.attr(rankdir='LR')\n        \n        for module, data in self.dependency_graph.items():\n            if data['type'] == 'external':\n                continue\n                \n            if '.' in module:\n                label = module.split('.')[-1]\n                tooltip = module\n            else:\n                label = module\n                tooltip = module\n                \n            dot.node(\n                module,\n                label=label,\n                tooltip=tooltip,\n                style='filled',\n                fillcolor='lightblue',\n                shape='box'\n            )\n        \n        for module, data in self.dependency_graph.items():\n            if data['type'] == 'external':\n                continue\n                \n            for dep in data['dependencies']:\n                if dep in self.dependency_graph and self.dependency_graph[dep]['type'] == 'internal':\n                    dot.edge(module, dep)\n        \n        dot.render(output_path, cleanup=True)\n        return f\"{output_path}.{format}\"",
        "file": "src/kit/dependency_analyzer.py"
      },
      {
        "name": "__init__",
        "type": "method",
        "start_line": 25,
        "end_line": 35,
        "code": "def __init__(self, repository: 'Repository'):\n        \"\"\"\n        Initialize the analyzer with a Repository instance.\n        \n        Args:\n            repository: A kit.Repository instance\n        \"\"\"\n        self.repo = repository\n        self.dependency_graph: Dict[str, Dict[str, Any]] = {}\n        self._module_map: Dict[str, str] = {}\n        self._initialized = False",
        "file": "src/kit/dependency_analyzer.py"
      },
      {
        "name": "build_dependency_graph",
        "type": "method",
        "start_line": 37,
        "end_line": 59,
        "code": "def build_dependency_graph(self) -> Dict[str, Dict[str, Any]]:\n        \"\"\"\n        Analyzes the entire repository and builds a dependency graph.\n        \n        Returns:\n            A dictionary representing the dependency graph, where:\n            - Keys are module identifiers\n            - Values are dictionaries containing:\n                - 'type': 'internal' or 'external'\n                - 'path': File path (for internal dependencies)\n                - 'dependencies': List of module identifiers this module depends on\n        \"\"\"\n        self.dependency_graph = {}\n        self._module_map = {}\n        \n        self._build_module_map()\n        \n        for file_info in self.repo.get_file_tree():\n            if file_info['path'].endswith('.py'):\n                self._process_file(file_info['path'])\n        \n        self._initialized = True\n        return self.dependency_graph",
        "file": "src/kit/dependency_analyzer.py"
      },
      {
        "name": "_build_module_map",
        "type": "method",
        "start_line": 61,
        "end_line": 83,
        "code": "def _build_module_map(self):\n        \"\"\"Maps module names to file paths for internal imports.\"\"\"\n        for file_info in self.repo.get_file_tree():\n            if not file_info['path'].endswith('.py'):\n                continue\n                \n            module_path = os.path.splitext(file_info['path'])[0]\n            module_name = module_path.replace('/', '.').replace('\\\\', '.')\n            \n            if module_name.endswith('.__init__'):\n                package_name = module_name[:-9]\n                self._module_map[package_name] = os.path.dirname(file_info['path'])\n            \n            self._module_map[module_name] = file_info['path']\n            \n            parts = module_name.split('.')\n            for i in range(1, len(parts) + 1):\n                potential_module = '.'.join(parts[:i])\n                if potential_module not in self._module_map:\n                    parent_dir = os.path.dirname(file_info['path'])\n                    for _ in range(len(parts) - i):\n                        parent_dir = os.path.dirname(parent_dir)\n                    self._module_map[potential_module] = parent_dir",
        "file": "src/kit/dependency_analyzer.py"
      },
      {
        "name": "_process_file",
        "type": "method",
        "start_line": 85,
        "end_line": 127,
        "code": "def _process_file(self, file_path: str):\n        \"\"\"\n        Process a single file to extract its dependencies.\n        \n        Args:\n            file_path: Path to the file to analyze\n        \"\"\"\n        try:\n            file_content = self.repo.get_file_content(file_path)\n            tree = ast.parse(file_content)\n            \n            module_path = os.path.splitext(file_path)[0]\n            module_name = module_path.replace('/', '.').replace('\\\\', '.')\n            if module_name.endswith('.__init__'):\n                module_name = module_name[:-9]\n            \n            if module_name not in self.dependency_graph:\n                self.dependency_graph[module_name] = {\n                    'type': 'internal',\n                    'path': file_path,\n                    'dependencies': set()\n                }\n            \n            for node in ast.walk(tree):\n                if isinstance(node, ast.Import):\n                    for name in node.names:\n                        imported_name = name.name\n                        self._add_dependency(module_name, imported_name)\n                \n                elif isinstance(node, ast.ImportFrom):\n                    if node.module is not None:\n                        module_path = node.module\n                        \n                        for name in node.names:\n                            specific_module = f\"{module_path}.{name.name}\"\n                            \n                            if specific_module in self._module_map:\n                                self._add_dependency(module_name, specific_module)\n                            else:\n                                self._add_dependency(module_name, module_path)\n        \n        except Exception as e:\n            logger.warning(f\"Error processing {file_path}: {e}\")",
        "file": "src/kit/dependency_analyzer.py"
      },
      {
        "name": "_add_dependency",
        "type": "method",
        "start_line": 129,
        "end_line": 151,
        "code": "def _add_dependency(self, source: str, target: str):\n        \"\"\"\n        Add a dependency from source to target in the graph.\n        \n        Args:\n            source: Source module name\n            target: Target module/package name\n        \"\"\"\n        self.dependency_graph[source]['dependencies'].add(target)\n        \n        if target not in self.dependency_graph:\n            if target in self._module_map:\n                dependency_type = 'internal'\n                dependency_path = self._module_map[target]\n            else:\n                dependency_type = 'external'\n                dependency_path = None\n                \n            self.dependency_graph[target] = {\n                'type': dependency_type,\n                'path': dependency_path,\n                'dependencies': set()\n            }",
        "file": "src/kit/dependency_analyzer.py"
      },
      {
        "name": "export_dependency_graph",
        "type": "method",
        "start_line": 153,
        "end_line": 214,
        "code": "def export_dependency_graph(self, output_format: str = 'json', output_path: Optional[str] = None) -> Union[Dict, str]:\n        \"\"\"\n        Export the dependency graph in various formats.\n        \n        Args:\n            output_format: Format to export ('json', 'dot', 'graphml', 'adjacency')\n            output_path: Path to save the output file (if None, returns the data)\n        \n        Returns:\n            Depending on format and output_path:\n            - If output_path is provided: Path to the output file\n            - If output_path is None: Formatted dependency data\n        \"\"\"\n        if not self._initialized:\n            self.build_dependency_graph()\n        \n        serializable_graph = {}\n        for module, data in self.dependency_graph.items():\n            serializable_graph[module] = {\n                'type': data['type'],\n                'path': data['path'],\n                'dependencies': list(data['dependencies'])\n            }\n        \n        if output_format == 'json':\n            import json\n            if output_path:\n                with open(output_path, 'w') as f:\n                    json.dump(serializable_graph, f, indent=2)\n                return output_path\n            return serializable_graph\n        \n        elif output_format == 'dot':\n            dot_content = self._generate_dot_file(serializable_graph)\n            if output_path:\n                with open(output_path, 'w') as f:\n                    f.write(dot_content)\n                return output_path\n            return dot_content\n        \n        elif output_format == 'graphml':\n            graphml_content = self._generate_graphml_file(serializable_graph)\n            if output_path:\n                with open(output_path, 'w') as f:\n                    f.write(graphml_content)\n                return output_path\n            return graphml_content\n        \n        elif output_format == 'adjacency':\n            adjacency_list = {}\n            for module, data in serializable_graph.items():\n                adjacency_list[module] = data['dependencies']\n            \n            if output_path:\n                import json\n                with open(output_path, 'w') as f:\n                    json.dump(adjacency_list, f, indent=2)\n                return output_path\n            return adjacency_list\n        \n        else:\n            raise ValueError(f\"Unsupported output format: {output_format}\")",
        "file": "src/kit/dependency_analyzer.py"
      },
      {
        "name": "_generate_dot_file",
        "type": "method",
        "start_line": 216,
        "end_line": 236,
        "code": "def _generate_dot_file(self, graph: Dict[str, Dict[str, Any]]) -> str:\n        \"\"\"Generate a DOT file for visualization with Graphviz.\"\"\"\n        dot_lines = ['digraph G {', '  rankdir=\"LR\";', '  node [shape=box];']\n        \n        # Add nodes\n        for module, data in graph.items():\n            node_color = \"lightblue\" if data['type'] == 'internal' else \"lightgreen\"\n            # Escape quotes in module name\n            safe_module = module.replace('\"', '\\\\\"')\n            dot_lines.append(f'  \"{safe_module}\" [style=filled, fillcolor={node_color}];')\n        \n        # Add edges\n        for module, data in graph.items():\n            safe_module = module.replace('\"', '\\\\\"')\n            for dep in data['dependencies']:\n                if dep in graph:  # Only add edges for modules in the graph\n                    safe_dep = dep.replace('\"', '\\\\\"')\n                    dot_lines.append(f'  \"{safe_module}\" -> \"{safe_dep}\";')\n        \n        dot_lines.append('}')\n        return '\\n'.join(dot_lines)",
        "file": "src/kit/dependency_analyzer.py"
      },
      {
        "name": "_generate_graphml_file",
        "type": "method",
        "start_line": 238,
        "end_line": 275,
        "code": "def _generate_graphml_file(self, graph: Dict[str, Dict[str, Any]]) -> str:\n        \"\"\"Generate a GraphML file for visualization with tools like Gephi or yEd.\"\"\"\n        graphml_lines = [\n            '<?xml version=\"1.0\" encoding=\"UTF-8\"?>',\n            '<graphml xmlns=\"http://graphml.graphdrawing.org/xmlns\"',\n            '  xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"',\n            '  xsi:schemaLocation=\"http://graphml.graphdrawing.org/xmlns',\n            '  http://graphml.graphdrawing.org/xmlns/1.0/graphml.xsd\">',\n            '<key id=\"type\" for=\"node\" attr.name=\"type\" attr.type=\"string\"/>',\n            '<key id=\"path\" for=\"node\" attr.name=\"path\" attr.type=\"string\"/>',\n            '<graph id=\"G\" edgedefault=\"directed\">'\n        ]\n        \n        # Add nodes\n        for module, data in graph.items():\n            # XML-escape module name\n            safe_module = module.replace('&', '&amp;').replace('<', '&lt;').replace('>', '&gt;').replace('\"', '&quot;')\n            \n            graphml_lines.append(f'  <node id=\"{safe_module}\">')\n            graphml_lines.append(f'    <data key=\"type\">{data[\"type\"]}</data>')\n            if data['path']:\n                safe_path = data['path'].replace('&', '&amp;').replace('<', '&lt;').replace('>', '&gt;').replace('\"', '&quot;')\n                graphml_lines.append(f'    <data key=\"path\">{safe_path}</data>')\n            graphml_lines.append('  </node>')\n        \n        # Add edges\n        edge_id = 0\n        for module, data in graph.items():\n            safe_module = module.replace('&', '&amp;').replace('<', '&lt;').replace('>', '&gt;').replace('\"', '&quot;')\n            for dep in data['dependencies']:\n                if dep in graph:  # Only add edges for modules in the graph\n                    safe_dep = dep.replace('&', '&amp;').replace('<', '&lt;').replace('>', '&gt;').replace('\"', '&quot;')\n                    graphml_lines.append(f'  <edge id=\"e{edge_id}\" source=\"{safe_module}\" target=\"{safe_dep}\"/>')\n                    edge_id += 1\n        \n        graphml_lines.append('</graph>')\n        graphml_lines.append('</graphml>')\n        return '\\n'.join(graphml_lines)",
        "file": "src/kit/dependency_analyzer.py"
      },
      {
        "name": "find_cycles",
        "type": "method",
        "start_line": 277,
        "end_line": 318,
        "code": "def find_cycles(self) -> List[List[str]]:\n        \"\"\"\n        Find cycles in the dependency graph.\n        \n        Returns:\n            List of cycles, where each cycle is a list of module names\n        \"\"\"\n        if not self._initialized:\n            self.build_dependency_graph()\n        \n        cycles = []\n        \n        for start_module in self.dependency_graph:\n            if self.dependency_graph[start_module]['type'] != 'internal':\n                continue\n                \n            path: List[str] = []\n            visited = set()\n            \n            def dfs(module):\n                if module in path:\n                    cycle_start = path.index(module)\n                    cycle = path[cycle_start:] + [module]\n                    if cycle not in cycles and len(cycle) > 1:\n                        cycles.append(cycle)\n                    return\n                \n                if module in visited or module not in self.dependency_graph:\n                    return\n                \n                visited.add(module)\n                path.append(module)\n                \n                for dep in self.dependency_graph[module]['dependencies']:\n                    if self.dependency_graph.get(dep, {}).get('type') == 'internal':\n                        dfs(dep)\n                \n                path.pop()\n            \n            dfs(start_module)\n        \n        return cycles",
        "file": "src/kit/dependency_analyzer.py"
      },
      {
        "name": "get_module_dependencies",
        "type": "method",
        "start_line": 320,
        "end_line": 358,
        "code": "def get_module_dependencies(self, module_name: str, include_indirect: bool = False) -> List[str]:\n        \"\"\"\n        Get dependencies for a specific module.\n        \n        Args:\n            module_name: Name of the module to check\n            include_indirect: Whether to include indirect dependencies\n            \n        Returns:\n            List of module names this module depends on\n        \"\"\"\n        if not self._initialized:\n            self.build_dependency_graph()\n        \n        if module_name not in self.dependency_graph:\n            return []\n        \n        if include_indirect:\n            all_deps = set()\n            visited = set()\n            \n            def dfs(module):\n                if module in visited or module not in self.dependency_graph:\n                    return\n                \n                visited.add(module)\n                \n                for dep in self.dependency_graph[module]['dependencies']:\n                    if dep in self.dependency_graph:\n                        all_deps.add(dep)\n                        dfs(dep)\n            \n            dfs(module_name)\n            return list(all_deps)\n        else:\n            return [\n                dep for dep in self.dependency_graph[module_name]['dependencies']\n                if dep in self.dependency_graph\n            ]",
        "file": "src/kit/dependency_analyzer.py"
      },
      {
        "name": "get_dependents",
        "type": "method",
        "start_line": 360,
        "end_line": 400,
        "code": "def get_dependents(self, module_name: str, include_indirect: bool = False) -> List[str]:\n        \"\"\"\n        Get modules that depend on the specified module.\n        \n        Args:\n            module_name: Name of the module to check\n            include_indirect: Whether to include indirect dependents\n            \n        Returns:\n            List of module names that depend on this module\n        \"\"\"\n        if not self._initialized:\n            self.build_dependency_graph()\n        \n        if module_name not in self.dependency_graph:\n            return []\n        \n        direct_dependents = [\n            mod for mod, data in self.dependency_graph.items()\n            if module_name in data['dependencies']\n        ]\n        \n        if not include_indirect:\n            return direct_dependents\n        \n        all_dependents = set(direct_dependents)\n        \n        def find_ancestors(module):\n            parents = [\n                mod for mod, data in self.dependency_graph.items()\n                if module in data['dependencies'] and mod not in all_dependents\n            ]\n            \n            for parent in parents:\n                all_dependents.add(parent)\n                find_ancestors(parent)\n        \n        for dep in direct_dependents:\n            find_ancestors(dep)\n        \n        return list(all_dependents)",
        "file": "src/kit/dependency_analyzer.py"
      },
      {
        "name": "get_file_dependencies",
        "type": "method",
        "start_line": 402,
        "end_line": 465,
        "code": "def get_file_dependencies(self, file_path: str) -> Dict[str, Any]:\n        \"\"\"\n        Get detailed dependency information for a specific file.\n        \n        Args:\n            file_path: Path to the file to analyze\n            \n        Returns:\n            Dictionary with dependency information for the file\n        \"\"\"\n        if not self._initialized:\n            self.build_dependency_graph()\n        \n        # Convert file path to module name\n        module_path = os.path.splitext(file_path)[0]\n        module_name = module_path.replace('/', '.').replace('\\\\', '.')\n        \n        # Handle __init__.py files\n        if module_name.endswith('.__init__'):\n            module_name = module_name[:-9]  # Remove .__init__\n        \n        if module_name not in self.dependency_graph:\n            return {\n                'file_path': file_path,\n                'module_name': module_name,\n                'dependencies': [],\n                'dependents': []\n            }\n        \n        # Get direct dependencies and dependents\n        dependencies = self.get_module_dependencies(module_name)\n        dependents = self.get_dependents(module_name)\n        \n        # Map dependencies to their types\n        dependencies_info = []\n        for dep in dependencies:\n            if dep in self.dependency_graph:\n                dependencies_info.append({\n                    'module': dep,\n                    'type': self.dependency_graph[dep]['type'],\n                    'path': self.dependency_graph[dep]['path']\n                })\n            else:\n                dependencies_info.append({\n                    'module': dep,\n                    'type': 'external',\n                    'path': None\n                })\n        \n        # Map dependents to their types\n        dependents_info = []\n        for dep in dependents:\n            dependents_info.append({\n                'module': dep,\n                'type': self.dependency_graph[dep]['type'],\n                'path': self.dependency_graph[dep]['path']\n            })\n        \n        return {\n            'file_path': file_path,\n            'module_name': module_name,\n            'dependencies': dependencies_info,\n            'dependents': dependents_info\n        }",
        "file": "src/kit/dependency_analyzer.py"
      },
      {
        "name": "generate_dependency_report",
        "type": "method",
        "start_line": 467,
        "end_line": 524,
        "code": "def generate_dependency_report(self, output_path: Optional[str] = None) -> Dict[str, Any]:\n        \"\"\"\n        Generate a comprehensive dependency report for the repository.\n        \n        Args:\n            output_path: Optional path to save the report JSON\n            \n        Returns:\n            Dictionary with the complete dependency report\n        \"\"\"\n        if not self._initialized:\n            self.build_dependency_graph()\n        \n        # Count internal and external dependencies\n        internal_modules = [m for m, data in self.dependency_graph.items() if data['type'] == 'internal']\n        external_modules = [m for m, data in self.dependency_graph.items() if data['type'] == 'external']\n        \n        # Find cycles\n        cycles = self.find_cycles()\n        \n        # Find highly connected modules (potential refactoring candidates)\n        high_dependency_modules = []\n        for module, data in self.dependency_graph.items():\n            if data['type'] == 'internal':\n                dependents = self.get_dependents(module)\n                dependencies = self.get_module_dependencies(module)\n                \n                if len(dependents) > 5 or len(dependencies) > 10:\n                    high_dependency_modules.append({\n                        'module': module,\n                        'path': data['path'],\n                        'dependent_count': len(dependents),\n                        'dependency_count': len(dependencies)\n                    })\n        \n        # Generate report\n        report = {\n            'summary': {\n                'total_modules': len(self.dependency_graph),\n                'internal_modules': len(internal_modules),\n                'external_modules': len(external_modules),\n                'dependency_cycles': len(cycles)\n            },\n            'cycles': cycles,\n            'high_dependency_modules': sorted(\n                high_dependency_modules,\n                key=lambda x: x['dependent_count'] + x['dependency_count'],\n                reverse=True\n            ),\n            'external_dependencies': sorted(external_modules)\n        }\n        \n        if output_path:\n            import json\n            with open(output_path, 'w') as f:\n                json.dump(report, f, indent=2)\n        \n        return report",
        "file": "src/kit/dependency_analyzer.py"
      },
      {
        "name": "visualize_dependencies",
        "type": "method",
        "start_line": 526,
        "end_line": 588,
        "code": "def visualize_dependencies(self, output_path: str, format: str = 'png') -> str:\n        \"\"\"\n        Generate a visualization of the dependency graph.\n        \n        Note: Requires Graphviz to be installed.\n        \n        Args:\n            output_path: Path to save the visualization\n            format: Output format ('png', 'svg', 'pdf')\n            \n        Returns:\n            Path to the generated visualization file\n        \"\"\"\n        try:\n            import graphviz\n        except ImportError:\n            raise ImportError(\n                \"Graphviz Python package is required for visualization. \"\n                \"Install with 'pip install graphviz'. \"\n                \"You also need the Graphviz binary installed on your system.\"\n            )\n        \n        if not self._initialized:\n            self.build_dependency_graph()\n            \n        dot = graphviz.Digraph(\n            'dependencies',\n            comment='Module Dependencies',\n            format=format,\n            engine='dot'\n        )\n        dot.attr(rankdir='LR')\n        \n        for module, data in self.dependency_graph.items():\n            if data['type'] == 'external':\n                continue\n                \n            if '.' in module:\n                label = module.split('.')[-1]\n                tooltip = module\n            else:\n                label = module\n                tooltip = module\n                \n            dot.node(\n                module,\n                label=label,\n                tooltip=tooltip,\n                style='filled',\n                fillcolor='lightblue',\n                shape='box'\n            )\n        \n        for module, data in self.dependency_graph.items():\n            if data['type'] == 'external':\n                continue\n                \n            for dep in data['dependencies']:\n                if dep in self.dependency_graph and self.dependency_graph[dep]['type'] == 'internal':\n                    dot.edge(module, dep)\n        \n        dot.render(output_path, cleanup=True)\n        return f\"{output_path}.{format}\"",
        "file": "src/kit/dependency_analyzer.py"
      }
    ],
    "src/kit/__init__.py": [],
    "src/kit/tree_sitter_symbol_extractor.py": [
      {
        "name": "TreeSitterSymbolExtractor",
        "type": "class",
        "start_line": 28,
        "end_line": 197,
        "code": "class TreeSitterSymbolExtractor:\n    \"\"\"\n    Multi-language symbol extractor using tree-sitter queries (tags.scm).\n    Register new languages by adding to LANGUAGES and providing a tags.scm.\n    \"\"\"\n    LANGUAGES = set(LANGUAGES.keys())\n    _parsers: ClassVar[dict[str, Any]] = {}\n    _queries: ClassVar[dict[str, Any]] = {}\n\n    @classmethod\n    def get_parser(cls, ext: str) -> Optional[Any]:\n        if ext not in LANGUAGES:\n            return None\n        if ext not in cls._parsers:\n            lang_name = LANGUAGES[ext]\n            parser = get_parser(cast(Any, lang_name))  # type: ignore[arg-type]\n            cls._parsers[ext] = parser\n        return cls._parsers[ext]\n\n    @classmethod\n    def get_query(cls, ext: str) -> Optional[Any]:\n        if ext not in LANGUAGES:\n            logger.debug(f\"get_query: Extension {ext} not supported.\")\n            return None\n        if ext in cls._queries:\n            logger.debug(f\"get_query: query cached for ext {ext}\")\n            return cls._queries[ext]\n\n        lang_name = LANGUAGES[ext]\n        logger.debug(f\"get_query: lang={lang_name}\")\n        query_dir: str = lang_name \n        tags_path: str = os.path.join(QUERIES_ROOT, query_dir, \"tags.scm\")\n        logger.debug(f\"get_query: tags_path={tags_path} exists={os.path.exists(tags_path)}\")\n        if not os.path.exists(tags_path):\n            logger.warning(f\"get_query: tags.scm not found at {tags_path}\")\n            return None\n        try:\n            language = get_language(cast(Any, lang_name))  # type: ignore[arg-type]\n            with open(tags_path, 'r') as f:\n                tags_content = f.read()\n            query = language.query(tags_content)\n            cls._queries[ext] = query\n            logger.debug(f\"get_query: Query loaded successfully for ext {ext}\")\n            return query\n        except Exception as e:\n            logger.error(f\"get_query: Query compile error for ext {ext}: {e}\")\n            logger.error(traceback.format_exc()) # Log stack trace\n            return None\n\n    @staticmethod\n    def extract_symbols(ext: str, source_code: str) -> List[Dict[str, Any]]:\n        \"\"\"Extracts symbols from source code using tree-sitter queries.\"\"\"\n        logger.debug(f\"[EXTRACT] Attempting to extract symbols for ext: {ext}\")\n        symbols: List[Dict[str, Any]] = []\n        query = TreeSitterSymbolExtractor.get_query(ext)\n        parser = TreeSitterSymbolExtractor.get_parser(ext)\n\n        if not query or not parser:\n            logger.warning(f\"[EXTRACT] No query or parser available for extension: {ext}\")\n            return []\n\n        try:\n            tree = parser.parse(bytes(source_code, \"utf8\"))\n            root = tree.root_node\n\n            matches = query.matches(root)\n            logger.debug(f\"[EXTRACT] Found {len(matches)} matches.\")\n\n            # matches is List[Tuple[int, Dict[str, Node]]]\n            # Each tuple is (pattern_index, {capture_name: Node})\n            for pattern_index, captures in matches:\n                logger.debug(f\"[MATCH pattern={pattern_index}] Processing match with captures: {list(captures.keys())}\")\n\n                # Determine symbol name: prefer @name, fallback to @type for blocks like terraform/locals\n                node_candidate = None\n                if 'name' in captures:\n                    node_candidate = captures['name']\n                elif 'type' in captures:\n                    node_candidate = captures['type']\n                else:\n                    # Fallback: take the first capture node\n                    first_capture_node = next(iter(captures.values()), None)\n                    if not first_capture_node:\n                        continue\n                    node_candidate = first_capture_node\n\n                # Handle list of nodes (tree-sitter may return a list)\n                if isinstance(node_candidate, list):\n                    if not node_candidate:\n                        continue  # skip empty list\n                    actual_name_node = node_candidate[0]\n                else:\n                    actual_name_node = node_candidate\n\n                # Now extract symbol name as before\n                symbol_name = actual_name_node.text.decode() if hasattr(actual_name_node, 'text') else str(actual_name_node)\n                # HCL: Strip quotes from string literals\n                if ext == '.tf' and hasattr(actual_name_node, 'type') and actual_name_node.type == 'string_lit':\n                    if len(symbol_name) >= 2 and symbol_name.startswith('\"') and symbol_name.endswith('\"'):\n                        symbol_name = symbol_name[1:-1]\n\n                definition_capture = next(((name, node) for name, node in captures.items() if name.startswith(\"definition.\")), None)\n                subtype = None\n                if definition_capture:\n                    definition_capture_name, definition_node = definition_capture\n                    symbol_type = definition_capture_name.split('.')[-1]\n                    # HCL: For resource/data, combine type and name, and set subtype to the specific resource/data type\n                    if ext == '.tf' and symbol_type in [\"resource\", \"data\"]:\n                        type_node = captures.get('type')\n                        if type_node:\n                            if isinstance(type_node, list):\n                                type_node = type_node[0] if type_node else None\n                            if type_node and hasattr(type_node, 'text'):\n                                type_name = type_node.text.decode()\n                                if hasattr(type_node, 'type') and type_node.type == 'string_lit':\n                                    if len(type_name) >= 2 and type_name.startswith('\"') and type_name.endswith('\"'):\n                                        type_name = type_name[1:-1]\n                                symbol_name = f\"{type_name}.{symbol_name}\"\n                                subtype = type_name\n                else:\n                    # Fallback: infer symbol type from first capture label (e.g., 'function', 'class')\n                    fallback_label = next(iter(captures.keys()), 'symbol')\n                    symbol_type = fallback_label.lstrip('definition.').lstrip('@')\n\n                # Determine the node for the full symbol body, its span, and its code content.\n                # Default to actual_name_node if no specific body capture is found.\n                node_for_body_span_and_code = actual_name_node \n                if definition_capture:\n                    _, captured_body_node = definition_capture # This is the node from @definition.foo\n                    temp_body_node = None\n                    if isinstance(captured_body_node, list):\n                        temp_body_node = captured_body_node[0] if captured_body_node else None\n                    else:\n                        temp_body_node = captured_body_node\n                    \n                    if temp_body_node: # If a valid body node was found from definition_capture\n                        node_for_body_span_and_code = temp_body_node\n\n                # Extract start_line, end_line, and code content from node_for_body_span_and_code\n                symbol_start_line = node_for_body_span_and_code.start_point[0]\n                symbol_end_line = node_for_body_span_and_code.end_point[0]\n                \n                if hasattr(node_for_body_span_and_code, 'text') and isinstance(node_for_body_span_and_code.text, bytes):\n                    symbol_code_content = node_for_body_span_and_code.text.decode('utf-8', errors='ignore')\n                elif hasattr(node_for_body_span_and_code, 'start_byte') and hasattr(node_for_body_span_and_code, 'end_byte'):\n                    # Fallback for nodes where .text might not be the full desired content or not directly available as decodable bytes\n                    symbol_code_content = source_code[node_for_body_span_and_code.start_byte:node_for_body_span_and_code.end_byte]\n                else:\n                    # Last resort, if node_for_body_span_and_code is unusual and lacks .text (bytes) or start/end_byte\n                    symbol_code_content = symbol_name # Fallback to just the name string\n\n                symbol = {\n                    \"name\": symbol_name, # symbol_name is from actual_name_node, potentially modified by HCL logic\n                    \"type\": symbol_type,\n                    \"start_line\": symbol_start_line,\n                    \"end_line\": symbol_end_line,\n                    \"code\": symbol_code_content, \n                }\n                if subtype:\n                    symbol[\"subtype\"] = subtype\n                symbols.append(symbol)\n                continue\n\n        except Exception as e:\n            logger.error(f\"[EXTRACT] Error parsing or processing file with ext {ext}: {e}\")\n            logger.error(traceback.format_exc())\n            return [] # Return empty list on error\n\n        logger.debug(f\"[EXTRACT] Finished extraction for ext {ext}. Found {len(symbols)} symbols.\")\n        return symbols",
        "file": "src/kit/tree_sitter_symbol_extractor.py"
      },
      {
        "name": "get_parser",
        "type": "method",
        "start_line": 38,
        "end_line": 45,
        "code": "def get_parser(cls, ext: str) -> Optional[Any]:\n        if ext not in LANGUAGES:\n            return None\n        if ext not in cls._parsers:\n            lang_name = LANGUAGES[ext]\n            parser = get_parser(cast(Any, lang_name))  # type: ignore[arg-type]\n            cls._parsers[ext] = parser\n        return cls._parsers[ext]",
        "file": "src/kit/tree_sitter_symbol_extractor.py"
      },
      {
        "name": "get_query",
        "type": "method",
        "start_line": 48,
        "end_line": 75,
        "code": "def get_query(cls, ext: str) -> Optional[Any]:\n        if ext not in LANGUAGES:\n            logger.debug(f\"get_query: Extension {ext} not supported.\")\n            return None\n        if ext in cls._queries:\n            logger.debug(f\"get_query: query cached for ext {ext}\")\n            return cls._queries[ext]\n\n        lang_name = LANGUAGES[ext]\n        logger.debug(f\"get_query: lang={lang_name}\")\n        query_dir: str = lang_name \n        tags_path: str = os.path.join(QUERIES_ROOT, query_dir, \"tags.scm\")\n        logger.debug(f\"get_query: tags_path={tags_path} exists={os.path.exists(tags_path)}\")\n        if not os.path.exists(tags_path):\n            logger.warning(f\"get_query: tags.scm not found at {tags_path}\")\n            return None\n        try:\n            language = get_language(cast(Any, lang_name))  # type: ignore[arg-type]\n            with open(tags_path, 'r') as f:\n                tags_content = f.read()\n            query = language.query(tags_content)\n            cls._queries[ext] = query\n            logger.debug(f\"get_query: Query loaded successfully for ext {ext}\")\n            return query\n        except Exception as e:\n            logger.error(f\"get_query: Query compile error for ext {ext}: {e}\")\n            logger.error(traceback.format_exc()) # Log stack trace\n            return None",
        "file": "src/kit/tree_sitter_symbol_extractor.py"
      },
      {
        "name": "extract_symbols",
        "type": "method",
        "start_line": 78,
        "end_line": 197,
        "code": "def extract_symbols(ext: str, source_code: str) -> List[Dict[str, Any]]:\n        \"\"\"Extracts symbols from source code using tree-sitter queries.\"\"\"\n        logger.debug(f\"[EXTRACT] Attempting to extract symbols for ext: {ext}\")\n        symbols: List[Dict[str, Any]] = []\n        query = TreeSitterSymbolExtractor.get_query(ext)\n        parser = TreeSitterSymbolExtractor.get_parser(ext)\n\n        if not query or not parser:\n            logger.warning(f\"[EXTRACT] No query or parser available for extension: {ext}\")\n            return []\n\n        try:\n            tree = parser.parse(bytes(source_code, \"utf8\"))\n            root = tree.root_node\n\n            matches = query.matches(root)\n            logger.debug(f\"[EXTRACT] Found {len(matches)} matches.\")\n\n            # matches is List[Tuple[int, Dict[str, Node]]]\n            # Each tuple is (pattern_index, {capture_name: Node})\n            for pattern_index, captures in matches:\n                logger.debug(f\"[MATCH pattern={pattern_index}] Processing match with captures: {list(captures.keys())}\")\n\n                # Determine symbol name: prefer @name, fallback to @type for blocks like terraform/locals\n                node_candidate = None\n                if 'name' in captures:\n                    node_candidate = captures['name']\n                elif 'type' in captures:\n                    node_candidate = captures['type']\n                else:\n                    # Fallback: take the first capture node\n                    first_capture_node = next(iter(captures.values()), None)\n                    if not first_capture_node:\n                        continue\n                    node_candidate = first_capture_node\n\n                # Handle list of nodes (tree-sitter may return a list)\n                if isinstance(node_candidate, list):\n                    if not node_candidate:\n                        continue  # skip empty list\n                    actual_name_node = node_candidate[0]\n                else:\n                    actual_name_node = node_candidate\n\n                # Now extract symbol name as before\n                symbol_name = actual_name_node.text.decode() if hasattr(actual_name_node, 'text') else str(actual_name_node)\n                # HCL: Strip quotes from string literals\n                if ext == '.tf' and hasattr(actual_name_node, 'type') and actual_name_node.type == 'string_lit':\n                    if len(symbol_name) >= 2 and symbol_name.startswith('\"') and symbol_name.endswith('\"'):\n                        symbol_name = symbol_name[1:-1]\n\n                definition_capture = next(((name, node) for name, node in captures.items() if name.startswith(\"definition.\")), None)\n                subtype = None\n                if definition_capture:\n                    definition_capture_name, definition_node = definition_capture\n                    symbol_type = definition_capture_name.split('.')[-1]\n                    # HCL: For resource/data, combine type and name, and set subtype to the specific resource/data type\n                    if ext == '.tf' and symbol_type in [\"resource\", \"data\"]:\n                        type_node = captures.get('type')\n                        if type_node:\n                            if isinstance(type_node, list):\n                                type_node = type_node[0] if type_node else None\n                            if type_node and hasattr(type_node, 'text'):\n                                type_name = type_node.text.decode()\n                                if hasattr(type_node, 'type') and type_node.type == 'string_lit':\n                                    if len(type_name) >= 2 and type_name.startswith('\"') and type_name.endswith('\"'):\n                                        type_name = type_name[1:-1]\n                                symbol_name = f\"{type_name}.{symbol_name}\"\n                                subtype = type_name\n                else:\n                    # Fallback: infer symbol type from first capture label (e.g., 'function', 'class')\n                    fallback_label = next(iter(captures.keys()), 'symbol')\n                    symbol_type = fallback_label.lstrip('definition.').lstrip('@')\n\n                # Determine the node for the full symbol body, its span, and its code content.\n                # Default to actual_name_node if no specific body capture is found.\n                node_for_body_span_and_code = actual_name_node \n                if definition_capture:\n                    _, captured_body_node = definition_capture # This is the node from @definition.foo\n                    temp_body_node = None\n                    if isinstance(captured_body_node, list):\n                        temp_body_node = captured_body_node[0] if captured_body_node else None\n                    else:\n                        temp_body_node = captured_body_node\n                    \n                    if temp_body_node: # If a valid body node was found from definition_capture\n                        node_for_body_span_and_code = temp_body_node\n\n                # Extract start_line, end_line, and code content from node_for_body_span_and_code\n                symbol_start_line = node_for_body_span_and_code.start_point[0]\n                symbol_end_line = node_for_body_span_and_code.end_point[0]\n                \n                if hasattr(node_for_body_span_and_code, 'text') and isinstance(node_for_body_span_and_code.text, bytes):\n                    symbol_code_content = node_for_body_span_and_code.text.decode('utf-8', errors='ignore')\n                elif hasattr(node_for_body_span_and_code, 'start_byte') and hasattr(node_for_body_span_and_code, 'end_byte'):\n                    # Fallback for nodes where .text might not be the full desired content or not directly available as decodable bytes\n                    symbol_code_content = source_code[node_for_body_span_and_code.start_byte:node_for_body_span_and_code.end_byte]\n                else:\n                    # Last resort, if node_for_body_span_and_code is unusual and lacks .text (bytes) or start/end_byte\n                    symbol_code_content = symbol_name # Fallback to just the name string\n\n                symbol = {\n                    \"name\": symbol_name, # symbol_name is from actual_name_node, potentially modified by HCL logic\n                    \"type\": symbol_type,\n                    \"start_line\": symbol_start_line,\n                    \"end_line\": symbol_end_line,\n                    \"code\": symbol_code_content, \n                }\n                if subtype:\n                    symbol[\"subtype\"] = subtype\n                symbols.append(symbol)\n                continue\n\n        except Exception as e:\n            logger.error(f\"[EXTRACT] Error parsing or processing file with ext {ext}: {e}\")\n            logger.error(traceback.format_exc())\n            return [] # Return empty list on error\n\n        logger.debug(f\"[EXTRACT] Finished extraction for ext {ext}. Found {len(symbols)} symbols.\")\n        return symbols",
        "file": "src/kit/tree_sitter_symbol_extractor.py"
      }
    ],
    "src/kit/cli.py": [
      {
        "name": "serve",
        "type": "function",
        "start_line": 7,
        "end_line": 24,
        "code": "def serve(host: str = \"0.0.0.0\", port: int = 8000, reload: bool = True):\n    \"\"\"Run the kit REST API server (requires `kit[api]` dependencies).\"\"\"\n    try:\n        import uvicorn\n        from kit.api import app as fastapi_app  # Import the FastAPI app instance\n    except ImportError:\n        typer.secho(\n            \"Error: FastAPI or Uvicorn not installed. Please run `pip install kit[api]`\",\n            fg=typer.colors.RED,\n        )\n        raise typer.Exit(code=1)\n\n    typer.echo(f\"Starting kit API server on http://{host}:{port}\")\n    # When reload=True, we must use import string instead of app instance\n    if reload:\n        uvicorn.run(\"kit.api.app:app\", host=host, port=port, reload=reload)\n    else:\n        uvicorn.run(fastapi_app, host=host, port=port, reload=reload)",
        "file": "src/kit/cli.py"
      },
      {
        "name": "search",
        "type": "function",
        "start_line": 28,
        "end_line": 46,
        "code": "def search(\n    path: str = typer.Argument(..., help=\"Path to the local repository.\"),\n    query: str = typer.Argument(..., help=\"Text or regex pattern to search for.\"),\n    pattern: str = typer.Option(\"*.py\", \"--pattern\", \"-p\", help=\"Glob pattern for files to search.\")\n):\n    \"\"\"Perform a textual search in a local repository.\"\"\"\n    from kit import Repository  # Local import to avoid circular deps if CLI is imported elsewhere\n\n    try:\n        repo = Repository(path)\n        results = repo.search_text(query, file_pattern=pattern)\n        if results:\n            for res in results:\n                typer.echo(f\"{res['file']}:{res['line_number']}: {res['line'].strip()}\")\n        else:\n            typer.echo(\"No results found.\")\n    except Exception as e:\n        typer.secho(f\"Error: {e}\", fg=typer.colors.RED)\n        raise typer.Exit(code=1)",
        "file": "src/kit/cli.py"
      }
    ],
    "src/kit/code_searcher.py": [
      {
        "name": "SearchOptions",
        "type": "class",
        "start_line": 8,
        "end_line": 14,
        "code": "class SearchOptions:\n    \"\"\"Configuration options for text search.\"\"\"\n    case_sensitive: bool = True\n    context_lines_before: int = 0\n    context_lines_after: int = 0\n    use_gitignore: bool = True # New option for gitignore\n    # Future options: whole_word: bool = False, exclude_patterns: List[str] = field(default_factory=list)",
        "file": "src/kit/code_searcher.py"
      },
      {
        "name": "CodeSearcher",
        "type": "class",
        "start_line": 16,
        "end_line": 112,
        "code": "class CodeSearcher:\n    \"\"\"\n    Provides text and regex search across the repository.\n    Supports multi-language, file patterns, and returns match details.\n    \"\"\"\n    def __init__(self, repo_path: str) -> None:\n        \"\"\"\n        Initializes the CodeSearcher with the repository path.\n        \n        Args:\n        repo_path (str): The path to the repository.\n        \"\"\"\n        self.repo_path: Path = Path(repo_path)\n        self._gitignore_spec = self._load_gitignore() # Load gitignore spec\n\n    def _load_gitignore(self):\n        \"\"\"Loads .gitignore rules from the repository root.\"\"\"\n        gitignore_path = self.repo_path / '.gitignore'\n        if gitignore_path.exists():\n            try:\n                with open(gitignore_path, 'r', encoding='utf-8') as f:\n                    return pathspec.PathSpec.from_lines('gitwildmatch', f)\n            except Exception as e:\n                # Log this error if logging is set up, or print\n                print(f\"Warning: Could not load .gitignore: {e}\")\n        return None\n\n    def _should_ignore(self, file: Path) -> bool:\n        \"\"\"Checks if a file should be ignored based on .gitignore rules.\"\"\"\n        if not self._gitignore_spec:\n            return False\n        \n        # Always ignore .git directory contents directly if pathspec doesn't catch it implicitly\n        # (though pathspec usually handles .git/ if specified in .gitignore)\n        if '.git' in file.parts:\n             return True\n\n        try:\n            rel_path = str(file.relative_to(self.repo_path))\n            return self._gitignore_spec.match_file(rel_path)\n        except ValueError: # file might not be relative to repo_path, e.g. symlink target outside\n            return False # Or decide to ignore such cases explicitly\n\n    def search_text(self, query: str, file_pattern: str = \"*.py\", options: Optional[SearchOptions] = None) -> List[Dict[str, Any]]:\n        \"\"\"\n        Search for a text pattern (regex) in files matching file_pattern.\n        \n        Args:\n            query (str): The text pattern to search for.\n            file_pattern (str): The file pattern to search in. Defaults to \"*.py\".\n            options (Optional[SearchOptions]): Search configuration options.\n        \n        Returns:\n            List[Dict[str, Any]]: A list of matches. Each match includes:\n                - \"file\" (str): Relative path to the file.\n                - \"line_number\" (int): 1-indexed line number of the match.\n                - \"line\" (str): The content of the matching line.\n                - \"context_before\" (List[str]): Lines immediately preceding the match.\n                - \"context_after\" (List[str]): Lines immediately succeeding the match.\n        \"\"\"\n        matches: List[Dict[str, Any]] = []\n        current_options = options or SearchOptions() # Use defaults if none provided\n\n        regex_flags = 0 if current_options.case_sensitive else re.IGNORECASE\n        regex = re.compile(query, regex_flags)\n\n        for file in self.repo_path.rglob(file_pattern):\n            if current_options.use_gitignore and self._should_ignore(file):\n                continue\n            if not file.is_file():\n                continue\n            try:\n                with open(file, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n                    lines = f.readlines() # Read all lines to handle context\n                \n                for i, line_content in enumerate(lines):\n                    if regex.search(line_content):\n                        start_context_before = max(0, i - current_options.context_lines_before)\n                        context_before = [l.rstrip('\\n') for l in lines[start_context_before:i]]\n                        \n                        # Context after should not include the matching line itself\n                        start_context_after = i + 1\n                        end_context_after = start_context_after + current_options.context_lines_after\n                        context_after = [l.rstrip('\\n') for l in lines[start_context_after:end_context_after]]\n\n                        matches.append({\n                            \"file\": str(file.relative_to(self.repo_path)),\n                            \"line_number\": i + 1, # 1-indexed\n                            \"line\": line_content.rstrip('\\n'),\n                            \"context_before\": context_before,\n                            \"context_after\": context_after\n                        })\n            except Exception as e:\n                # Log the exception for debugging purposes\n                print(f\"Error searching file {file}: {e}\")\n                continue\n        return matches",
        "file": "src/kit/code_searcher.py"
      },
      {
        "name": "__init__",
        "type": "method",
        "start_line": 21,
        "end_line": 29,
        "code": "def __init__(self, repo_path: str) -> None:\n        \"\"\"\n        Initializes the CodeSearcher with the repository path.\n        \n        Args:\n        repo_path (str): The path to the repository.\n        \"\"\"\n        self.repo_path: Path = Path(repo_path)\n        self._gitignore_spec = self._load_gitignore() # Load gitignore spec",
        "file": "src/kit/code_searcher.py"
      },
      {
        "name": "_load_gitignore",
        "type": "method",
        "start_line": 31,
        "end_line": 41,
        "code": "def _load_gitignore(self):\n        \"\"\"Loads .gitignore rules from the repository root.\"\"\"\n        gitignore_path = self.repo_path / '.gitignore'\n        if gitignore_path.exists():\n            try:\n                with open(gitignore_path, 'r', encoding='utf-8') as f:\n                    return pathspec.PathSpec.from_lines('gitwildmatch', f)\n            except Exception as e:\n                # Log this error if logging is set up, or print\n                print(f\"Warning: Could not load .gitignore: {e}\")\n        return None",
        "file": "src/kit/code_searcher.py"
      },
      {
        "name": "_should_ignore",
        "type": "method",
        "start_line": 43,
        "end_line": 57,
        "code": "def _should_ignore(self, file: Path) -> bool:\n        \"\"\"Checks if a file should be ignored based on .gitignore rules.\"\"\"\n        if not self._gitignore_spec:\n            return False\n        \n        # Always ignore .git directory contents directly if pathspec doesn't catch it implicitly\n        # (though pathspec usually handles .git/ if specified in .gitignore)\n        if '.git' in file.parts:\n             return True\n\n        try:\n            rel_path = str(file.relative_to(self.repo_path))\n            return self._gitignore_spec.match_file(rel_path)\n        except ValueError: # file might not be relative to repo_path, e.g. symlink target outside\n            return False # Or decide to ignore such cases explicitly",
        "file": "src/kit/code_searcher.py"
      },
      {
        "name": "search_text",
        "type": "method",
        "start_line": 59,
        "end_line": 112,
        "code": "def search_text(self, query: str, file_pattern: str = \"*.py\", options: Optional[SearchOptions] = None) -> List[Dict[str, Any]]:\n        \"\"\"\n        Search for a text pattern (regex) in files matching file_pattern.\n        \n        Args:\n            query (str): The text pattern to search for.\n            file_pattern (str): The file pattern to search in. Defaults to \"*.py\".\n            options (Optional[SearchOptions]): Search configuration options.\n        \n        Returns:\n            List[Dict[str, Any]]: A list of matches. Each match includes:\n                - \"file\" (str): Relative path to the file.\n                - \"line_number\" (int): 1-indexed line number of the match.\n                - \"line\" (str): The content of the matching line.\n                - \"context_before\" (List[str]): Lines immediately preceding the match.\n                - \"context_after\" (List[str]): Lines immediately succeeding the match.\n        \"\"\"\n        matches: List[Dict[str, Any]] = []\n        current_options = options or SearchOptions() # Use defaults if none provided\n\n        regex_flags = 0 if current_options.case_sensitive else re.IGNORECASE\n        regex = re.compile(query, regex_flags)\n\n        for file in self.repo_path.rglob(file_pattern):\n            if current_options.use_gitignore and self._should_ignore(file):\n                continue\n            if not file.is_file():\n                continue\n            try:\n                with open(file, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n                    lines = f.readlines() # Read all lines to handle context\n                \n                for i, line_content in enumerate(lines):\n                    if regex.search(line_content):\n                        start_context_before = max(0, i - current_options.context_lines_before)\n                        context_before = [l.rstrip('\\n') for l in lines[start_context_before:i]]\n                        \n                        # Context after should not include the matching line itself\n                        start_context_after = i + 1\n                        end_context_after = start_context_after + current_options.context_lines_after\n                        context_after = [l.rstrip('\\n') for l in lines[start_context_after:end_context_after]]\n\n                        matches.append({\n                            \"file\": str(file.relative_to(self.repo_path)),\n                            \"line_number\": i + 1, # 1-indexed\n                            \"line\": line_content.rstrip('\\n'),\n                            \"context_before\": context_before,\n                            \"context_after\": context_after\n                        })\n            except Exception as e:\n                # Log the exception for debugging purposes\n                print(f\"Error searching file {file}: {e}\")\n                continue\n        return matches",
        "file": "src/kit/code_searcher.py"
      }
    ],
    "src/kit/repository.py": [
      {
        "name": "Repository",
        "type": "class",
        "start_line": 17,
        "end_line": 354,
        "code": "class Repository:\n    \"\"\"\n    Main interface for codebase operations: file tree, symbol extraction, search, and context.\n    Provides a unified API for downstream tools and workflows.\n    \"\"\"\n    def __init__(self, path_or_url: str, github_token: Optional[str] = None, cache_dir: Optional[str] = None) -> None:\n        if path_or_url.startswith(\"http://\") or path_or_url.startswith(\"https://\"):  # Remote repo\n            self.local_path = self._clone_github_repo(path_or_url, github_token, cache_dir)\n        else:\n            self.local_path = Path(path_or_url).resolve()\n        self.repo_path: str = str(self.local_path)\n        self.mapper: RepoMapper = RepoMapper(self.repo_path)\n        self.searcher: CodeSearcher = CodeSearcher(self.repo_path)\n        self.context: ContextExtractor = ContextExtractor(self.repo_path)\n        self.vector_searcher: Optional[VectorSearcher] = None\n\n    def __str__(self) -> str:\n        file_count = len(self.get_file_tree())\n        # The self.repo_path is already a string, set in __init__\n        path_info = self.repo_path \n        \n        # Check if it's a git repo and try to get ref.\n        # This assumes local_path is a Path object and points to a git repo.\n        ref_info = \"\"\n        # self.local_path is already a Path object from __init__\n        git_dir = self.local_path / \".git\"\n        if git_dir.exists() and git_dir.is_dir():\n            try:\n                # Get current branch name\n                branch_cmd = [\"git\", \"rev-parse\", \"--abbrev-ref\", \"HEAD\"]\n                # Use self.repo_path (string) for cwd as subprocess expects string path\n                branch_result = subprocess.run(branch_cmd, cwd=self.repo_path, capture_output=True, text=True, check=False)\n                if branch_result.returncode == 0 and branch_result.stdout.strip() != \"HEAD\":\n                    ref_info = f\", branch: {branch_result.stdout.strip()}\"\n                else:\n                    # If not on a branch (detached HEAD), get commit SHA\n                    sha_cmd = [\"git\", \"rev-parse\", \"--short\", \"HEAD\"]\n                    sha_result = subprocess.run(sha_cmd, cwd=self.repo_path, capture_output=True, text=True, check=False)\n                    if sha_result.returncode == 0:\n                        ref_info = f\", commit: {sha_result.stdout.strip()}\"\n            except Exception:\n                pass # Silently ignore errors in getting git info for __str__\n\n        return f\"<Repository path='{path_info}'{ref_info}, files: {file_count}>\"\n\n    def _clone_github_repo(self, url: str, token: Optional[str], cache_dir: Optional[str]) -> Path:\n        from urllib.parse import urlparse\n        \n        repo_name = urlparse(url).path.strip(\"/\").replace(\"/\", \"-\")\n        cache_root = Path(cache_dir or tempfile.gettempdir()) / \"kit-repo-cache\"\n        cache_root.mkdir(parents=True, exist_ok=True)\n        \n        repo_path = cache_root / repo_name\n        if repo_path.exists() and (repo_path / \".git\").exists():\n            # Optionally: git pull to update\n            return repo_path\n        \n        clone_url = url\n        \n        if token:\n            # Insert token for private repos\n            clone_url = url.replace(\"https://\", f\"https://{token}@\")\n        subprocess.run([\"git\", \"clone\", \"--depth=1\", clone_url, str(repo_path)], check=True)\n        return repo_path\n\n    def get_file_tree(self) -> List[Dict[str, Any]]:\n        \"\"\"\n        Returns the file tree of the repository.\n        \n        Returns:\n            List[Dict[str, Any]]: A list of dictionaries representing the file tree.\n        \"\"\"\n        return self.mapper.get_file_tree()\n\n    def extract_symbols(self, file_path: Optional[str] = None) -> List[Dict[str, Any]]:\n        \"\"\"\n        Extracts symbols from the repository.\n        \n        Args:\n            file_path (Optional[str], optional): The path to the file to extract symbols from. Defaults to None.\n        \n        Returns:\n            List[Dict[str, Any]]: A list of dictionaries representing the extracted symbols.\n        \"\"\"\n        return self.mapper.extract_symbols(file_path)  # type: ignore[arg-type]\n\n    def search_text(self, query: str, file_pattern: str = \"*\") -> List[Dict[str, Any]]:\n        \"\"\"\n        Searches for text in the repository.\n        \n        Args:\n            query (str): The text to search for.\n            file_pattern (str, optional): The file pattern to search in. Defaults to \"*\".\n        \n        Returns:\n            List[Dict[str, Any]]: A list of dictionaries representing the search results.\n        \"\"\"\n        return self.searcher.search_text(query, file_pattern)\n\n    def chunk_file_by_lines(self, file_path: str, max_lines: int = 50) -> List[str]:\n        \"\"\"\n        Chunks a file into lines.\n        \n        Args:\n            file_path (str): The path to the file to chunk.\n            max_lines (int, optional): The maximum number of lines to chunk. Defaults to 50.\n        \n        Returns:\n            List[str]: A list of strings representing the chunked lines.\n        \"\"\"\n        return self.context.chunk_file_by_lines(file_path, max_lines)\n\n    def chunk_file_by_symbols(self, file_path: str) -> List[Dict[str, Any]]:\n        \"\"\"\n        Chunks a file into symbols.\n        \n        Args:\n            file_path (str): The path to the file to chunk.\n        \n        Returns:\n            List[Dict[str, Any]]: A list of dictionaries representing the chunked symbols.\n        \"\"\"\n        return self.context.chunk_file_by_symbols(file_path)\n\n    def extract_context_around_line(self, file_path: str, line: int) -> Optional[Dict[str, Any]]:\n        \"\"\"\n        Extracts context around a line in a file.\n        \n        Args:\n            file_path (str): The path to the file to extract context from.\n            line (int): The line number to extract context around.\n        \n        Returns:\n            Optional[Dict[str, Any]]: A dictionary representing the extracted context, or None if not found.\n        \"\"\"\n        return self.context.extract_context_around_line(file_path, line)\n\n    def get_file_content(self, file_path: str) -> str:\n        \"\"\"\n        Reads and returns the content of a file within the repository.\n        \n        Args:\n            file_path (str): The path to the file, relative to the repository root.\n        \n        Returns:\n            str: The content of the file.\n        \n        Raises:\n            FileNotFoundError: If the file does not exist within the repository.\n        \"\"\"\n        full_path = self.local_path / file_path\n        if not full_path.is_file():\n            raise FileNotFoundError(f\"File not found in repository: {file_path}\")\n        try:\n            with open(full_path, 'r', encoding='utf-8') as f:\n                return f.read()\n        except Exception as e:\n            # Catch potential decoding errors or other file reading issues\n            raise IOError(f\"Error reading file {file_path}: {e}\") from e\n\n    def index(self) -> Dict[str, Any]:\n        \"\"\"\n        Builds and returns a full index of the repo, including file tree and symbols.\n        \n        Returns:\n            Dict[str, Any]: A dictionary representing the index.\n        \"\"\"\n        tree = self.get_file_tree()\n        return {\n            \"file_tree\": tree,  # legacy key\n            \"files\": tree,      # preferred\n            \"symbols\": self.mapper.get_repo_map()[\"symbols\"],\n        }\n\n    def get_vector_searcher(self, embed_fn=None, backend=None, persist_dir=None):\n        if self.vector_searcher is None:\n            if embed_fn is None:\n                raise ValueError(\"embed_fn must be provided on first use (e.g. OpenAI/HF embedding function)\")\n            self.vector_searcher = VectorSearcher(self, embed_fn, backend=backend, persist_dir=persist_dir)\n        return self.vector_searcher\n\n    def search_semantic(self, query: str, top_k: int = 5, embed_fn=None) -> List[Dict[str, Any]]:\n        vs = self.get_vector_searcher(embed_fn=embed_fn)\n        return vs.search(query, top_k=top_k)\n\n    def get_summarizer(self, config: Optional[Union['OpenAIConfig', 'AnthropicConfig', 'GoogleConfig']] = None) -> 'Summarizer': \n        \"\"\"\n        Factory method to get a Summarizer instance configured for this repository.\n        \n        Requires LLM dependencies (e.g., openai, anthropic, google-generativeai) to be installed.\n        Example: `pip install kit[openai,anthropic,google]` or the specific one needed.\n        \n        Args:\n            config: Optional configuration object (e.g., OpenAIConfig, AnthropicConfig, GoogleConfig). \n                    If None, defaults to OpenAIConfig using environment variables.\n        \n        Returns:\n            A Summarizer instance ready to use.\n        \n        Raises:\n            ImportError: If required LLM libraries are not installed.\n            ValueError: If configuration (like API key) is missing.\n        \"\"\"\n        # Lazy import Summarizer and its config here to avoid mandatory dependency\n        try:\n            from .summaries import Summarizer, OpenAIConfig, AnthropicConfig, GoogleConfig\n        except ImportError as e:\n             raise ImportError(\n                 \"Summarizer dependencies not found. Did you install kit with LLM extras (e.g., kit[openai])?\"\n             ) from e\n\n        # Determine config: use provided or default (which checks env vars)\n        # If no config is provided, it defaults to OpenAIConfig. Users must explicitly pass\n        # AnthropicConfig or GoogleConfig if they want to use those providers.\n        llm_config = config if config is not None else OpenAIConfig()\n        \n        # Check if the provided or default config is one of the supported types\n        if not isinstance(llm_config, (OpenAIConfig, AnthropicConfig, GoogleConfig)):\n             raise NotImplementedError(\n                 f\"Unsupported configuration type: {type(llm_config)}. Supported types are OpenAIConfig, AnthropicConfig, GoogleConfig.\"\n             )\n        else:\n            # Return the initialized Summarizer\n            return Summarizer(repo=self, config=llm_config)\n\n\n    def get_context_assembler(self) -> 'ContextAssembler':\n        \"\"\"Return a ContextAssembler bound to this repository.\"\"\"\n        return ContextAssembler(self)\n        \n    def get_dependency_analyzer(self) -> 'DependencyAnalyzer':\n        \"\"\"\n        Factory method to get a DependencyAnalyzer instance configured for this repository.\n        \n        The DependencyAnalyzer helps visualize and analyze dependencies between modules\n        in your codebase, identifying import relationships, cycles, and more.\n        \n        Returns:\n            A DependencyAnalyzer instance bound to this repository.\n            \n        Example:\n            >>> analyzer = repo.get_dependency_analyzer()\n            >>> graph = analyzer.build_dependency_graph()\n            >>> analyzer.export_dependency_graph(output_format=\"dot\", output_path=\"dependencies.dot\")\n            >>> cycles = analyzer.find_cycles()\n        \"\"\"\n        from .dependency_analyzer import DependencyAnalyzer\n        return DependencyAnalyzer(self)\n\n    def find_symbol_usages(self, symbol_name: str, symbol_type: Optional[str] = None) -> List[Dict[str, Any]]:\n        \"\"\"\n        Finds all usages of a symbol (by name and optional type) across the repo's indexed symbols.\n        Args:\n            symbol_name (str): The name of the symbol to search for.\n            symbol_type (Optional[str], optional): Optionally restrict to a symbol type (e.g., 'function', 'class').\n        Returns:\n            List[Dict[str, Any]]: List of usage dicts with file, line, and context if available.\n        \"\"\"\n        usages = []\n        repo_map = self.mapper.get_repo_map()\n        for file, symbols in repo_map[\"symbols\"].items():\n            for sym in symbols:\n                if sym[\"name\"] == symbol_name and (symbol_type is None or sym[\"type\"] == symbol_type):\n                    usages.append({\n                        \"file\": file,\n                        \"type\": sym[\"type\"],\n                        \"name\": sym[\"name\"],\n                        \"line\": sym.get(\"line\"),\n                        \"context\": sym.get(\"context\")\n                    })\n        # Optionally: search for references (calls/imports) using search_text or static analysis\n        # Here, we do a simple text search for the symbol name in all files\n        text_hits = self.searcher.search_text(symbol_name)\n        for hit in text_hits:\n            usages.append({\n                \"file\": hit.get(\"file\"),\n                \"line\": hit.get(\"line\"),\n                # Always use 'line' or 'line_content' as context for search hits\n                \"context\": hit.get(\"line_content\") or hit.get(\"line\") or \"\"\n            })\n        return usages\n\n    def write_index(self, file_path: str) -> None:\n        \"\"\"\n        Writes the full repo index (file tree and symbols) to a JSON file.\n        Args:\n            file_path (str): The path to the output file.\n        \"\"\"\n        import json\n        with open(file_path, \"w\") as f:\n            json.dump(self.index(), f, indent=2)\n\n    def write_symbols(self, file_path: str, symbols: Optional[list] = None) -> None:\n        \"\"\"\n        Writes all extracted symbols (or provided symbols) to a JSON file.\n        Args:\n            file_path (str): The path to the output file.\n            symbols (Optional[list]): List of symbol dicts. If None, extracts all symbols in the repo.\n        \"\"\"\n        import json\n        syms = symbols if symbols is not None else [s for file_syms in self.index()[\"symbols\"].values() for s in file_syms]\n        with open(file_path, \"w\") as f:\n            json.dump(syms, f, indent=2)\n\n    def write_file_tree(self, file_path: str) -> None:\n        \"\"\"\n        Writes the file tree to a JSON file.\n        Args:\n            file_path (str): The path to the output file.\n        \"\"\"\n        import json\n        with open(file_path, \"w\") as f:\n            json.dump(self.get_file_tree(), f, indent=2)\n\n    def write_symbol_usages(self, symbol_name: str, file_path: str, symbol_type: Optional[str] = None) -> None:\n        \"\"\"\n        Writes all usages of a symbol to a JSON file.\n        Args:\n            symbol_name (str): The name of the symbol.\n            file_path (str): The path to the output file.\n            symbol_type (Optional[str]): Optionally restrict to a symbol type.\n        \"\"\"\n        import json\n        usages = self.find_symbol_usages(symbol_name, symbol_type)\n        with open(file_path, \"w\") as f:\n            json.dump(usages, f, indent=2)\n\n    def get_abs_path(self, relative_path: str) -> str:\n        \"\"\"\n        Resolves a relative path within the repository to an absolute path.\n\n        Args:\n            relative_path: The path relative to the repository root.\n\n        Returns:\n            The absolute path as a string.\n        \"\"\"\n        return str(self.local_path / relative_path)",
        "file": "src/kit/repository.py"
      },
      {
        "name": "__init__",
        "type": "method",
        "start_line": 22,
        "end_line": 31,
        "code": "def __init__(self, path_or_url: str, github_token: Optional[str] = None, cache_dir: Optional[str] = None) -> None:\n        if path_or_url.startswith(\"http://\") or path_or_url.startswith(\"https://\"):  # Remote repo\n            self.local_path = self._clone_github_repo(path_or_url, github_token, cache_dir)\n        else:\n            self.local_path = Path(path_or_url).resolve()\n        self.repo_path: str = str(self.local_path)\n        self.mapper: RepoMapper = RepoMapper(self.repo_path)\n        self.searcher: CodeSearcher = CodeSearcher(self.repo_path)\n        self.context: ContextExtractor = ContextExtractor(self.repo_path)\n        self.vector_searcher: Optional[VectorSearcher] = None",
        "file": "src/kit/repository.py"
      },
      {
        "name": "__str__",
        "type": "method",
        "start_line": 33,
        "end_line": 60,
        "code": "def __str__(self) -> str:\n        file_count = len(self.get_file_tree())\n        # The self.repo_path is already a string, set in __init__\n        path_info = self.repo_path \n        \n        # Check if it's a git repo and try to get ref.\n        # This assumes local_path is a Path object and points to a git repo.\n        ref_info = \"\"\n        # self.local_path is already a Path object from __init__\n        git_dir = self.local_path / \".git\"\n        if git_dir.exists() and git_dir.is_dir():\n            try:\n                # Get current branch name\n                branch_cmd = [\"git\", \"rev-parse\", \"--abbrev-ref\", \"HEAD\"]\n                # Use self.repo_path (string) for cwd as subprocess expects string path\n                branch_result = subprocess.run(branch_cmd, cwd=self.repo_path, capture_output=True, text=True, check=False)\n                if branch_result.returncode == 0 and branch_result.stdout.strip() != \"HEAD\":\n                    ref_info = f\", branch: {branch_result.stdout.strip()}\"\n                else:\n                    # If not on a branch (detached HEAD), get commit SHA\n                    sha_cmd = [\"git\", \"rev-parse\", \"--short\", \"HEAD\"]\n                    sha_result = subprocess.run(sha_cmd, cwd=self.repo_path, capture_output=True, text=True, check=False)\n                    if sha_result.returncode == 0:\n                        ref_info = f\", commit: {sha_result.stdout.strip()}\"\n            except Exception:\n                pass # Silently ignore errors in getting git info for __str__\n\n        return f\"<Repository path='{path_info}'{ref_info}, files: {file_count}>\"",
        "file": "src/kit/repository.py"
      },
      {
        "name": "_clone_github_repo",
        "type": "method",
        "start_line": 62,
        "end_line": 80,
        "code": "def _clone_github_repo(self, url: str, token: Optional[str], cache_dir: Optional[str]) -> Path:\n        from urllib.parse import urlparse\n        \n        repo_name = urlparse(url).path.strip(\"/\").replace(\"/\", \"-\")\n        cache_root = Path(cache_dir or tempfile.gettempdir()) / \"kit-repo-cache\"\n        cache_root.mkdir(parents=True, exist_ok=True)\n        \n        repo_path = cache_root / repo_name\n        if repo_path.exists() and (repo_path / \".git\").exists():\n            # Optionally: git pull to update\n            return repo_path\n        \n        clone_url = url\n        \n        if token:\n            # Insert token for private repos\n            clone_url = url.replace(\"https://\", f\"https://{token}@\")\n        subprocess.run([\"git\", \"clone\", \"--depth=1\", clone_url, str(repo_path)], check=True)\n        return repo_path",
        "file": "src/kit/repository.py"
      },
      {
        "name": "get_file_tree",
        "type": "method",
        "start_line": 82,
        "end_line": 89,
        "code": "def get_file_tree(self) -> List[Dict[str, Any]]:\n        \"\"\"\n        Returns the file tree of the repository.\n        \n        Returns:\n            List[Dict[str, Any]]: A list of dictionaries representing the file tree.\n        \"\"\"\n        return self.mapper.get_file_tree()",
        "file": "src/kit/repository.py"
      },
      {
        "name": "extract_symbols",
        "type": "method",
        "start_line": 91,
        "end_line": 101,
        "code": "def extract_symbols(self, file_path: Optional[str] = None) -> List[Dict[str, Any]]:\n        \"\"\"\n        Extracts symbols from the repository.\n        \n        Args:\n            file_path (Optional[str], optional): The path to the file to extract symbols from. Defaults to None.\n        \n        Returns:\n            List[Dict[str, Any]]: A list of dictionaries representing the extracted symbols.\n        \"\"\"\n        return self.mapper.extract_symbols(file_path)  # type: ignore[arg-type]",
        "file": "src/kit/repository.py"
      },
      {
        "name": "search_text",
        "type": "method",
        "start_line": 103,
        "end_line": 114,
        "code": "def search_text(self, query: str, file_pattern: str = \"*\") -> List[Dict[str, Any]]:\n        \"\"\"\n        Searches for text in the repository.\n        \n        Args:\n            query (str): The text to search for.\n            file_pattern (str, optional): The file pattern to search in. Defaults to \"*\".\n        \n        Returns:\n            List[Dict[str, Any]]: A list of dictionaries representing the search results.\n        \"\"\"\n        return self.searcher.search_text(query, file_pattern)",
        "file": "src/kit/repository.py"
      },
      {
        "name": "chunk_file_by_lines",
        "type": "method",
        "start_line": 116,
        "end_line": 127,
        "code": "def chunk_file_by_lines(self, file_path: str, max_lines: int = 50) -> List[str]:\n        \"\"\"\n        Chunks a file into lines.\n        \n        Args:\n            file_path (str): The path to the file to chunk.\n            max_lines (int, optional): The maximum number of lines to chunk. Defaults to 50.\n        \n        Returns:\n            List[str]: A list of strings representing the chunked lines.\n        \"\"\"\n        return self.context.chunk_file_by_lines(file_path, max_lines)",
        "file": "src/kit/repository.py"
      },
      {
        "name": "chunk_file_by_symbols",
        "type": "method",
        "start_line": 129,
        "end_line": 139,
        "code": "def chunk_file_by_symbols(self, file_path: str) -> List[Dict[str, Any]]:\n        \"\"\"\n        Chunks a file into symbols.\n        \n        Args:\n            file_path (str): The path to the file to chunk.\n        \n        Returns:\n            List[Dict[str, Any]]: A list of dictionaries representing the chunked symbols.\n        \"\"\"\n        return self.context.chunk_file_by_symbols(file_path)",
        "file": "src/kit/repository.py"
      },
      {
        "name": "extract_context_around_line",
        "type": "method",
        "start_line": 141,
        "end_line": 152,
        "code": "def extract_context_around_line(self, file_path: str, line: int) -> Optional[Dict[str, Any]]:\n        \"\"\"\n        Extracts context around a line in a file.\n        \n        Args:\n            file_path (str): The path to the file to extract context from.\n            line (int): The line number to extract context around.\n        \n        Returns:\n            Optional[Dict[str, Any]]: A dictionary representing the extracted context, or None if not found.\n        \"\"\"\n        return self.context.extract_context_around_line(file_path, line)",
        "file": "src/kit/repository.py"
      },
      {
        "name": "get_file_content",
        "type": "method",
        "start_line": 154,
        "end_line": 175,
        "code": "def get_file_content(self, file_path: str) -> str:\n        \"\"\"\n        Reads and returns the content of a file within the repository.\n        \n        Args:\n            file_path (str): The path to the file, relative to the repository root.\n        \n        Returns:\n            str: The content of the file.\n        \n        Raises:\n            FileNotFoundError: If the file does not exist within the repository.\n        \"\"\"\n        full_path = self.local_path / file_path\n        if not full_path.is_file():\n            raise FileNotFoundError(f\"File not found in repository: {file_path}\")\n        try:\n            with open(full_path, 'r', encoding='utf-8') as f:\n                return f.read()\n        except Exception as e:\n            # Catch potential decoding errors or other file reading issues\n            raise IOError(f\"Error reading file {file_path}: {e}\") from e",
        "file": "src/kit/repository.py"
      },
      {
        "name": "index",
        "type": "method",
        "start_line": 177,
        "end_line": 189,
        "code": "def index(self) -> Dict[str, Any]:\n        \"\"\"\n        Builds and returns a full index of the repo, including file tree and symbols.\n        \n        Returns:\n            Dict[str, Any]: A dictionary representing the index.\n        \"\"\"\n        tree = self.get_file_tree()\n        return {\n            \"file_tree\": tree,  # legacy key\n            \"files\": tree,      # preferred\n            \"symbols\": self.mapper.get_repo_map()[\"symbols\"],\n        }",
        "file": "src/kit/repository.py"
      },
      {
        "name": "get_vector_searcher",
        "type": "method",
        "start_line": 191,
        "end_line": 196,
        "code": "def get_vector_searcher(self, embed_fn=None, backend=None, persist_dir=None):\n        if self.vector_searcher is None:\n            if embed_fn is None:\n                raise ValueError(\"embed_fn must be provided on first use (e.g. OpenAI/HF embedding function)\")\n            self.vector_searcher = VectorSearcher(self, embed_fn, backend=backend, persist_dir=persist_dir)\n        return self.vector_searcher",
        "file": "src/kit/repository.py"
      },
      {
        "name": "search_semantic",
        "type": "method",
        "start_line": 198,
        "end_line": 200,
        "code": "def search_semantic(self, query: str, top_k: int = 5, embed_fn=None) -> List[Dict[str, Any]]:\n        vs = self.get_vector_searcher(embed_fn=embed_fn)\n        return vs.search(query, top_k=top_k)",
        "file": "src/kit/repository.py"
      },
      {
        "name": "get_summarizer",
        "type": "method",
        "start_line": 202,
        "end_line": 240,
        "code": "def get_summarizer(self, config: Optional[Union['OpenAIConfig', 'AnthropicConfig', 'GoogleConfig']] = None) -> 'Summarizer': \n        \"\"\"\n        Factory method to get a Summarizer instance configured for this repository.\n        \n        Requires LLM dependencies (e.g., openai, anthropic, google-generativeai) to be installed.\n        Example: `pip install kit[openai,anthropic,google]` or the specific one needed.\n        \n        Args:\n            config: Optional configuration object (e.g., OpenAIConfig, AnthropicConfig, GoogleConfig). \n                    If None, defaults to OpenAIConfig using environment variables.\n        \n        Returns:\n            A Summarizer instance ready to use.\n        \n        Raises:\n            ImportError: If required LLM libraries are not installed.\n            ValueError: If configuration (like API key) is missing.\n        \"\"\"\n        # Lazy import Summarizer and its config here to avoid mandatory dependency\n        try:\n            from .summaries import Summarizer, OpenAIConfig, AnthropicConfig, GoogleConfig\n        except ImportError as e:\n             raise ImportError(\n                 \"Summarizer dependencies not found. Did you install kit with LLM extras (e.g., kit[openai])?\"\n             ) from e\n\n        # Determine config: use provided or default (which checks env vars)\n        # If no config is provided, it defaults to OpenAIConfig. Users must explicitly pass\n        # AnthropicConfig or GoogleConfig if they want to use those providers.\n        llm_config = config if config is not None else OpenAIConfig()\n        \n        # Check if the provided or default config is one of the supported types\n        if not isinstance(llm_config, (OpenAIConfig, AnthropicConfig, GoogleConfig)):\n             raise NotImplementedError(\n                 f\"Unsupported configuration type: {type(llm_config)}. Supported types are OpenAIConfig, AnthropicConfig, GoogleConfig.\"\n             )\n        else:\n            # Return the initialized Summarizer\n            return Summarizer(repo=self, config=llm_config)",
        "file": "src/kit/repository.py"
      },
      {
        "name": "get_context_assembler",
        "type": "method",
        "start_line": 243,
        "end_line": 245,
        "code": "def get_context_assembler(self) -> 'ContextAssembler':\n        \"\"\"Return a ContextAssembler bound to this repository.\"\"\"\n        return ContextAssembler(self)",
        "file": "src/kit/repository.py"
      },
      {
        "name": "get_dependency_analyzer",
        "type": "method",
        "start_line": 247,
        "end_line": 264,
        "code": "def get_dependency_analyzer(self) -> 'DependencyAnalyzer':\n        \"\"\"\n        Factory method to get a DependencyAnalyzer instance configured for this repository.\n        \n        The DependencyAnalyzer helps visualize and analyze dependencies between modules\n        in your codebase, identifying import relationships, cycles, and more.\n        \n        Returns:\n            A DependencyAnalyzer instance bound to this repository.\n            \n        Example:\n            >>> analyzer = repo.get_dependency_analyzer()\n            >>> graph = analyzer.build_dependency_graph()\n            >>> analyzer.export_dependency_graph(output_format=\"dot\", output_path=\"dependencies.dot\")\n            >>> cycles = analyzer.find_cycles()\n        \"\"\"\n        from .dependency_analyzer import DependencyAnalyzer\n        return DependencyAnalyzer(self)",
        "file": "src/kit/repository.py"
      },
      {
        "name": "find_symbol_usages",
        "type": "method",
        "start_line": 266,
        "end_line": 297,
        "code": "def find_symbol_usages(self, symbol_name: str, symbol_type: Optional[str] = None) -> List[Dict[str, Any]]:\n        \"\"\"\n        Finds all usages of a symbol (by name and optional type) across the repo's indexed symbols.\n        Args:\n            symbol_name (str): The name of the symbol to search for.\n            symbol_type (Optional[str], optional): Optionally restrict to a symbol type (e.g., 'function', 'class').\n        Returns:\n            List[Dict[str, Any]]: List of usage dicts with file, line, and context if available.\n        \"\"\"\n        usages = []\n        repo_map = self.mapper.get_repo_map()\n        for file, symbols in repo_map[\"symbols\"].items():\n            for sym in symbols:\n                if sym[\"name\"] == symbol_name and (symbol_type is None or sym[\"type\"] == symbol_type):\n                    usages.append({\n                        \"file\": file,\n                        \"type\": sym[\"type\"],\n                        \"name\": sym[\"name\"],\n                        \"line\": sym.get(\"line\"),\n                        \"context\": sym.get(\"context\")\n                    })\n        # Optionally: search for references (calls/imports) using search_text or static analysis\n        # Here, we do a simple text search for the symbol name in all files\n        text_hits = self.searcher.search_text(symbol_name)\n        for hit in text_hits:\n            usages.append({\n                \"file\": hit.get(\"file\"),\n                \"line\": hit.get(\"line\"),\n                # Always use 'line' or 'line_content' as context for search hits\n                \"context\": hit.get(\"line_content\") or hit.get(\"line\") or \"\"\n            })\n        return usages",
        "file": "src/kit/repository.py"
      },
      {
        "name": "write_index",
        "type": "method",
        "start_line": 299,
        "end_line": 307,
        "code": "def write_index(self, file_path: str) -> None:\n        \"\"\"\n        Writes the full repo index (file tree and symbols) to a JSON file.\n        Args:\n            file_path (str): The path to the output file.\n        \"\"\"\n        import json\n        with open(file_path, \"w\") as f:\n            json.dump(self.index(), f, indent=2)",
        "file": "src/kit/repository.py"
      },
      {
        "name": "write_symbols",
        "type": "method",
        "start_line": 309,
        "end_line": 319,
        "code": "def write_symbols(self, file_path: str, symbols: Optional[list] = None) -> None:\n        \"\"\"\n        Writes all extracted symbols (or provided symbols) to a JSON file.\n        Args:\n            file_path (str): The path to the output file.\n            symbols (Optional[list]): List of symbol dicts. If None, extracts all symbols in the repo.\n        \"\"\"\n        import json\n        syms = symbols if symbols is not None else [s for file_syms in self.index()[\"symbols\"].values() for s in file_syms]\n        with open(file_path, \"w\") as f:\n            json.dump(syms, f, indent=2)",
        "file": "src/kit/repository.py"
      },
      {
        "name": "write_file_tree",
        "type": "method",
        "start_line": 321,
        "end_line": 329,
        "code": "def write_file_tree(self, file_path: str) -> None:\n        \"\"\"\n        Writes the file tree to a JSON file.\n        Args:\n            file_path (str): The path to the output file.\n        \"\"\"\n        import json\n        with open(file_path, \"w\") as f:\n            json.dump(self.get_file_tree(), f, indent=2)",
        "file": "src/kit/repository.py"
      },
      {
        "name": "write_symbol_usages",
        "type": "method",
        "start_line": 331,
        "end_line": 342,
        "code": "def write_symbol_usages(self, symbol_name: str, file_path: str, symbol_type: Optional[str] = None) -> None:\n        \"\"\"\n        Writes all usages of a symbol to a JSON file.\n        Args:\n            symbol_name (str): The name of the symbol.\n            file_path (str): The path to the output file.\n            symbol_type (Optional[str]): Optionally restrict to a symbol type.\n        \"\"\"\n        import json\n        usages = self.find_symbol_usages(symbol_name, symbol_type)\n        with open(file_path, \"w\") as f:\n            json.dump(usages, f, indent=2)",
        "file": "src/kit/repository.py"
      },
      {
        "name": "get_abs_path",
        "type": "method",
        "start_line": 344,
        "end_line": 354,
        "code": "def get_abs_path(self, relative_path: str) -> str:\n        \"\"\"\n        Resolves a relative path within the repository to an absolute path.\n\n        Args:\n            relative_path: The path relative to the repository root.\n\n        Returns:\n            The absolute path as a string.\n        \"\"\"\n        return str(self.local_path / relative_path)",
        "file": "src/kit/repository.py"
      }
    ],
    "src/kit/repo_mapper.py": [
      {
        "name": "RepoMapper",
        "type": "class",
        "start_line": 9,
        "end_line": 148,
        "code": "class RepoMapper:\n    \"\"\"\n    Maps the structure and symbols of a code repository.\n    Implements incremental scanning and robust symbol extraction.\n    Supports multi-language via tree-sitter queries.\n    \"\"\"\n    def __init__(self, repo_path: str) -> None:\n        self.repo_path: Path = Path(repo_path)\n        self._symbol_map: Dict[str, Dict[str, Any]] = {}  # file -> {mtime, symbols}\n        self._file_tree: Optional[List[Dict[str, Any]]] = None\n        self._gitignore_spec = self._load_gitignore()\n\n    def _load_gitignore(self):\n        gitignore_path = self.repo_path / '.gitignore'\n        if gitignore_path.exists():\n            with open(gitignore_path) as f:\n                return pathspec.PathSpec.from_lines('gitwildmatch', f)\n        return None\n\n    def _should_ignore(self, file: Path) -> bool:\n        rel_path = str(file.relative_to(self.repo_path))\n        # Always ignore .git and its contents\n        if '.git' in file.parts:\n            return True\n        # Ignore files matching .gitignore\n        if self._gitignore_spec and self._gitignore_spec.match_file(rel_path):\n            return True\n        return False\n\n    def get_file_tree(self) -> List[Dict[str, Any]]:\n        \"\"\"\n        Returns a list of dicts representing all files in the repo.\n        Each dict contains: path, size, mtime, is_file.\n        \"\"\"\n        if self._file_tree is not None:\n            return self._file_tree\n        tree = []\n        for path in self.repo_path.rglob(\"*\"):\n            if self._should_ignore(path):\n                continue\n            tree.append({\n                \"path\": str(path.relative_to(self.repo_path)),\n                \"is_dir\": path.is_dir(),\n                \"name\": path.name,\n                \"size\": path.stat().st_size if path.is_file() else 0\n            })\n        self._file_tree = tree\n        return tree\n\n    def scan_repo(self) -> None:\n        \"\"\"\n        Scan all supported files and update symbol map incrementally.\n        Uses mtime to avoid redundant parsing.\n        \"\"\"\n        for file in self.repo_path.rglob(\"*\"):\n            if not file.is_file():\n                continue\n            if self._should_ignore(file):\n                continue\n            ext = file.suffix.lower()\n            if ext in TreeSitterSymbolExtractor.LANGUAGES or ext == \".py\":\n                self._scan_file(file)\n\n    def _scan_file(self, file: Path) -> None:\n        try:\n            mtime: float = os.path.getmtime(file)\n            entry = self._symbol_map.get(str(file))\n            if entry and entry[\"mtime\"] == mtime:\n                return  # No change\n            symbols: List[Dict[str, Any]] = self._extract_symbols_from_file(file)\n            self._symbol_map[str(file)] = {\"mtime\": mtime, \"symbols\": symbols}\n        except Exception as e:\n            logging.warning(f\"Error scanning file {file}: {e}\", exc_info=True)\n\n    def _extract_symbols_from_file(self, file: Path) -> List[Dict[str, Any]]:\n        ext = file.suffix.lower()\n        try:\n            with open(file, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n                code = f.read()\n        except Exception as e:\n            logging.warning(f\"Could not read file {file} for symbol extraction: {e}\")\n            return []\n        if ext in TreeSitterSymbolExtractor.LANGUAGES:\n            try:\n                symbols = TreeSitterSymbolExtractor.extract_symbols(ext, code)\n                for s in symbols:\n                    s[\"file\"] = str(file)\n                return symbols\n            except Exception as e:\n                logging.warning(f\"Error extracting symbols from {file} using TreeSitter: {e}\")\n                return []\n        return []\n\n    def extract_symbols(self, file_path: str) -> List[Dict[str, Any]]:\n        \"\"\"\n        Extracts symbols from a single specified file on demand.\n        This method performs a fresh extraction and does not use the internal cache.\n        For cached or repository-wide symbols, use scan_repo() and get_repo_map().\n\n        Args:\n            file_path (str): The relative path to the file from the repository root.\n\n        Returns:\n            List[Dict[str, Any]]: A list of symbols extracted from the file.\n                                 Returns an empty list if the file is ignored,\n                                 not supported, or if an error occurs.\n        \"\"\"\n        abs_path = self.repo_path / file_path\n        if self._should_ignore(abs_path):\n            logging.debug(f\"Ignoring file specified in extract_symbols: {file_path}\")\n            return []\n\n        ext = abs_path.suffix.lower()\n        if ext in TreeSitterSymbolExtractor.LANGUAGES:\n            try:\n                code = abs_path.read_text(encoding=\"utf-8\", errors=\"ignore\")\n                symbols = TreeSitterSymbolExtractor.extract_symbols(ext, code)\n                for s in symbols:\n                    s[\"file\"] = str(abs_path.relative_to(self.repo_path))\n                return symbols\n            except Exception as e:\n                logging.warning(f\"Error extracting symbols from {abs_path} in extract_symbols: {e}\")\n                return []\n        else:\n            logging.debug(f\"File type {ext} not supported for symbol extraction: {file_path}\")\n            return []\n\n    def get_repo_map(self) -> Dict[str, Any]:\n        \"\"\"\n        Returns a dict with file tree and a mapping of files to their symbols.\n        Ensures the symbol map is up-to-date by scanning the repo and refreshes the file tree.\n        \"\"\"\n        self.scan_repo()\n        self._file_tree = None\n        return {\n            \"file_tree\": self.get_file_tree(),\n            \"symbols\": {k: v[\"symbols\"] for k, v in self._symbol_map.items()}\n        }\n\n    # --- Helper methods ---",
        "file": "src/kit/repo_mapper.py"
      },
      {
        "name": "__init__",
        "type": "method",
        "start_line": 15,
        "end_line": 19,
        "code": "def __init__(self, repo_path: str) -> None:\n        self.repo_path: Path = Path(repo_path)\n        self._symbol_map: Dict[str, Dict[str, Any]] = {}  # file -> {mtime, symbols}\n        self._file_tree: Optional[List[Dict[str, Any]]] = None\n        self._gitignore_spec = self._load_gitignore()",
        "file": "src/kit/repo_mapper.py"
      },
      {
        "name": "_load_gitignore",
        "type": "method",
        "start_line": 21,
        "end_line": 26,
        "code": "def _load_gitignore(self):\n        gitignore_path = self.repo_path / '.gitignore'\n        if gitignore_path.exists():\n            with open(gitignore_path) as f:\n                return pathspec.PathSpec.from_lines('gitwildmatch', f)\n        return None",
        "file": "src/kit/repo_mapper.py"
      },
      {
        "name": "_should_ignore",
        "type": "method",
        "start_line": 28,
        "end_line": 36,
        "code": "def _should_ignore(self, file: Path) -> bool:\n        rel_path = str(file.relative_to(self.repo_path))\n        # Always ignore .git and its contents\n        if '.git' in file.parts:\n            return True\n        # Ignore files matching .gitignore\n        if self._gitignore_spec and self._gitignore_spec.match_file(rel_path):\n            return True\n        return False",
        "file": "src/kit/repo_mapper.py"
      },
      {
        "name": "get_file_tree",
        "type": "method",
        "start_line": 38,
        "end_line": 56,
        "code": "def get_file_tree(self) -> List[Dict[str, Any]]:\n        \"\"\"\n        Returns a list of dicts representing all files in the repo.\n        Each dict contains: path, size, mtime, is_file.\n        \"\"\"\n        if self._file_tree is not None:\n            return self._file_tree\n        tree = []\n        for path in self.repo_path.rglob(\"*\"):\n            if self._should_ignore(path):\n                continue\n            tree.append({\n                \"path\": str(path.relative_to(self.repo_path)),\n                \"is_dir\": path.is_dir(),\n                \"name\": path.name,\n                \"size\": path.stat().st_size if path.is_file() else 0\n            })\n        self._file_tree = tree\n        return tree",
        "file": "src/kit/repo_mapper.py"
      },
      {
        "name": "scan_repo",
        "type": "method",
        "start_line": 58,
        "end_line": 70,
        "code": "def scan_repo(self) -> None:\n        \"\"\"\n        Scan all supported files and update symbol map incrementally.\n        Uses mtime to avoid redundant parsing.\n        \"\"\"\n        for file in self.repo_path.rglob(\"*\"):\n            if not file.is_file():\n                continue\n            if self._should_ignore(file):\n                continue\n            ext = file.suffix.lower()\n            if ext in TreeSitterSymbolExtractor.LANGUAGES or ext == \".py\":\n                self._scan_file(file)",
        "file": "src/kit/repo_mapper.py"
      },
      {
        "name": "_scan_file",
        "type": "method",
        "start_line": 72,
        "end_line": 81,
        "code": "def _scan_file(self, file: Path) -> None:\n        try:\n            mtime: float = os.path.getmtime(file)\n            entry = self._symbol_map.get(str(file))\n            if entry and entry[\"mtime\"] == mtime:\n                return  # No change\n            symbols: List[Dict[str, Any]] = self._extract_symbols_from_file(file)\n            self._symbol_map[str(file)] = {\"mtime\": mtime, \"symbols\": symbols}\n        except Exception as e:\n            logging.warning(f\"Error scanning file {file}: {e}\", exc_info=True)",
        "file": "src/kit/repo_mapper.py"
      },
      {
        "name": "_extract_symbols_from_file",
        "type": "method",
        "start_line": 83,
        "end_line": 100,
        "code": "def _extract_symbols_from_file(self, file: Path) -> List[Dict[str, Any]]:\n        ext = file.suffix.lower()\n        try:\n            with open(file, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n                code = f.read()\n        except Exception as e:\n            logging.warning(f\"Could not read file {file} for symbol extraction: {e}\")\n            return []\n        if ext in TreeSitterSymbolExtractor.LANGUAGES:\n            try:\n                symbols = TreeSitterSymbolExtractor.extract_symbols(ext, code)\n                for s in symbols:\n                    s[\"file\"] = str(file)\n                return symbols\n            except Exception as e:\n                logging.warning(f\"Error extracting symbols from {file} using TreeSitter: {e}\")\n                return []\n        return []",
        "file": "src/kit/repo_mapper.py"
      },
      {
        "name": "extract_symbols",
        "type": "method",
        "start_line": 102,
        "end_line": 134,
        "code": "def extract_symbols(self, file_path: str) -> List[Dict[str, Any]]:\n        \"\"\"\n        Extracts symbols from a single specified file on demand.\n        This method performs a fresh extraction and does not use the internal cache.\n        For cached or repository-wide symbols, use scan_repo() and get_repo_map().\n\n        Args:\n            file_path (str): The relative path to the file from the repository root.\n\n        Returns:\n            List[Dict[str, Any]]: A list of symbols extracted from the file.\n                                 Returns an empty list if the file is ignored,\n                                 not supported, or if an error occurs.\n        \"\"\"\n        abs_path = self.repo_path / file_path\n        if self._should_ignore(abs_path):\n            logging.debug(f\"Ignoring file specified in extract_symbols: {file_path}\")\n            return []\n\n        ext = abs_path.suffix.lower()\n        if ext in TreeSitterSymbolExtractor.LANGUAGES:\n            try:\n                code = abs_path.read_text(encoding=\"utf-8\", errors=\"ignore\")\n                symbols = TreeSitterSymbolExtractor.extract_symbols(ext, code)\n                for s in symbols:\n                    s[\"file\"] = str(abs_path.relative_to(self.repo_path))\n                return symbols\n            except Exception as e:\n                logging.warning(f\"Error extracting symbols from {abs_path} in extract_symbols: {e}\")\n                return []\n        else:\n            logging.debug(f\"File type {ext} not supported for symbol extraction: {file_path}\")\n            return []",
        "file": "src/kit/repo_mapper.py"
      },
      {
        "name": "get_repo_map",
        "type": "method",
        "start_line": 136,
        "end_line": 146,
        "code": "def get_repo_map(self) -> Dict[str, Any]:\n        \"\"\"\n        Returns a dict with file tree and a mapping of files to their symbols.\n        Ensures the symbol map is up-to-date by scanning the repo and refreshes the file tree.\n        \"\"\"\n        self.scan_repo()\n        self._file_tree = None\n        return {\n            \"file_tree\": self.get_file_tree(),\n            \"symbols\": {k: v[\"symbols\"] for k, v in self._symbol_map.items()}\n        }",
        "file": "src/kit/repo_mapper.py"
      }
    ],
    "src/kit/vector_searcher.py": [
      {
        "name": "VectorDBBackend",
        "type": "class",
        "start_line": 11,
        "end_line": 29,
        "code": "class VectorDBBackend:\n    \"\"\"\n    Abstract vector DB interface for pluggable backends.\n    \"\"\"\n    def add(self, embeddings: List[List[float]], metadatas: List[Dict[str, Any]], ids: Optional[List[str]] = None):\n        raise NotImplementedError\n\n    def query(self, embedding: List[float], top_k: int) -> List[Dict[str, Any]]:\n        raise NotImplementedError\n\n    def persist(self):\n        pass\n    \n    def delete(self, ids: List[str]):  # noqa: D401 \u2013 simple interface, no return\n        \"\"\"Remove vectors by their IDs. Backends that don't support fine-grained deletes may no-op.\"\"\"\n        raise NotImplementedError\n\n    def count(self) -> int:\n        raise NotImplementedError",
        "file": "src/kit/vector_searcher.py"
      },
      {
        "name": "add",
        "type": "method",
        "start_line": 15,
        "end_line": 16,
        "code": "def add(self, embeddings: List[List[float]], metadatas: List[Dict[str, Any]], ids: Optional[List[str]] = None):\n        raise NotImplementedError",
        "file": "src/kit/vector_searcher.py"
      },
      {
        "name": "query",
        "type": "method",
        "start_line": 18,
        "end_line": 19,
        "code": "def query(self, embedding: List[float], top_k: int) -> List[Dict[str, Any]]:\n        raise NotImplementedError",
        "file": "src/kit/vector_searcher.py"
      },
      {
        "name": "persist",
        "type": "method",
        "start_line": 21,
        "end_line": 22,
        "code": "def persist(self):\n        pass",
        "file": "src/kit/vector_searcher.py"
      },
      {
        "name": "delete",
        "type": "method",
        "start_line": 24,
        "end_line": 26,
        "code": "def delete(self, ids: List[str]):  # noqa: D401 \u2013 simple interface, no return\n        \"\"\"Remove vectors by their IDs. Backends that don't support fine-grained deletes may no-op.\"\"\"\n        raise NotImplementedError",
        "file": "src/kit/vector_searcher.py"
      },
      {
        "name": "count",
        "type": "method",
        "start_line": 28,
        "end_line": 29,
        "code": "def count(self) -> int:\n        raise NotImplementedError",
        "file": "src/kit/vector_searcher.py"
      },
      {
        "name": "ChromaDBBackend",
        "type": "class",
        "start_line": 31,
        "end_line": 103,
        "code": "class ChromaDBBackend(VectorDBBackend):\n    def __init__(self, persist_dir: str, collection_name: Optional[str] = None):\n        if chromadb is None:\n            raise ImportError(\"chromadb is not installed. Run 'pip install chromadb'.\")\n        self.persist_dir = persist_dir\n        self.client = chromadb.Client(Settings(persist_directory=persist_dir))\n        \n        final_collection_name = collection_name\n        if final_collection_name is None:\n            # Use a collection name scoped to persist_dir to avoid dimension clashes across multiple tests/processes\n            final_collection_name = f\"kit_code_chunks_{abs(hash(persist_dir))}\"\n        self.collection = self.client.get_or_create_collection(final_collection_name)\n\n    def add(self, embeddings, metadatas, ids: Optional[List[str]] = None):\n        # Skip adding if there is nothing to add (prevents ChromaDB error)\n        if not embeddings or not metadatas:\n            return\n        # Clear collection before adding (for index overwrite)\n        # This behavior of clearing the collection on 'add' might need review.\n        # If the goal is to truly overwrite, this is one way. If it's to append\n        # or update, this logic would need to change. For now, assuming overwrite.\n        if self.collection.count() > 0: # Check if collection has items before deleting\n            try:\n                # Attempt to delete all existing documents. This is a common pattern for a full refresh.\n                # Chroma's API for deleting all can be tricky; using a non-empty ID match is a workaround.\n                # If a more direct `clear()` or `delete_all()` method becomes available, prefer that.\n                self.collection.delete(where={\"source\": {\"$ne\": \"impossible_source_value_to_match_all\"}}) # type: ignore[dict-item]\n                # Or, if you know a common metadata key, like 'file_path' from previous version:\n                # self.collection.delete(where={\"file_path\": {\"$ne\": \"impossible_file_path\"}})\n            except Exception as e:\n                # Log or handle cases where delete might fail or is not supported as expected.\n                # For instance, if the collection was empty, some backends might error on delete-all attempts.\n                # logger.warning(f\"Could not clear collection before adding: {e}\")\n                pass # Continue to add, might result in duplicates if not truly cleared.\n\n        final_ids = ids\n        if final_ids is None:\n            final_ids = [str(i) for i in range(len(metadatas))]\n        elif len(final_ids) != len(embeddings):\n            raise ValueError(\"The number of IDs must match the number of embeddings and metadatas.\")\n\n        self.collection.add(embeddings=embeddings, metadatas=metadatas, ids=final_ids)\n\n    def query(self, embedding, top_k):\n        if top_k <= 0:\n            return []\n        results = self.collection.query(query_embeddings=[embedding], n_results=top_k)\n        hits = []\n        for i in range(len(results[\"ids\"][0])):\n            meta = results[\"metadatas\"][0][i]\n            meta[\"score\"] = results[\"distances\"][0][i]\n            hits.append(meta)\n        return hits\n\n    def persist(self):\n        # ChromaDB v1.x does not require or support explicit persist, it is automatic.\n        pass\n\n    def count(self) -> int:\n        return self.collection.count()\n\n    # ------------------------------------------------------------------\n    # Incremental-index support helpers\n    # ------------------------------------------------------------------\n    def delete(self, ids: List[str]):\n        \"\"\"Delete vectors by ID if the underlying collection supports it.\"\"\"\n        if not ids:\n            return\n        try:\n            self.collection.delete(ids=ids)\n        except Exception:\n            # Some Chroma versions require where filter; fall back to no-op\n            pass",
        "file": "src/kit/vector_searcher.py"
      },
      {
        "name": "__init__",
        "type": "method",
        "start_line": 32,
        "end_line": 42,
        "code": "def __init__(self, persist_dir: str, collection_name: Optional[str] = None):\n        if chromadb is None:\n            raise ImportError(\"chromadb is not installed. Run 'pip install chromadb'.\")\n        self.persist_dir = persist_dir\n        self.client = chromadb.Client(Settings(persist_directory=persist_dir))\n        \n        final_collection_name = collection_name\n        if final_collection_name is None:\n            # Use a collection name scoped to persist_dir to avoid dimension clashes across multiple tests/processes\n            final_collection_name = f\"kit_code_chunks_{abs(hash(persist_dir))}\"\n        self.collection = self.client.get_or_create_collection(final_collection_name)",
        "file": "src/kit/vector_searcher.py"
      },
      {
        "name": "add",
        "type": "method",
        "start_line": 44,
        "end_line": 72,
        "code": "def add(self, embeddings, metadatas, ids: Optional[List[str]] = None):\n        # Skip adding if there is nothing to add (prevents ChromaDB error)\n        if not embeddings or not metadatas:\n            return\n        # Clear collection before adding (for index overwrite)\n        # This behavior of clearing the collection on 'add' might need review.\n        # If the goal is to truly overwrite, this is one way. If it's to append\n        # or update, this logic would need to change. For now, assuming overwrite.\n        if self.collection.count() > 0: # Check if collection has items before deleting\n            try:\n                # Attempt to delete all existing documents. This is a common pattern for a full refresh.\n                # Chroma's API for deleting all can be tricky; using a non-empty ID match is a workaround.\n                # If a more direct `clear()` or `delete_all()` method becomes available, prefer that.\n                self.collection.delete(where={\"source\": {\"$ne\": \"impossible_source_value_to_match_all\"}}) # type: ignore[dict-item]\n                # Or, if you know a common metadata key, like 'file_path' from previous version:\n                # self.collection.delete(where={\"file_path\": {\"$ne\": \"impossible_file_path\"}})\n            except Exception as e:\n                # Log or handle cases where delete might fail or is not supported as expected.\n                # For instance, if the collection was empty, some backends might error on delete-all attempts.\n                # logger.warning(f\"Could not clear collection before adding: {e}\")\n                pass # Continue to add, might result in duplicates if not truly cleared.\n\n        final_ids = ids\n        if final_ids is None:\n            final_ids = [str(i) for i in range(len(metadatas))]\n        elif len(final_ids) != len(embeddings):\n            raise ValueError(\"The number of IDs must match the number of embeddings and metadatas.\")\n\n        self.collection.add(embeddings=embeddings, metadatas=metadatas, ids=final_ids)",
        "file": "src/kit/vector_searcher.py"
      },
      {
        "name": "query",
        "type": "method",
        "start_line": 74,
        "end_line": 83,
        "code": "def query(self, embedding, top_k):\n        if top_k <= 0:\n            return []\n        results = self.collection.query(query_embeddings=[embedding], n_results=top_k)\n        hits = []\n        for i in range(len(results[\"ids\"][0])):\n            meta = results[\"metadatas\"][0][i]\n            meta[\"score\"] = results[\"distances\"][0][i]\n            hits.append(meta)\n        return hits",
        "file": "src/kit/vector_searcher.py"
      },
      {
        "name": "persist",
        "type": "method",
        "start_line": 85,
        "end_line": 87,
        "code": "def persist(self):\n        # ChromaDB v1.x does not require or support explicit persist, it is automatic.\n        pass",
        "file": "src/kit/vector_searcher.py"
      },
      {
        "name": "count",
        "type": "method",
        "start_line": 89,
        "end_line": 90,
        "code": "def count(self) -> int:\n        return self.collection.count()",
        "file": "src/kit/vector_searcher.py"
      },
      {
        "name": "delete",
        "type": "method",
        "start_line": 95,
        "end_line": 103,
        "code": "def delete(self, ids: List[str]):\n        \"\"\"Delete vectors by ID if the underlying collection supports it.\"\"\"\n        if not ids:\n            return\n        try:\n            self.collection.delete(ids=ids)\n        except Exception:\n            # Some Chroma versions require where filter; fall back to no-op\n            pass",
        "file": "src/kit/vector_searcher.py"
      },
      {
        "name": "VectorSearcher",
        "type": "class",
        "start_line": 105,
        "end_line": 155,
        "code": "class VectorSearcher:\n    def __init__(self, repo, embed_fn, backend: Optional[VectorDBBackend] = None, persist_dir: Optional[str] = None):\n        self.repo = repo\n        self.embed_fn = embed_fn  # Function: str -> List[float]\n        self.persist_dir = persist_dir or os.path.join(\".kit\", \"vector_db\")\n        self.backend = backend or ChromaDBBackend(self.persist_dir)\n        self.chunk_metadatas: List[Dict[str, Any]] = []\n        self.chunk_embeddings: List[List[float]] = []\n\n    def build_index(self, chunk_by: str = \"symbols\"):\n        self.chunk_metadatas = []\n        chunk_codes: List[str] = []\n\n        for file in self.repo.get_file_tree():\n            if file[\"is_dir\"]:\n                continue\n            path = file[\"path\"]\n            if chunk_by == \"symbols\":\n                chunks = self.repo.chunk_file_by_symbols(path)\n                for chunk in chunks:\n                    code = chunk[\"code\"]\n                    self.chunk_metadatas.append({\"file\": path, **chunk})\n                    chunk_codes.append(code)\n            else:\n                chunks = self.repo.chunk_file_by_lines(path, max_lines=50)\n                for code in chunks:\n                    self.chunk_metadatas.append({\"file\": path, \"code\": code})\n                    chunk_codes.append(code)\n\n        # Embed in batch (attempt). Fallback to per-item if embed_fn doesn't support list input.\n        if chunk_codes:\n            self.chunk_embeddings = self._batch_embed(chunk_codes)\n            self.backend.add(self.chunk_embeddings, self.chunk_metadatas)\n            self.backend.persist()\n\n    def _batch_embed(self, texts: List[str]) -> List[List[float]]:\n        \"\"\"Embed a list of texts, falling back to per-item calls if necessary.\"\"\"\n        try:\n            bulk = self.embed_fn(texts)  # type: ignore[arg-type]\n            if isinstance(bulk, list) and len(bulk) == len(texts) and all(isinstance(v, (list, tuple)) for v in bulk):\n                return [list(map(float, v)) for v in bulk]  # ensure list of list[float]\n        except Exception:\n            pass  # Fall back to per-item\n        # Fallback slow path\n        return [self.embed_fn(t) for t in texts]\n\n    def search(self, query: str, top_k: int = 5) -> List[Dict[str, Any]]:\n        if top_k <= 0:\n            return []\n        emb = self.embed_fn(query)\n        return self.backend.query(emb, top_k)",
        "file": "src/kit/vector_searcher.py"
      },
      {
        "name": "__init__",
        "type": "method",
        "start_line": 106,
        "end_line": 112,
        "code": "def __init__(self, repo, embed_fn, backend: Optional[VectorDBBackend] = None, persist_dir: Optional[str] = None):\n        self.repo = repo\n        self.embed_fn = embed_fn  # Function: str -> List[float]\n        self.persist_dir = persist_dir or os.path.join(\".kit\", \"vector_db\")\n        self.backend = backend or ChromaDBBackend(self.persist_dir)\n        self.chunk_metadatas: List[Dict[str, Any]] = []\n        self.chunk_embeddings: List[List[float]] = []",
        "file": "src/kit/vector_searcher.py"
      },
      {
        "name": "build_index",
        "type": "method",
        "start_line": 114,
        "end_line": 138,
        "code": "def build_index(self, chunk_by: str = \"symbols\"):\n        self.chunk_metadatas = []\n        chunk_codes: List[str] = []\n\n        for file in self.repo.get_file_tree():\n            if file[\"is_dir\"]:\n                continue\n            path = file[\"path\"]\n            if chunk_by == \"symbols\":\n                chunks = self.repo.chunk_file_by_symbols(path)\n                for chunk in chunks:\n                    code = chunk[\"code\"]\n                    self.chunk_metadatas.append({\"file\": path, **chunk})\n                    chunk_codes.append(code)\n            else:\n                chunks = self.repo.chunk_file_by_lines(path, max_lines=50)\n                for code in chunks:\n                    self.chunk_metadatas.append({\"file\": path, \"code\": code})\n                    chunk_codes.append(code)\n\n        # Embed in batch (attempt). Fallback to per-item if embed_fn doesn't support list input.\n        if chunk_codes:\n            self.chunk_embeddings = self._batch_embed(chunk_codes)\n            self.backend.add(self.chunk_embeddings, self.chunk_metadatas)\n            self.backend.persist()",
        "file": "src/kit/vector_searcher.py"
      },
      {
        "name": "_batch_embed",
        "type": "method",
        "start_line": 140,
        "end_line": 149,
        "code": "def _batch_embed(self, texts: List[str]) -> List[List[float]]:\n        \"\"\"Embed a list of texts, falling back to per-item calls if necessary.\"\"\"\n        try:\n            bulk = self.embed_fn(texts)  # type: ignore[arg-type]\n            if isinstance(bulk, list) and len(bulk) == len(texts) and all(isinstance(v, (list, tuple)) for v in bulk):\n                return [list(map(float, v)) for v in bulk]  # ensure list of list[float]\n        except Exception:\n            pass  # Fall back to per-item\n        # Fallback slow path\n        return [self.embed_fn(t) for t in texts]",
        "file": "src/kit/vector_searcher.py"
      },
      {
        "name": "search",
        "type": "method",
        "start_line": 151,
        "end_line": 155,
        "code": "def search(self, query: str, top_k: int = 5) -> List[Dict[str, Any]]:\n        if top_k <= 0:\n            return []\n        emb = self.embed_fn(query)\n        return self.backend.query(emb, top_k)",
        "file": "src/kit/vector_searcher.py"
      }
    ],
    "src/kit/summaries.py": [
      {
        "name": "LLMClientProtocol",
        "type": "class",
        "start_line": 10,
        "end_line": 13,
        "code": "class LLMClientProtocol(Protocol):\n    \"\"\"Protocol defining the interface for LLM clients.\"\"\"\n    # This is a structural protocol - any object with compatible methods will be accepted\n    pass",
        "file": "src/kit/summaries.py"
      },
      {
        "name": "LLMError",
        "type": "class",
        "start_line": 31,
        "end_line": 33,
        "code": "class LLMError(Exception):\n    \"\"\"Custom exception for LLM related errors.\"\"\"\n    pass",
        "file": "src/kit/summaries.py"
      },
      {
        "name": "SymbolNotFoundError",
        "type": "class",
        "start_line": 36,
        "end_line": 38,
        "code": "class SymbolNotFoundError(Exception):\n    \"\"\"Custom exception for when a symbol (function, class) is not found.\"\"\"\n    pass",
        "file": "src/kit/summaries.py"
      },
      {
        "name": "OpenAIConfig",
        "type": "class",
        "start_line": 42,
        "end_line": 55,
        "code": "class OpenAIConfig:\n    \"\"\"Configuration for OpenAI API access.\"\"\"\n    api_key: Optional[str] = field(default_factory=lambda: os.environ.get(\"OPENAI_API_KEY\"))\n    model: str = \"gpt-4o\"\n    temperature: float = 0.7\n    max_tokens: int = 1000  # Default max tokens for summary\n    base_url: Optional[str] = None\n\n    def __post_init__(self):\n        if not self.api_key:\n            raise ValueError(\n                \"OpenAI API key not found. \"\n                \"Set OPENAI_API_KEY environment variable or pass api_key directly.\"\n            )",
        "file": "src/kit/summaries.py"
      },
      {
        "name": "__post_init__",
        "type": "method",
        "start_line": 50,
        "end_line": 55,
        "code": "def __post_init__(self):\n        if not self.api_key:\n            raise ValueError(\n                \"OpenAI API key not found. \"\n                \"Set OPENAI_API_KEY environment variable or pass api_key directly.\"\n            )",
        "file": "src/kit/summaries.py"
      },
      {
        "name": "AnthropicConfig",
        "type": "class",
        "start_line": 59,
        "end_line": 71,
        "code": "class AnthropicConfig:\n    \"\"\"Configuration for Anthropic API access.\"\"\"\n    api_key: Optional[str] = field(default_factory=lambda: os.environ.get(\"ANTHROPIC_API_KEY\"))\n    model: str = \"claude-3-opus-20240229\"\n    temperature: float = 0.7\n    max_tokens: int = 1000 # Corresponds to Anthropic's max_tokens_to_sample\n\n    def __post_init__(self):\n        if not self.api_key:\n            raise ValueError(\n                \"Anthropic API key not found. \"\n                \"Set ANTHROPIC_API_KEY environment variable or pass api_key directly.\"\n            )",
        "file": "src/kit/summaries.py"
      },
      {
        "name": "__post_init__",
        "type": "method",
        "start_line": 66,
        "end_line": 71,
        "code": "def __post_init__(self):\n        if not self.api_key:\n            raise ValueError(\n                \"Anthropic API key not found. \"\n                \"Set ANTHROPIC_API_KEY environment variable or pass api_key directly.\"\n            )",
        "file": "src/kit/summaries.py"
      },
      {
        "name": "GoogleConfig",
        "type": "class",
        "start_line": 75,
        "end_line": 88,
        "code": "class GoogleConfig:\n    \"\"\"Configuration for Google Generative AI API access.\"\"\"\n    api_key: Optional[str] = field(default_factory=lambda: os.environ.get(\"GOOGLE_API_KEY\"))\n    model: str = \"gemini-1.5-pro-latest\"\n    temperature: Optional[float] = 0.7\n    max_output_tokens: Optional[int] = 1000 # Corresponds to Gemini's max_output_tokens\n    model_kwargs: Optional[Dict[str, Any]] = field(default_factory=dict)\n\n    def __post_init__(self):\n        if not self.api_key:\n            raise ValueError(\n                \"Google API key not found. \"\n                \"Set GOOGLE_API_KEY environment variable or pass api_key directly.\"\n            )",
        "file": "src/kit/summaries.py"
      },
      {
        "name": "__post_init__",
        "type": "method",
        "start_line": 83,
        "end_line": 88,
        "code": "def __post_init__(self):\n        if not self.api_key:\n            raise ValueError(\n                \"Google API key not found. \"\n                \"Set GOOGLE_API_KEY environment variable or pass api_key directly.\"\n            )",
        "file": "src/kit/summaries.py"
      },
      {
        "name": "Summarizer",
        "type": "class",
        "start_line": 96,
        "end_line": 745,
        "code": "class Summarizer:\n    \"\"\"Provides methods to summarize code using a configured LLM.\"\"\"\n\n    _tokenizer_cache: Dict[str, Any] = {} # Cache for tiktoken encoders\n    config: Optional[Union[OpenAIConfig, AnthropicConfig, GoogleConfig]]\n    repo: 'Repository'\n    _llm_client: Optional[Any]  # type: ignore\n\n    def _get_tokenizer(self, model_name: str):\n        if model_name in self._tokenizer_cache:\n            return self._tokenizer_cache[model_name]\n        try:\n            encoding = tiktoken.encoding_for_model(model_name)\n            self._tokenizer_cache[model_name] = encoding\n            return encoding\n        except KeyError:\n            try:\n                # Fallback for models not directly in tiktoken.model.MODEL_TO_ENCODING\n                encoding = tiktoken.get_encoding(\"cl100k_base\")\n                self._tokenizer_cache[model_name] = encoding\n                return encoding\n            except Exception as e:\n                logger.warning(f\"Could not load tiktoken encoder for {model_name} due to {e}, token count will be approximate (char count).\")\n                return None\n\n    def _count_tokens(self, text: str, model_name: Optional[str] = None) -> int:\n        \"\"\"Count the number of tokens in a text string for a given model.\"\"\"\n        if not text:\n            return 0\n        \n        # Use model from config if available, otherwise use a default\n        if model_name is None:\n            if self.config is not None and hasattr(self.config, 'model'):\n                model_name = self.config.model\n            else:\n                # Default to a common model if no config or model specified\n                model_name = \"gpt-4o\"  # Default fallback\n        \n        try:\n            # Try to use tiktoken for accurate token counting\n            if tiktoken:\n                try:\n                    if model_name in self._tokenizer_cache:\n                        encoder = self._tokenizer_cache[model_name]\n                    else:\n                        try:\n                            encoder = tiktoken.encoding_for_model(model_name)\n                        except KeyError:\n                            # Model not found, use cl100k_base as fallback\n                            encoder = tiktoken.get_encoding(\"cl100k_base\")\n                        self._tokenizer_cache[model_name] = encoder\n                    \n                    return len(encoder.encode(text))\n                except Exception as e:\n                    logger.warning(f\"Error using tiktoken for model {model_name}: {e}\")\n                    # Fall through to character-based approximation\n            else:\n                logger.warning(f\"No tiktoken encoder found for model {model_name}, token count will be approximate (char count).\")\n        except NameError:\n            # tiktoken not available\n            logger.warning(\"tiktoken not available, token count will be approximate (char count).\")\n        \n        # Fallback: approximate token count based on characters (4 chars ~= 1 token)\n        return len(text) // 4\n\n    def _count_openai_chat_tokens(self, messages: List[Dict[str, str]], model_name: str) -> Optional[int]:\n        \"\"\"Return the number of tokens used by a list of messages for OpenAI chat models.\"\"\"\n        encoding = self._get_tokenizer(model_name)\n        if not encoding:\n            logger.warning(f\"Cannot count OpenAI chat tokens for {model_name}, no tiktoken encoder available.\")\n            return None\n\n        # Logic adapted from OpenAI cookbook for counting tokens for chat completions\n        # See: https://github.com/openai/openai-cookbook/blob/main/examples/how_to_count_tokens_with_tiktoken.ipynb\n        if model_name in {\n            \"gpt-3.5-turbo-0613\",\n            \"gpt-3.5-turbo-16k-0613\",\n            \"gpt-4-0314\",\n            \"gpt-4-32k-0314\",\n            \"gpt-4-0613\",\n            \"gpt-4-32k-0613\",\n        }:\n            tokens_per_message = 3\n            tokens_per_name = 1\n        elif model_name == \"gpt-3.5-turbo-0301\":\n            tokens_per_message = 4  # every message follows <|start|>{role/name}\\n{content}<|end|>\\n\n            tokens_per_name = -1  # if there's a name, the role is omitted\n        elif \"gpt-3.5-turbo\" in model_name: # Covers general gpt-3.5-turbo and variants not explicitly listed\n            # Defaulting to newer model token counts as a general heuristic\n            logger.debug(f\"Using token counting parameters for gpt-3.5-turbo-0613 for model {model_name}.\")\n            tokens_per_message = 3\n            tokens_per_name = 1\n        elif \"gpt-4\" in model_name: # Covers general gpt-4 and variants not explicitly listed\n            logger.debug(f\"Using token counting parameters for gpt-4-0613 for model {model_name}.\")\n            tokens_per_message = 3\n            tokens_per_name = 1\n        else:\n            # Fallback for unknown models; this might not be perfectly accurate.\n            # Raise an error or use a default if this model is not supported by tiktoken's encoding_for_model\n            # For now, using a common default and logging a warning.\n            logger.warning(\n                f\"_count_openai_chat_tokens() may not be accurate for model {model_name}. \"\n                f\"It's not explicitly handled. Using default token counting parameters (3 tokens/message, 1 token/name). \"\n                f\"See OpenAI's documentation for details on your specific model.\"\n            )\n            tokens_per_message = 3\n            tokens_per_name = 1\n\n        num_tokens = 0\n        for message in messages:\n            num_tokens += tokens_per_message\n            for key, value in message.items():\n                if value is None: # Ensure value is not None before attempting to encode\n                    logger.debug(f\"Encountered None value for key '{key}' in message, skipping for token counting.\")\n                    continue\n                try:\n                    num_tokens += len(encoding.encode(str(value))) # Ensure value is string\n                except Exception as e:\n                    # This catch is a safeguard; tiktoken should handle most string inputs.\n                    logger.error(f\"Could not encode value for token counting: '{str(value)[:50]}...', error: {e}\")\n                    return None # Inability to encode part of message means count is unreliable\n                if key == \"name\":\n                    num_tokens += tokens_per_name\n        num_tokens += 3  # every reply is primed with <|start|>assistant<|message|> (approximates assistant's first tokens)\n        return num_tokens\n\n    def __init__(self, repo: 'Repository', \n                 config: Optional[Union[OpenAIConfig, AnthropicConfig, GoogleConfig]] = None, \n                 llm_client: Optional[Any] = None):\n        \"\"\"\n        Initializes the Summarizer.\n\n        Args:\n            repo: The kit.Repository instance containing the code.\n            config: LLM configuration (OpenAIConfig, AnthropicConfig, or GoogleConfig).\n                    If None, defaults to OpenAIConfig.\n            llm_client: Optional pre-initialized LLM client. If None, client will be\n                        lazy-loaded on first use based on the config.\n        \"\"\"\n        self.repo = repo\n        self._llm_client = llm_client  # Store provided llm_client directly\n        self.config = config          # Store provided config\n\n        if self._llm_client is None:\n            # Only create/setup LLM if a client wasn't directly provided\n            if self.config is None:\n                # If no config is provided either, default to OpenAIConfig\n                # This will raise ValueError if OPENAI_API_KEY is not set.\n                self.config = OpenAIConfig()\n            \n            if isinstance(self.config, OpenAIConfig):\n                try:\n                    import openai\n                    if self.config.base_url:\n                        self._llm_client = openai.OpenAI(api_key=self.config.api_key, base_url=self.config.base_url)\n                    else:\n                        self._llm_client = openai.OpenAI(api_key=self.config.api_key)\n                except ImportError:\n                    raise LLMError(\"OpenAI SDK (openai) not available. Please install it.\")\n            elif isinstance(self.config, AnthropicConfig):\n                try:\n                    import anthropic\n                    self._llm_client = anthropic.Anthropic(api_key=self.config.api_key)\n                except ImportError:\n                    raise LLMError(\"Anthropic SDK (anthropic) not available. Please install it.\")\n            elif isinstance(self.config, GoogleConfig):\n                try:\n                    import google.genai as genai\n                    self._llm_client = genai.Client(api_key=self.config.api_key) # Use the new client\n                except ImportError:\n                    raise LLMError(\"Google Gen AI SDK (google-genai) not available. Please install it.\")\n            else:\n                # This case implies self.config was set to something unexpected if self._llm_client was None\n                # and self.config was also None initially. Or self.config was passed with an invalid type.\n                if self.config is not None: # Only raise if config is of an unsupported type\n                    raise TypeError(f\"Unsupported LLM configuration type: {type(self.config)}\")\n                # If self.config is None here, it means OpenAIConfig() failed, but that should raise its own error.\n                # Or, it implies a logic flaw if this path is reached with self.config being None.\n        # If _llm_client was provided, we assume it's configured and ready.\n        # self.config might be None if only llm_client was passed.\n\n    def _get_llm_client(self) -> Any:\n        \"\"\"Lazy loads the appropriate LLM client based on self.config.\"\"\"\n        if self._llm_client is not None:\n            return self._llm_client\n\n        try:\n            if isinstance(self.config, OpenAIConfig):\n                from openai import OpenAI # Local import for OpenAI client\n                if self.config.base_url:\n                    client = OpenAI(api_key=self.config.api_key, base_url=self.config.base_url)\n                else:\n                    client = OpenAI(api_key=self.config.api_key)\n            elif isinstance(self.config, AnthropicConfig):\n                from anthropic import Anthropic # Local import for Anthropic client\n                client = Anthropic(api_key=self.config.api_key)  # type: ignore # Different client type\n            elif isinstance(self.config, GoogleConfig):\n                if genai is None or genai_types is None:\n                    raise LLMError(\"Google Gen AI SDK (google-genai) is not installed. Please install it to use Google models.\")\n                # API key is picked up from GOOGLE_API_KEY env var by default if not passed to Client()\n                # However, we have it in self.config.api_key, so we pass it explicitly.\n                client = genai.Client(api_key=self.config.api_key)  # type: ignore # Different client type\n            else:\n                # This case should ideally be prevented by the __init__ type check,\n                # but as a safeguard:\n                raise LLMError(f\"Unsupported LLM configuration type: {type(self.config)}\")\n            \n            self._llm_client = client\n            return self._llm_client\n        except ImportError as e:\n            sdk_name = \"\"\n            if \"openai\" in str(e).lower(): sdk_name = \"openai\"\n            elif \"anthropic\" in str(e).lower(): sdk_name = \"anthropic\"\n            # google-genai import is handled by genai being None\n            if sdk_name:\n                raise LLMError(f\"The {sdk_name} SDK is not installed. Please install it to use {sdk_name.capitalize()} models.\") from e\n            raise # Re-raise if it's a different import error\n        except Exception as e:\n            logger.error(f\"Error initializing LLM client: {e}\")\n            raise LLMError(f\"Error initializing LLM client: {e}\") from e\n\n    def summarize_file(self, file_path: str) -> str:\n        \"\"\"\n        Summarizes the content of a single file.\n\n        Args:\n            file_path: The path to the file to summarize.\n\n        Returns:\n            A string containing the summary of the file.\n\n        Raises:\n            FileNotFoundError: If the file_path does not exist.\n            LLMError: If there's an error from the LLM API or an empty summary.\n        \"\"\"\n        logger.debug(f\"Attempting to summarize file: {file_path}\")\n        abs_file_path = self.repo.get_abs_path(file_path) # Use get_abs_path\n    \n        try:\n            file_content = self.repo.get_file_content(abs_file_path)\n        except FileNotFoundError:\n            # Re-raise to ensure the Summarizer's contract is met\n            raise FileNotFoundError(f\"File not found via repo: {abs_file_path}\")\n\n        if not file_content.strip():\n            logger.warning(f\"File {abs_file_path} is empty or contains only whitespace. Skipping summary.\")\n            return \"\"\n\n        if len(file_content) > MAX_FILE_SUMMARIZE_CHARS:\n            logger.warning(f\"File content for {file_path} ({len(file_content)} chars) is too large for summarization (limit: {MAX_FILE_SUMMARIZE_CHARS}).\")\n            return f\"File content too large ({len(file_content)} characters) to summarize with current limits.\"\n\n        # Max model context is 128000 tokens. Avg ~4 chars/token -> ~512,000 chars for total message.\n        # Let's set a threshold for the raw content itself.\n        MAX_CHARS_FOR_SUMMARY = 400_000  # Approx 100k tokens\n        if len(file_content) > MAX_CHARS_FOR_SUMMARY:\n            logger.warning(\n                f\"File {abs_file_path} content is too large ({len(file_content)} chars) \"\n                f\"to summarize reliably. Skipping.\"\n            )\n            # Return a placeholder summary or an empty string\n            return f\"File content too large ({len(file_content)} characters) to summarize.\"\n\n        system_prompt_text = \"You are an expert assistant skilled in creating concise and informative code summaries.\"\n        user_prompt_text = f\"Summarize the following code from the file '{file_path}'. Provide a high-level overview of its purpose, key components, and functionality. Focus on what the code does, not just how it's written. The code is:\\n\\n```\\n{file_content}\\n```\"\n\n        client = self._get_llm_client()\n        summary = \"\"\n\n        logger.debug(f\"System Prompt for {file_path}: {system_prompt_text}\")\n        logger.debug(f\"User Prompt for {file_path} (first 200 chars): {user_prompt_text[:200]}...\")\n        # Get model name from config if available, otherwise pass None for default\n        model_name = self.config.model if self.config is not None and hasattr(self.config, 'model') else None\n        token_count = self._count_tokens(user_prompt_text, model_name)\n        if token_count is not None:\n            logger.debug(f\"Estimated tokens for user prompt ({file_path}): {token_count}\")\n        else:\n            logger.debug(f\"Approximate characters for user prompt ({file_path}): {len(user_prompt_text)}\")\n\n        try:\n            # If a custom llm_client was provided without a config, use it directly\n            if self.config is None:\n                # For custom llm_client without config, assume it knows how to handle the prompt\n                # This is used in tests with FakeOpenAI\n                try:\n                    # Try OpenAI-style interface first\n                    response = client.chat.completions.create(\n                        messages=[\n                            {\"role\": \"system\", \"content\": system_prompt_text},\n                            {\"role\": \"user\", \"content\": user_prompt_text}\n                        ]\n                    )\n                    summary = response.choices[0].message.content\n                except (AttributeError, TypeError) as e:\n                    # If that fails, the client might have a different interface\n                    logger.warning(f\"Custom LLM client doesn't support OpenAI-style interface: {e}\")\n                    raise LLMError(f\"Custom LLM client without config doesn't support expected interface: {e}\")\n            elif isinstance(self.config, OpenAIConfig):\n                messages_for_api = [\n                    {\"role\": \"system\", \"content\": system_prompt_text},\n                    {\"role\": \"user\", \"content\": user_prompt_text}\n                ]\n                prompt_token_count = self._count_openai_chat_tokens(messages_for_api, self.config.model)\n                if prompt_token_count is not None and prompt_token_count > OPENAI_MAX_PROMPT_TOKENS:\n                    summary = f\"Summary generation failed: OpenAI prompt too large ({prompt_token_count} tokens). Limit is {OPENAI_MAX_PROMPT_TOKENS} tokens.\"\n                else:\n                    response = client.chat.completions.create(\n                        model=self.config.model,\n                        messages=messages_for_api,\n                        temperature=self.config.temperature,\n                        max_tokens=self.config.max_tokens,\n                    )\n                    summary = response.choices[0].message.content\n                    if response.usage:\n                        logger.debug(f\"OpenAI API usage for {file_path}: {response.usage}\")\n            elif isinstance(self.config, AnthropicConfig):\n                response = client.messages.create(\n                    model=self.config.model,\n                    system=system_prompt_text,\n                    messages=[\n                        {\"role\": \"user\", \"content\": user_prompt_text}\n                    ],\n                    max_tokens=self.config.max_tokens,\n                    temperature=self.config.temperature,\n                )\n                summary = response.content[0].text\n            elif isinstance(self.config, GoogleConfig):\n                if not genai_types:\n                    raise LLMError(\"Google Gen AI SDK (google-genai) types not available. SDK might not be installed correctly.\")\n                \n                generation_config_params: Dict[str, Any] = self.config.model_kwargs.copy() if self.config.model_kwargs is not None else {}\n\n                if self.config.temperature is not None:\n                    generation_config_params[\"temperature\"] = self.config.temperature\n                if self.config.max_output_tokens is not None:\n                    generation_config_params[\"max_output_tokens\"] = self.config.max_output_tokens\n                \n                final_sdk_params = generation_config_params if generation_config_params else None\n\n                response = client.models.generate_content(\n                    model=self.config.model,\n                    contents=user_prompt_text,\n                    generation_config=final_sdk_params\n                )\n                # Check for blocked prompt first\n                if hasattr(response, 'prompt_feedback') and response.prompt_feedback and response.prompt_feedback.block_reason:\n                    logger.warning(f\"Google LLM prompt for file {file_path} blocked. Reason: {response.prompt_feedback.block_reason}\")\n                    summary = f\"Summary generation failed: Prompt blocked by API (Reason: {response.prompt_feedback.block_reason})\"\n                elif not response.text:\n                    logger.warning(f\"Google LLM returned no text for file {file_path}. Response: {response}\")\n                    summary = \"Summary generation failed: No text returned by API.\"\n                else:\n                    summary = response.text\n            \n            if not summary or not summary.strip():\n                logger.warning(f\"LLM returned an empty or whitespace-only summary for file {file_path}.\")\n                raise LLMError(f\"LLM returned an empty summary for file {file_path}.\")\n            \n            logger.debug(f\"LLM summary for file {file_path} (first 200 chars): {summary[:200]}...\")\n            return summary.strip()\n\n        except Exception as e:\n            logger.error(f\"Error communicating with LLM API for file {file_path}: {e}\")\n            raise LLMError(f\"Error communicating with LLM API: {e}\") from e\n\n    def summarize_function(self, file_path: str, function_name: str) -> str:\n        \"\"\"\n        Summarizes a specific function within a file.\n\n        Args:\n            file_path: The path to the file containing the function.\n            function_name: The name of the function to summarize.\n\n        Returns:\n            A string containing the summary of the function.\n\n        Raises:\n            FileNotFoundError: If the file_path does not exist.\n            ValueError: If the function cannot be found in the file.\n            LLMError: If there's an error from the LLM API or an empty summary.\n        \"\"\"\n        logger.debug(f\"Attempting to summarize function: {function_name} in file: {file_path}\")\n        \n        symbols = self.repo.extract_symbols(file_path)\n        function_code = None\n        for symbol in symbols:\n            # Use node_path if available (more precise), fallback to name\n            current_symbol_name = symbol.get(\"node_path\", symbol.get(\"name\"))\n            if current_symbol_name == function_name and symbol.get(\"type\", \"\").upper() in [\"FUNCTION\", \"METHOD\"]:\n                function_code = symbol.get(\"code\")\n                break\n        \n        if not function_code:\n            raise ValueError(f\"Could not find function '{function_name}' in '{file_path}'.\")\n\n        # Max model context is 128000 tokens. Avg ~4 chars/token -> ~512,000 chars for total message.\n        # Let's set a threshold for the raw content itself.\n        MAX_CHARS_FOR_SUMMARY = 400_000  # Approx 100k tokens\n        if len(function_code) > MAX_CHARS_FOR_SUMMARY:\n            logger.warning(\n                f\"Function {function_name} in file {file_path} content is too large ({len(function_code)} chars) \"\n                f\"to summarize reliably. Skipping.\"\n            )\n            return f\"Function content too large ({len(function_code)} characters) to summarize.\"\n\n        system_prompt_text = \"You are an expert assistant skilled in creating concise code summaries for functions.\"\n        user_prompt_text = f\"Summarize the following function named '{function_name}' from the file '{file_path}'. Describe its purpose, parameters, and return value. The function definition is:\\n\\n```\\n{function_code}\\n```\"\n\n        client = self._get_llm_client()\n        summary = \"\"\n\n        logger.debug(f\"System Prompt for {function_name} in {file_path}: {system_prompt_text}\")\n        logger.debug(f\"User Prompt for {function_name} in {file_path} (first 200 chars): {user_prompt_text[:200]}...\")\n        # Get model name from config if available, otherwise pass None for default\n        model_name = self.config.model if self.config is not None and hasattr(self.config, 'model') else None\n        token_count = self._count_tokens(user_prompt_text, model_name)\n        logger.debug(f\"Token count for {function_name} in {file_path}: {token_count}\")\n\n        try:\n            # If a custom llm_client was provided without a config, use it directly\n            if self.config is None:\n                # For custom llm_client without config, assume it knows how to handle the prompt\n                # This is used in tests with FakeOpenAI\n                try:\n                    # Try OpenAI-style interface first\n                    response = client.chat.completions.create(\n                        messages=[\n                            {\"role\": \"system\", \"content\": system_prompt_text},\n                            {\"role\": \"user\", \"content\": user_prompt_text}\n                        ]\n                    )\n                    summary = response.choices[0].message.content\n                except (AttributeError, TypeError) as e:\n                    # If that fails, the client might have a different interface\n                    # In a real implementation, you'd need more robust handling here\n                    logger.warning(f\"Custom LLM client doesn't support OpenAI-style interface: {e}\")\n                    raise LLMError(f\"Custom LLM client without config doesn't support expected interface: {e}\")\n            elif isinstance(self.config, OpenAIConfig):\n                messages_for_api = [\n                    {\"role\": \"system\", \"content\": system_prompt_text},\n                    {\"role\": \"user\", \"content\": user_prompt_text}\n                ]\n                prompt_token_count = self._count_openai_chat_tokens(messages_for_api, self.config.model)\n                if prompt_token_count is not None and prompt_token_count > OPENAI_MAX_PROMPT_TOKENS:\n                    summary = f\"Summary generation failed: OpenAI prompt too large ({prompt_token_count} tokens). Limit is {OPENAI_MAX_PROMPT_TOKENS} tokens.\"\n                else:\n                    response = client.chat.completions.create(\n                        model=self.config.model,\n                        messages=messages_for_api,\n                        temperature=self.config.temperature,\n                        max_tokens=self.config.max_tokens,\n                    )\n                    summary = response.choices[0].message.content\n                    if response.usage:\n                        logger.debug(f\"OpenAI API usage for {function_name} in {file_path}: {response.usage}\")\n            elif isinstance(self.config, AnthropicConfig):\n                response = client.messages.create(\n                    model=self.config.model,\n                    system=system_prompt_text,\n                    messages=[\n                        {\"role\": \"user\", \"content\": user_prompt_text}\n                    ],\n                    max_tokens=self.config.max_tokens,\n                    temperature=self.config.temperature,\n                )\n                summary = response.content[0].text\n                # Anthropic usage might be in response.usage (confirm API docs)\n                # Example: logger.debug(f\"Anthropic API usage for {function_name} in {file_path}: {response.usage}\")\n            elif isinstance(self.config, GoogleConfig):\n                if not genai_types:\n                    raise LLMError(\"Google Gen AI SDK (google-genai) types not available. SDK might not be installed correctly.\")\n\n                generation_config_params: Dict[str, Any] = self.config.model_kwargs.copy() if self.config.model_kwargs is not None else {}\n\n                if self.config.temperature is not None:\n                    generation_config_params[\"temperature\"] = self.config.temperature\n                if self.config.max_output_tokens is not None:\n                    generation_config_params[\"max_output_tokens\"] = self.config.max_output_tokens\n                \n                final_sdk_params = generation_config_params if generation_config_params else None\n\n                response = client.models.generate_content(\n                    model=self.config.model,\n                    contents=user_prompt_text,\n                    generation_config=final_sdk_params\n                )\n                if hasattr(response, 'prompt_feedback') and response.prompt_feedback and response.prompt_feedback.block_reason:\n                    logger.warning(f\"Google LLM prompt for {function_name} in {file_path} blocked. Reason: {response.prompt_feedback.block_reason}\")\n                    summary = f\"Summary generation failed: Prompt blocked by API (Reason: {response.prompt_feedback.block_reason})\"\n                elif not response.text:\n                    logger.warning(f\"Google LLM returned no text for {function_name} in {file_path}. Response: {response}\")\n                    summary = \"Summary generation failed: No text returned by API.\"\n                else:\n                    summary = response.text\n            else:\n                # This should never happen with our current logic, but as a safeguard\n                raise LLMError(f\"Unsupported LLM configuration type: {type(self.config) if self.config else None}\")\n            \n            if not summary or not summary.strip():\n                logger.warning(f\"LLM returned an empty or whitespace-only summary for function {function_name} in {file_path}.\")\n                raise LLMError(f\"LLM returned an empty summary for function {function_name}.\")\n            \n            logger.debug(f\"LLM summary for {function_name} in {file_path} (first 200 chars): {summary[:200]}...\")\n            return summary.strip()\n\n        except Exception as e:\n            logger.error(f\"Error communicating with LLM API for function {function_name} in {file_path}: {e}\")\n            raise LLMError(f\"Error communicating with LLM API for function {function_name}: {e}\") from e\n\n    def summarize_class(self, file_path: str, class_name: str) -> str:\n        \"\"\"\n        Summarizes a specific class within a file.\n\n        Args:\n            file_path: The path to the file containing the class.\n            class_name: The name of the class to summarize.\n\n        Returns:\n            A string containing the summary of the class.\n\n        Raises:\n            FileNotFoundError: If the file_path does not exist.\n            ValueError: If the class cannot be found in the file.\n            LLMError: If there's an error from the LLM API or an empty summary.\n        \"\"\"\n        logger.debug(f\"Attempting to summarize class: {class_name} in file: {file_path}\")\n\n        symbols = self.repo.extract_symbols(file_path)\n        class_code = None\n        for symbol in symbols:\n            # Use node_path if available (more precise), fallback to name\n            current_symbol_name = symbol.get(\"node_path\", symbol.get(\"name\"))\n            if current_symbol_name == class_name and symbol.get(\"type\", \"\").upper() == \"CLASS\":\n                class_code = symbol.get(\"code\")\n                break\n\n        if not class_code:\n            raise ValueError(f\"Could not find class '{class_name}' in '{file_path}'.\")\n\n        # Max model context is 128000 tokens. Avg ~4 chars/token -> ~512,000 chars for total message.\n        # Let's set a threshold for the raw content itself.\n        MAX_CHARS_FOR_SUMMARY = 400_000  # Approx 100k tokens\n        if len(class_code) > MAX_CHARS_FOR_SUMMARY:\n            logger.warning(\n                f\"Class {class_name} in file {file_path} content is too large ({len(class_code)} chars) \"\n                f\"to summarize reliably. Skipping.\"\n            )\n            return f\"Class content too large ({len(class_code)} characters) to summarize.\"\n        \n        system_prompt_text = \"You are an expert assistant skilled in creating concise code summaries for classes.\"\n        user_prompt_text = f\"Summarize the following class named '{class_name}' from the file '{file_path}'. Describe its purpose, key attributes, and main methods. The class definition is:\\n\\n```\\n{class_code}\\n```\"\n        \n        client = self._get_llm_client()\n        summary = \"\"\n\n        logger.debug(f\"System Prompt for {class_name} in {file_path}: {system_prompt_text}\")\n        logger.debug(f\"User Prompt for {class_name} (first 200 chars): {user_prompt_text[:200]}...\")\n        # Get model name from config if available, otherwise pass None for default\n        model_name = self.config.model if self.config is not None and hasattr(self.config, 'model') else None\n        token_count = self._count_tokens(user_prompt_text, model_name)\n        logger.debug(f\"Token count for {class_name} in {file_path}: {token_count}\")\n\n        try:\n            # If a custom llm_client was provided without a config, use it directly\n            if self.config is None:\n                # For custom llm_client without config, assume it knows how to handle the prompt\n                # This is used in tests with FakeOpenAI\n                try:\n                    # Try OpenAI-style interface first\n                    response = client.chat.completions.create(\n                        messages=[\n                            {\"role\": \"system\", \"content\": system_prompt_text},\n                            {\"role\": \"user\", \"content\": user_prompt_text}\n                        ]\n                    )\n                    summary = response.choices[0].message.content\n                except (AttributeError, TypeError) as e:\n                    logger.warning(f\"Custom LLM client doesn't support OpenAI-style interface: {e}\")\n                    raise LLMError(f\"Custom LLM client without config doesn't support expected interface: {e}\")\n            elif isinstance(self.config, OpenAIConfig):\n                messages_for_api = [\n                    {\"role\": \"system\", \"content\": system_prompt_text},\n                    {\"role\": \"user\", \"content\": user_prompt_text}\n                ]\n                prompt_token_count = self._count_openai_chat_tokens(messages_for_api, self.config.model)\n                if prompt_token_count is not None and prompt_token_count > OPENAI_MAX_PROMPT_TOKENS:\n                    summary = f\"Summary generation failed: OpenAI prompt too large ({prompt_token_count} tokens). Limit is {OPENAI_MAX_PROMPT_TOKENS} tokens.\"\n                else:\n                    response = client.chat.completions.create(\n                        model=self.config.model,\n                        messages=messages_for_api,\n                        temperature=self.config.temperature,\n                        max_tokens=self.config.max_tokens,\n                    )\n                    summary = response.choices[0].message.content\n                    if response.usage:\n                        logger.debug(f\"OpenAI API usage for {class_name} in {file_path}: {response.usage}\")\n            elif isinstance(self.config, AnthropicConfig):\n                response = client.messages.create(\n                    model=self.config.model,\n                    system=system_prompt_text,\n                    messages=[\n                        {\"role\": \"user\", \"content\": user_prompt_text}\n                    ],\n                    max_tokens=self.config.max_tokens,\n                    temperature=self.config.temperature,\n                )\n                summary = response.content[0].text\n                # Anthropic usage might be in response.usage (confirm API docs)\n                # Example: logger.debug(f\"Anthropic API usage for {class_name} in {file_path}: {response.usage}\")\n            elif isinstance(self.config, GoogleConfig):\n                if not genai_types:\n                    raise LLMError(\"Google Gen AI SDK (google-genai) types not available. SDK might not be installed correctly.\")\n                \n                generation_config_params: Dict[str, Any] = self.config.model_kwargs.copy() if self.config.model_kwargs is not None else {}\n\n                if self.config.temperature is not None:\n                    generation_config_params[\"temperature\"] = self.config.temperature\n                if self.config.max_output_tokens is not None:\n                    generation_config_params[\"max_output_tokens\"] = self.config.max_output_tokens\n                \n                final_sdk_params = generation_config_params if generation_config_params else None\n\n                response = client.models.generate_content(\n                    model=self.config.model,\n                    contents=user_prompt_text,\n                    generation_config=final_sdk_params\n                )\n                if hasattr(response, 'prompt_feedback') and response.prompt_feedback and response.prompt_feedback.block_reason:\n                    logger.warning(f\"Google LLM prompt for {class_name} in {file_path} blocked. Reason: {response.prompt_feedback.block_reason}\")\n                    summary = f\"Summary generation failed: Prompt blocked by API (Reason: {response.prompt_feedback.block_reason})\"\n                elif not response.text:\n                    logger.warning(f\"Google LLM returned no text for {class_name} in {file_path}. Response: {response}\")\n                    summary = \"Summary generation failed: No text returned by API.\"\n                else:\n                    summary = response.text\n            else:\n                # This should never happen with our current logic, but as a safeguard\n                raise LLMError(f\"Unsupported LLM configuration type: {type(self.config) if self.config else None}\")\n            \n            if not summary or not summary.strip():\n                logger.warning(f\"LLM returned an empty or whitespace-only summary for class {class_name} in {file_path}.\")\n                raise LLMError(f\"LLM returned an empty summary for class {class_name}.\")\n            \n            logger.debug(f\"LLM summary for {class_name} in {file_path} (first 200 chars): {summary[:200]}...\")\n            return summary.strip()\n\n        except Exception as e:\n            logger.error(f\"Error communicating with LLM API for class {class_name} in {file_path}: {e}\")\n            raise LLMError(f\"Error communicating with LLM API for class {class_name}: {e}\") from e",
        "file": "src/kit/summaries.py"
      },
      {
        "name": "_get_tokenizer",
        "type": "method",
        "start_line": 104,
        "end_line": 119,
        "code": "def _get_tokenizer(self, model_name: str):\n        if model_name in self._tokenizer_cache:\n            return self._tokenizer_cache[model_name]\n        try:\n            encoding = tiktoken.encoding_for_model(model_name)\n            self._tokenizer_cache[model_name] = encoding\n            return encoding\n        except KeyError:\n            try:\n                # Fallback for models not directly in tiktoken.model.MODEL_TO_ENCODING\n                encoding = tiktoken.get_encoding(\"cl100k_base\")\n                self._tokenizer_cache[model_name] = encoding\n                return encoding\n            except Exception as e:\n                logger.warning(f\"Could not load tiktoken encoder for {model_name} due to {e}, token count will be approximate (char count).\")\n                return None",
        "file": "src/kit/summaries.py"
      },
      {
        "name": "_count_tokens",
        "type": "method",
        "start_line": 121,
        "end_line": 159,
        "code": "def _count_tokens(self, text: str, model_name: Optional[str] = None) -> int:\n        \"\"\"Count the number of tokens in a text string for a given model.\"\"\"\n        if not text:\n            return 0\n        \n        # Use model from config if available, otherwise use a default\n        if model_name is None:\n            if self.config is not None and hasattr(self.config, 'model'):\n                model_name = self.config.model\n            else:\n                # Default to a common model if no config or model specified\n                model_name = \"gpt-4o\"  # Default fallback\n        \n        try:\n            # Try to use tiktoken for accurate token counting\n            if tiktoken:\n                try:\n                    if model_name in self._tokenizer_cache:\n                        encoder = self._tokenizer_cache[model_name]\n                    else:\n                        try:\n                            encoder = tiktoken.encoding_for_model(model_name)\n                        except KeyError:\n                            # Model not found, use cl100k_base as fallback\n                            encoder = tiktoken.get_encoding(\"cl100k_base\")\n                        self._tokenizer_cache[model_name] = encoder\n                    \n                    return len(encoder.encode(text))\n                except Exception as e:\n                    logger.warning(f\"Error using tiktoken for model {model_name}: {e}\")\n                    # Fall through to character-based approximation\n            else:\n                logger.warning(f\"No tiktoken encoder found for model {model_name}, token count will be approximate (char count).\")\n        except NameError:\n            # tiktoken not available\n            logger.warning(\"tiktoken not available, token count will be approximate (char count).\")\n        \n        # Fallback: approximate token count based on characters (4 chars ~= 1 token)\n        return len(text) // 4",
        "file": "src/kit/summaries.py"
      },
      {
        "name": "_count_openai_chat_tokens",
        "type": "method",
        "start_line": 161,
        "end_line": 220,
        "code": "def _count_openai_chat_tokens(self, messages: List[Dict[str, str]], model_name: str) -> Optional[int]:\n        \"\"\"Return the number of tokens used by a list of messages for OpenAI chat models.\"\"\"\n        encoding = self._get_tokenizer(model_name)\n        if not encoding:\n            logger.warning(f\"Cannot count OpenAI chat tokens for {model_name}, no tiktoken encoder available.\")\n            return None\n\n        # Logic adapted from OpenAI cookbook for counting tokens for chat completions\n        # See: https://github.com/openai/openai-cookbook/blob/main/examples/how_to_count_tokens_with_tiktoken.ipynb\n        if model_name in {\n            \"gpt-3.5-turbo-0613\",\n            \"gpt-3.5-turbo-16k-0613\",\n            \"gpt-4-0314\",\n            \"gpt-4-32k-0314\",\n            \"gpt-4-0613\",\n            \"gpt-4-32k-0613\",\n        }:\n            tokens_per_message = 3\n            tokens_per_name = 1\n        elif model_name == \"gpt-3.5-turbo-0301\":\n            tokens_per_message = 4  # every message follows <|start|>{role/name}\\n{content}<|end|>\\n\n            tokens_per_name = -1  # if there's a name, the role is omitted\n        elif \"gpt-3.5-turbo\" in model_name: # Covers general gpt-3.5-turbo and variants not explicitly listed\n            # Defaulting to newer model token counts as a general heuristic\n            logger.debug(f\"Using token counting parameters for gpt-3.5-turbo-0613 for model {model_name}.\")\n            tokens_per_message = 3\n            tokens_per_name = 1\n        elif \"gpt-4\" in model_name: # Covers general gpt-4 and variants not explicitly listed\n            logger.debug(f\"Using token counting parameters for gpt-4-0613 for model {model_name}.\")\n            tokens_per_message = 3\n            tokens_per_name = 1\n        else:\n            # Fallback for unknown models; this might not be perfectly accurate.\n            # Raise an error or use a default if this model is not supported by tiktoken's encoding_for_model\n            # For now, using a common default and logging a warning.\n            logger.warning(\n                f\"_count_openai_chat_tokens() may not be accurate for model {model_name}. \"\n                f\"It's not explicitly handled. Using default token counting parameters (3 tokens/message, 1 token/name). \"\n                f\"See OpenAI's documentation for details on your specific model.\"\n            )\n            tokens_per_message = 3\n            tokens_per_name = 1\n\n        num_tokens = 0\n        for message in messages:\n            num_tokens += tokens_per_message\n            for key, value in message.items():\n                if value is None: # Ensure value is not None before attempting to encode\n                    logger.debug(f\"Encountered None value for key '{key}' in message, skipping for token counting.\")\n                    continue\n                try:\n                    num_tokens += len(encoding.encode(str(value))) # Ensure value is string\n                except Exception as e:\n                    # This catch is a safeguard; tiktoken should handle most string inputs.\n                    logger.error(f\"Could not encode value for token counting: '{str(value)[:50]}...', error: {e}\")\n                    return None # Inability to encode part of message means count is unreliable\n                if key == \"name\":\n                    num_tokens += tokens_per_name\n        num_tokens += 3  # every reply is primed with <|start|>assistant<|message|> (approximates assistant's first tokens)\n        return num_tokens",
        "file": "src/kit/summaries.py"
      },
      {
        "name": "__init__",
        "type": "method",
        "start_line": 222,
        "end_line": 275,
        "code": "def __init__(self, repo: 'Repository', \n                 config: Optional[Union[OpenAIConfig, AnthropicConfig, GoogleConfig]] = None, \n                 llm_client: Optional[Any] = None):\n        \"\"\"\n        Initializes the Summarizer.\n\n        Args:\n            repo: The kit.Repository instance containing the code.\n            config: LLM configuration (OpenAIConfig, AnthropicConfig, or GoogleConfig).\n                    If None, defaults to OpenAIConfig.\n            llm_client: Optional pre-initialized LLM client. If None, client will be\n                        lazy-loaded on first use based on the config.\n        \"\"\"\n        self.repo = repo\n        self._llm_client = llm_client  # Store provided llm_client directly\n        self.config = config          # Store provided config\n\n        if self._llm_client is None:\n            # Only create/setup LLM if a client wasn't directly provided\n            if self.config is None:\n                # If no config is provided either, default to OpenAIConfig\n                # This will raise ValueError if OPENAI_API_KEY is not set.\n                self.config = OpenAIConfig()\n            \n            if isinstance(self.config, OpenAIConfig):\n                try:\n                    import openai\n                    if self.config.base_url:\n                        self._llm_client = openai.OpenAI(api_key=self.config.api_key, base_url=self.config.base_url)\n                    else:\n                        self._llm_client = openai.OpenAI(api_key=self.config.api_key)\n                except ImportError:\n                    raise LLMError(\"OpenAI SDK (openai) not available. Please install it.\")\n            elif isinstance(self.config, AnthropicConfig):\n                try:\n                    import anthropic\n                    self._llm_client = anthropic.Anthropic(api_key=self.config.api_key)\n                except ImportError:\n                    raise LLMError(\"Anthropic SDK (anthropic) not available. Please install it.\")\n            elif isinstance(self.config, GoogleConfig):\n                try:\n                    import google.genai as genai\n                    self._llm_client = genai.Client(api_key=self.config.api_key) # Use the new client\n                except ImportError:\n                    raise LLMError(\"Google Gen AI SDK (google-genai) not available. Please install it.\")\n            else:\n                # This case implies self.config was set to something unexpected if self._llm_client was None\n                # and self.config was also None initially. Or self.config was passed with an invalid type.\n                if self.config is not None: # Only raise if config is of an unsupported type\n                    raise TypeError(f\"Unsupported LLM configuration type: {type(self.config)}\")\n                # If self.config is None here, it means OpenAIConfig() failed, but that should raise its own error.\n                # Or, it implies a logic flaw if this path is reached with self.config being None.\n        # If _llm_client was provided, we assume it's configured and ready.\n        # self.config might be None if only llm_client was passed.",
        "file": "src/kit/summaries.py"
      },
      {
        "name": "_get_llm_client",
        "type": "method",
        "start_line": 277,
        "end_line": 315,
        "code": "def _get_llm_client(self) -> Any:\n        \"\"\"Lazy loads the appropriate LLM client based on self.config.\"\"\"\n        if self._llm_client is not None:\n            return self._llm_client\n\n        try:\n            if isinstance(self.config, OpenAIConfig):\n                from openai import OpenAI # Local import for OpenAI client\n                if self.config.base_url:\n                    client = OpenAI(api_key=self.config.api_key, base_url=self.config.base_url)\n                else:\n                    client = OpenAI(api_key=self.config.api_key)\n            elif isinstance(self.config, AnthropicConfig):\n                from anthropic import Anthropic # Local import for Anthropic client\n                client = Anthropic(api_key=self.config.api_key)  # type: ignore # Different client type\n            elif isinstance(self.config, GoogleConfig):\n                if genai is None or genai_types is None:\n                    raise LLMError(\"Google Gen AI SDK (google-genai) is not installed. Please install it to use Google models.\")\n                # API key is picked up from GOOGLE_API_KEY env var by default if not passed to Client()\n                # However, we have it in self.config.api_key, so we pass it explicitly.\n                client = genai.Client(api_key=self.config.api_key)  # type: ignore # Different client type\n            else:\n                # This case should ideally be prevented by the __init__ type check,\n                # but as a safeguard:\n                raise LLMError(f\"Unsupported LLM configuration type: {type(self.config)}\")\n            \n            self._llm_client = client\n            return self._llm_client\n        except ImportError as e:\n            sdk_name = \"\"\n            if \"openai\" in str(e).lower(): sdk_name = \"openai\"\n            elif \"anthropic\" in str(e).lower(): sdk_name = \"anthropic\"\n            # google-genai import is handled by genai being None\n            if sdk_name:\n                raise LLMError(f\"The {sdk_name} SDK is not installed. Please install it to use {sdk_name.capitalize()} models.\") from e\n            raise # Re-raise if it's a different import error\n        except Exception as e:\n            logger.error(f\"Error initializing LLM client: {e}\")\n            raise LLMError(f\"Error initializing LLM client: {e}\") from e",
        "file": "src/kit/summaries.py"
      },
      {
        "name": "summarize_file",
        "type": "method",
        "start_line": 317,
        "end_line": 459,
        "code": "def summarize_file(self, file_path: str) -> str:\n        \"\"\"\n        Summarizes the content of a single file.\n\n        Args:\n            file_path: The path to the file to summarize.\n\n        Returns:\n            A string containing the summary of the file.\n\n        Raises:\n            FileNotFoundError: If the file_path does not exist.\n            LLMError: If there's an error from the LLM API or an empty summary.\n        \"\"\"\n        logger.debug(f\"Attempting to summarize file: {file_path}\")\n        abs_file_path = self.repo.get_abs_path(file_path) # Use get_abs_path\n    \n        try:\n            file_content = self.repo.get_file_content(abs_file_path)\n        except FileNotFoundError:\n            # Re-raise to ensure the Summarizer's contract is met\n            raise FileNotFoundError(f\"File not found via repo: {abs_file_path}\")\n\n        if not file_content.strip():\n            logger.warning(f\"File {abs_file_path} is empty or contains only whitespace. Skipping summary.\")\n            return \"\"\n\n        if len(file_content) > MAX_FILE_SUMMARIZE_CHARS:\n            logger.warning(f\"File content for {file_path} ({len(file_content)} chars) is too large for summarization (limit: {MAX_FILE_SUMMARIZE_CHARS}).\")\n            return f\"File content too large ({len(file_content)} characters) to summarize with current limits.\"\n\n        # Max model context is 128000 tokens. Avg ~4 chars/token -> ~512,000 chars for total message.\n        # Let's set a threshold for the raw content itself.\n        MAX_CHARS_FOR_SUMMARY = 400_000  # Approx 100k tokens\n        if len(file_content) > MAX_CHARS_FOR_SUMMARY:\n            logger.warning(\n                f\"File {abs_file_path} content is too large ({len(file_content)} chars) \"\n                f\"to summarize reliably. Skipping.\"\n            )\n            # Return a placeholder summary or an empty string\n            return f\"File content too large ({len(file_content)} characters) to summarize.\"\n\n        system_prompt_text = \"You are an expert assistant skilled in creating concise and informative code summaries.\"\n        user_prompt_text = f\"Summarize the following code from the file '{file_path}'. Provide a high-level overview of its purpose, key components, and functionality. Focus on what the code does, not just how it's written. The code is:\\n\\n```\\n{file_content}\\n```\"\n\n        client = self._get_llm_client()\n        summary = \"\"\n\n        logger.debug(f\"System Prompt for {file_path}: {system_prompt_text}\")\n        logger.debug(f\"User Prompt for {file_path} (first 200 chars): {user_prompt_text[:200]}...\")\n        # Get model name from config if available, otherwise pass None for default\n        model_name = self.config.model if self.config is not None and hasattr(self.config, 'model') else None\n        token_count = self._count_tokens(user_prompt_text, model_name)\n        if token_count is not None:\n            logger.debug(f\"Estimated tokens for user prompt ({file_path}): {token_count}\")\n        else:\n            logger.debug(f\"Approximate characters for user prompt ({file_path}): {len(user_prompt_text)}\")\n\n        try:\n            # If a custom llm_client was provided without a config, use it directly\n            if self.config is None:\n                # For custom llm_client without config, assume it knows how to handle the prompt\n                # This is used in tests with FakeOpenAI\n                try:\n                    # Try OpenAI-style interface first\n                    response = client.chat.completions.create(\n                        messages=[\n                            {\"role\": \"system\", \"content\": system_prompt_text},\n                            {\"role\": \"user\", \"content\": user_prompt_text}\n                        ]\n                    )\n                    summary = response.choices[0].message.content\n                except (AttributeError, TypeError) as e:\n                    # If that fails, the client might have a different interface\n                    logger.warning(f\"Custom LLM client doesn't support OpenAI-style interface: {e}\")\n                    raise LLMError(f\"Custom LLM client without config doesn't support expected interface: {e}\")\n            elif isinstance(self.config, OpenAIConfig):\n                messages_for_api = [\n                    {\"role\": \"system\", \"content\": system_prompt_text},\n                    {\"role\": \"user\", \"content\": user_prompt_text}\n                ]\n                prompt_token_count = self._count_openai_chat_tokens(messages_for_api, self.config.model)\n                if prompt_token_count is not None and prompt_token_count > OPENAI_MAX_PROMPT_TOKENS:\n                    summary = f\"Summary generation failed: OpenAI prompt too large ({prompt_token_count} tokens). Limit is {OPENAI_MAX_PROMPT_TOKENS} tokens.\"\n                else:\n                    response = client.chat.completions.create(\n                        model=self.config.model,\n                        messages=messages_for_api,\n                        temperature=self.config.temperature,\n                        max_tokens=self.config.max_tokens,\n                    )\n                    summary = response.choices[0].message.content\n                    if response.usage:\n                        logger.debug(f\"OpenAI API usage for {file_path}: {response.usage}\")\n            elif isinstance(self.config, AnthropicConfig):\n                response = client.messages.create(\n                    model=self.config.model,\n                    system=system_prompt_text,\n                    messages=[\n                        {\"role\": \"user\", \"content\": user_prompt_text}\n                    ],\n                    max_tokens=self.config.max_tokens,\n                    temperature=self.config.temperature,\n                )\n                summary = response.content[0].text\n            elif isinstance(self.config, GoogleConfig):\n                if not genai_types:\n                    raise LLMError(\"Google Gen AI SDK (google-genai) types not available. SDK might not be installed correctly.\")\n                \n                generation_config_params: Dict[str, Any] = self.config.model_kwargs.copy() if self.config.model_kwargs is not None else {}\n\n                if self.config.temperature is not None:\n                    generation_config_params[\"temperature\"] = self.config.temperature\n                if self.config.max_output_tokens is not None:\n                    generation_config_params[\"max_output_tokens\"] = self.config.max_output_tokens\n                \n                final_sdk_params = generation_config_params if generation_config_params else None\n\n                response = client.models.generate_content(\n                    model=self.config.model,\n                    contents=user_prompt_text,\n                    generation_config=final_sdk_params\n                )\n                # Check for blocked prompt first\n                if hasattr(response, 'prompt_feedback') and response.prompt_feedback and response.prompt_feedback.block_reason:\n                    logger.warning(f\"Google LLM prompt for file {file_path} blocked. Reason: {response.prompt_feedback.block_reason}\")\n                    summary = f\"Summary generation failed: Prompt blocked by API (Reason: {response.prompt_feedback.block_reason})\"\n                elif not response.text:\n                    logger.warning(f\"Google LLM returned no text for file {file_path}. Response: {response}\")\n                    summary = \"Summary generation failed: No text returned by API.\"\n                else:\n                    summary = response.text\n            \n            if not summary or not summary.strip():\n                logger.warning(f\"LLM returned an empty or whitespace-only summary for file {file_path}.\")\n                raise LLMError(f\"LLM returned an empty summary for file {file_path}.\")\n            \n            logger.debug(f\"LLM summary for file {file_path} (first 200 chars): {summary[:200]}...\")\n            return summary.strip()\n\n        except Exception as e:\n            logger.error(f\"Error communicating with LLM API for file {file_path}: {e}\")\n            raise LLMError(f\"Error communicating with LLM API: {e}\") from e",
        "file": "src/kit/summaries.py"
      },
      {
        "name": "summarize_function",
        "type": "method",
        "start_line": 461,
        "end_line": 603,
        "code": "def summarize_function(self, file_path: str, function_name: str) -> str:\n        \"\"\"\n        Summarizes a specific function within a file.\n\n        Args:\n            file_path: The path to the file containing the function.\n            function_name: The name of the function to summarize.\n\n        Returns:\n            A string containing the summary of the function.\n\n        Raises:\n            FileNotFoundError: If the file_path does not exist.\n            ValueError: If the function cannot be found in the file.\n            LLMError: If there's an error from the LLM API or an empty summary.\n        \"\"\"\n        logger.debug(f\"Attempting to summarize function: {function_name} in file: {file_path}\")\n        \n        symbols = self.repo.extract_symbols(file_path)\n        function_code = None\n        for symbol in symbols:\n            # Use node_path if available (more precise), fallback to name\n            current_symbol_name = symbol.get(\"node_path\", symbol.get(\"name\"))\n            if current_symbol_name == function_name and symbol.get(\"type\", \"\").upper() in [\"FUNCTION\", \"METHOD\"]:\n                function_code = symbol.get(\"code\")\n                break\n        \n        if not function_code:\n            raise ValueError(f\"Could not find function '{function_name}' in '{file_path}'.\")\n\n        # Max model context is 128000 tokens. Avg ~4 chars/token -> ~512,000 chars for total message.\n        # Let's set a threshold for the raw content itself.\n        MAX_CHARS_FOR_SUMMARY = 400_000  # Approx 100k tokens\n        if len(function_code) > MAX_CHARS_FOR_SUMMARY:\n            logger.warning(\n                f\"Function {function_name} in file {file_path} content is too large ({len(function_code)} chars) \"\n                f\"to summarize reliably. Skipping.\"\n            )\n            return f\"Function content too large ({len(function_code)} characters) to summarize.\"\n\n        system_prompt_text = \"You are an expert assistant skilled in creating concise code summaries for functions.\"\n        user_prompt_text = f\"Summarize the following function named '{function_name}' from the file '{file_path}'. Describe its purpose, parameters, and return value. The function definition is:\\n\\n```\\n{function_code}\\n```\"\n\n        client = self._get_llm_client()\n        summary = \"\"\n\n        logger.debug(f\"System Prompt for {function_name} in {file_path}: {system_prompt_text}\")\n        logger.debug(f\"User Prompt for {function_name} in {file_path} (first 200 chars): {user_prompt_text[:200]}...\")\n        # Get model name from config if available, otherwise pass None for default\n        model_name = self.config.model if self.config is not None and hasattr(self.config, 'model') else None\n        token_count = self._count_tokens(user_prompt_text, model_name)\n        logger.debug(f\"Token count for {function_name} in {file_path}: {token_count}\")\n\n        try:\n            # If a custom llm_client was provided without a config, use it directly\n            if self.config is None:\n                # For custom llm_client without config, assume it knows how to handle the prompt\n                # This is used in tests with FakeOpenAI\n                try:\n                    # Try OpenAI-style interface first\n                    response = client.chat.completions.create(\n                        messages=[\n                            {\"role\": \"system\", \"content\": system_prompt_text},\n                            {\"role\": \"user\", \"content\": user_prompt_text}\n                        ]\n                    )\n                    summary = response.choices[0].message.content\n                except (AttributeError, TypeError) as e:\n                    # If that fails, the client might have a different interface\n                    # In a real implementation, you'd need more robust handling here\n                    logger.warning(f\"Custom LLM client doesn't support OpenAI-style interface: {e}\")\n                    raise LLMError(f\"Custom LLM client without config doesn't support expected interface: {e}\")\n            elif isinstance(self.config, OpenAIConfig):\n                messages_for_api = [\n                    {\"role\": \"system\", \"content\": system_prompt_text},\n                    {\"role\": \"user\", \"content\": user_prompt_text}\n                ]\n                prompt_token_count = self._count_openai_chat_tokens(messages_for_api, self.config.model)\n                if prompt_token_count is not None and prompt_token_count > OPENAI_MAX_PROMPT_TOKENS:\n                    summary = f\"Summary generation failed: OpenAI prompt too large ({prompt_token_count} tokens). Limit is {OPENAI_MAX_PROMPT_TOKENS} tokens.\"\n                else:\n                    response = client.chat.completions.create(\n                        model=self.config.model,\n                        messages=messages_for_api,\n                        temperature=self.config.temperature,\n                        max_tokens=self.config.max_tokens,\n                    )\n                    summary = response.choices[0].message.content\n                    if response.usage:\n                        logger.debug(f\"OpenAI API usage for {function_name} in {file_path}: {response.usage}\")\n            elif isinstance(self.config, AnthropicConfig):\n                response = client.messages.create(\n                    model=self.config.model,\n                    system=system_prompt_text,\n                    messages=[\n                        {\"role\": \"user\", \"content\": user_prompt_text}\n                    ],\n                    max_tokens=self.config.max_tokens,\n                    temperature=self.config.temperature,\n                )\n                summary = response.content[0].text\n                # Anthropic usage might be in response.usage (confirm API docs)\n                # Example: logger.debug(f\"Anthropic API usage for {function_name} in {file_path}: {response.usage}\")\n            elif isinstance(self.config, GoogleConfig):\n                if not genai_types:\n                    raise LLMError(\"Google Gen AI SDK (google-genai) types not available. SDK might not be installed correctly.\")\n\n                generation_config_params: Dict[str, Any] = self.config.model_kwargs.copy() if self.config.model_kwargs is not None else {}\n\n                if self.config.temperature is not None:\n                    generation_config_params[\"temperature\"] = self.config.temperature\n                if self.config.max_output_tokens is not None:\n                    generation_config_params[\"max_output_tokens\"] = self.config.max_output_tokens\n                \n                final_sdk_params = generation_config_params if generation_config_params else None\n\n                response = client.models.generate_content(\n                    model=self.config.model,\n                    contents=user_prompt_text,\n                    generation_config=final_sdk_params\n                )\n                if hasattr(response, 'prompt_feedback') and response.prompt_feedback and response.prompt_feedback.block_reason:\n                    logger.warning(f\"Google LLM prompt for {function_name} in {file_path} blocked. Reason: {response.prompt_feedback.block_reason}\")\n                    summary = f\"Summary generation failed: Prompt blocked by API (Reason: {response.prompt_feedback.block_reason})\"\n                elif not response.text:\n                    logger.warning(f\"Google LLM returned no text for {function_name} in {file_path}. Response: {response}\")\n                    summary = \"Summary generation failed: No text returned by API.\"\n                else:\n                    summary = response.text\n            else:\n                # This should never happen with our current logic, but as a safeguard\n                raise LLMError(f\"Unsupported LLM configuration type: {type(self.config) if self.config else None}\")\n            \n            if not summary or not summary.strip():\n                logger.warning(f\"LLM returned an empty or whitespace-only summary for function {function_name} in {file_path}.\")\n                raise LLMError(f\"LLM returned an empty summary for function {function_name}.\")\n            \n            logger.debug(f\"LLM summary for {function_name} in {file_path} (first 200 chars): {summary[:200]}...\")\n            return summary.strip()\n\n        except Exception as e:\n            logger.error(f\"Error communicating with LLM API for function {function_name} in {file_path}: {e}\")\n            raise LLMError(f\"Error communicating with LLM API for function {function_name}: {e}\") from e",
        "file": "src/kit/summaries.py"
      },
      {
        "name": "summarize_class",
        "type": "method",
        "start_line": 605,
        "end_line": 745,
        "code": "def summarize_class(self, file_path: str, class_name: str) -> str:\n        \"\"\"\n        Summarizes a specific class within a file.\n\n        Args:\n            file_path: The path to the file containing the class.\n            class_name: The name of the class to summarize.\n\n        Returns:\n            A string containing the summary of the class.\n\n        Raises:\n            FileNotFoundError: If the file_path does not exist.\n            ValueError: If the class cannot be found in the file.\n            LLMError: If there's an error from the LLM API or an empty summary.\n        \"\"\"\n        logger.debug(f\"Attempting to summarize class: {class_name} in file: {file_path}\")\n\n        symbols = self.repo.extract_symbols(file_path)\n        class_code = None\n        for symbol in symbols:\n            # Use node_path if available (more precise), fallback to name\n            current_symbol_name = symbol.get(\"node_path\", symbol.get(\"name\"))\n            if current_symbol_name == class_name and symbol.get(\"type\", \"\").upper() == \"CLASS\":\n                class_code = symbol.get(\"code\")\n                break\n\n        if not class_code:\n            raise ValueError(f\"Could not find class '{class_name}' in '{file_path}'.\")\n\n        # Max model context is 128000 tokens. Avg ~4 chars/token -> ~512,000 chars for total message.\n        # Let's set a threshold for the raw content itself.\n        MAX_CHARS_FOR_SUMMARY = 400_000  # Approx 100k tokens\n        if len(class_code) > MAX_CHARS_FOR_SUMMARY:\n            logger.warning(\n                f\"Class {class_name} in file {file_path} content is too large ({len(class_code)} chars) \"\n                f\"to summarize reliably. Skipping.\"\n            )\n            return f\"Class content too large ({len(class_code)} characters) to summarize.\"\n        \n        system_prompt_text = \"You are an expert assistant skilled in creating concise code summaries for classes.\"\n        user_prompt_text = f\"Summarize the following class named '{class_name}' from the file '{file_path}'. Describe its purpose, key attributes, and main methods. The class definition is:\\n\\n```\\n{class_code}\\n```\"\n        \n        client = self._get_llm_client()\n        summary = \"\"\n\n        logger.debug(f\"System Prompt for {class_name} in {file_path}: {system_prompt_text}\")\n        logger.debug(f\"User Prompt for {class_name} (first 200 chars): {user_prompt_text[:200]}...\")\n        # Get model name from config if available, otherwise pass None for default\n        model_name = self.config.model if self.config is not None and hasattr(self.config, 'model') else None\n        token_count = self._count_tokens(user_prompt_text, model_name)\n        logger.debug(f\"Token count for {class_name} in {file_path}: {token_count}\")\n\n        try:\n            # If a custom llm_client was provided without a config, use it directly\n            if self.config is None:\n                # For custom llm_client without config, assume it knows how to handle the prompt\n                # This is used in tests with FakeOpenAI\n                try:\n                    # Try OpenAI-style interface first\n                    response = client.chat.completions.create(\n                        messages=[\n                            {\"role\": \"system\", \"content\": system_prompt_text},\n                            {\"role\": \"user\", \"content\": user_prompt_text}\n                        ]\n                    )\n                    summary = response.choices[0].message.content\n                except (AttributeError, TypeError) as e:\n                    logger.warning(f\"Custom LLM client doesn't support OpenAI-style interface: {e}\")\n                    raise LLMError(f\"Custom LLM client without config doesn't support expected interface: {e}\")\n            elif isinstance(self.config, OpenAIConfig):\n                messages_for_api = [\n                    {\"role\": \"system\", \"content\": system_prompt_text},\n                    {\"role\": \"user\", \"content\": user_prompt_text}\n                ]\n                prompt_token_count = self._count_openai_chat_tokens(messages_for_api, self.config.model)\n                if prompt_token_count is not None and prompt_token_count > OPENAI_MAX_PROMPT_TOKENS:\n                    summary = f\"Summary generation failed: OpenAI prompt too large ({prompt_token_count} tokens). Limit is {OPENAI_MAX_PROMPT_TOKENS} tokens.\"\n                else:\n                    response = client.chat.completions.create(\n                        model=self.config.model,\n                        messages=messages_for_api,\n                        temperature=self.config.temperature,\n                        max_tokens=self.config.max_tokens,\n                    )\n                    summary = response.choices[0].message.content\n                    if response.usage:\n                        logger.debug(f\"OpenAI API usage for {class_name} in {file_path}: {response.usage}\")\n            elif isinstance(self.config, AnthropicConfig):\n                response = client.messages.create(\n                    model=self.config.model,\n                    system=system_prompt_text,\n                    messages=[\n                        {\"role\": \"user\", \"content\": user_prompt_text}\n                    ],\n                    max_tokens=self.config.max_tokens,\n                    temperature=self.config.temperature,\n                )\n                summary = response.content[0].text\n                # Anthropic usage might be in response.usage (confirm API docs)\n                # Example: logger.debug(f\"Anthropic API usage for {class_name} in {file_path}: {response.usage}\")\n            elif isinstance(self.config, GoogleConfig):\n                if not genai_types:\n                    raise LLMError(\"Google Gen AI SDK (google-genai) types not available. SDK might not be installed correctly.\")\n                \n                generation_config_params: Dict[str, Any] = self.config.model_kwargs.copy() if self.config.model_kwargs is not None else {}\n\n                if self.config.temperature is not None:\n                    generation_config_params[\"temperature\"] = self.config.temperature\n                if self.config.max_output_tokens is not None:\n                    generation_config_params[\"max_output_tokens\"] = self.config.max_output_tokens\n                \n                final_sdk_params = generation_config_params if generation_config_params else None\n\n                response = client.models.generate_content(\n                    model=self.config.model,\n                    contents=user_prompt_text,\n                    generation_config=final_sdk_params\n                )\n                if hasattr(response, 'prompt_feedback') and response.prompt_feedback and response.prompt_feedback.block_reason:\n                    logger.warning(f\"Google LLM prompt for {class_name} in {file_path} blocked. Reason: {response.prompt_feedback.block_reason}\")\n                    summary = f\"Summary generation failed: Prompt blocked by API (Reason: {response.prompt_feedback.block_reason})\"\n                elif not response.text:\n                    logger.warning(f\"Google LLM returned no text for {class_name} in {file_path}. Response: {response}\")\n                    summary = \"Summary generation failed: No text returned by API.\"\n                else:\n                    summary = response.text\n            else:\n                # This should never happen with our current logic, but as a safeguard\n                raise LLMError(f\"Unsupported LLM configuration type: {type(self.config) if self.config else None}\")\n            \n            if not summary or not summary.strip():\n                logger.warning(f\"LLM returned an empty or whitespace-only summary for class {class_name} in {file_path}.\")\n                raise LLMError(f\"LLM returned an empty summary for class {class_name}.\")\n            \n            logger.debug(f\"LLM summary for {class_name} in {file_path} (first 200 chars): {summary[:200]}...\")\n            return summary.strip()\n\n        except Exception as e:\n            logger.error(f\"Error communicating with LLM API for class {class_name} in {file_path}: {e}\")\n            raise LLMError(f\"Error communicating with LLM API for class {class_name}: {e}\") from e",
        "file": "src/kit/summaries.py"
      }
    ],
    "src/kit/api/__init__.py": [],
    "src/kit/api/app.py": [
      {
        "name": "RepoIn",
        "type": "class",
        "start_line": 14,
        "end_line": 16,
        "code": "class RepoIn(BaseModel):\n    path_or_url: str\n    github_token: str | None = None",
        "file": "src/kit/api/app.py"
      },
      {
        "name": "open_repo",
        "type": "function",
        "start_line": 23,
        "end_line": 28,
        "code": "def open_repo(body: RepoIn):\n    \"\"\"Create/open a repository and return its ID.\"\"\"\n    repo = Repository(body.path_or_url, github_token=body.github_token)\n    repo_id = str(len(_repos) + 1)\n    _repos[repo_id] = repo\n    return {\"id\": repo_id}",
        "file": "src/kit/api/app.py"
      },
      {
        "name": "search_text",
        "type": "function",
        "start_line": 32,
        "end_line": 36,
        "code": "def search_text(repo_id: str, q: str, pattern: str = \"*.py\"):\n    repo = _repos.get(repo_id)\n    if not repo:\n        raise HTTPException(status_code=404, detail=\"Repo not found\")\n    return repo.search_text(q, file_pattern=pattern)",
        "file": "src/kit/api/app.py"
      },
      {
        "name": "build_context",
        "type": "function",
        "start_line": 40,
        "end_line": 46,
        "code": "def build_context(repo_id: str, diff: str = Body(..., embed=True)):\n    repo = _repos.get(repo_id)\n    if not repo:\n        raise HTTPException(status_code=404, detail=\"Repo not found\")\n    assembler: ContextAssembler = repo.get_context_assembler()\n    assembler.add_diff(diff)\n    return {\"context\": assembler.format_context()}",
        "file": "src/kit/api/app.py"
      }
    ],
    "tests/fixtures/realistic_repo/__init__.py": [],
    "tests/fixtures/realistic_repo/utils.py": [
      {
        "name": "greet",
        "type": "function",
        "start_line": 5,
        "end_line": 7,
        "code": "def greet(name: str) -> str:\n    \"\"\"Return a friendly greeting.\"\"\"\n    return f\"Hello, {name}!\"",
        "file": "tests/fixtures/realistic_repo/utils.py"
      },
      {
        "name": "format_timestamp",
        "type": "function",
        "start_line": 9,
        "end_line": 20,
        "code": "def format_timestamp(ts: float, format_string: str = \"%Y-%m-%d %H:%M:%S\") -> str:\n    \"\"\"Formats a Unix timestamp into a human-readable string.\n\n    Args:\n        ts: The Unix timestamp (float).\n        format_string: The strftime format string.\n\n    Returns:\n        A string representing the formatted timestamp.\n    \"\"\"\n    dt_object = datetime.datetime.fromtimestamp(ts)\n    return dt_object.strftime(format_string)",
        "file": "tests/fixtures/realistic_repo/utils.py"
      },
      {
        "name": "is_valid_email",
        "type": "function",
        "start_line": 22,
        "end_line": 35,
        "code": "def is_valid_email(email: str) -> bool:\n    \"\"\"Checks if the provided string is a valid email address (basic check).\n\n    Args:\n        email: The email string to validate.\n\n    Returns:\n        True if the email format is valid, False otherwise.\n    \"\"\"\n    # A simple regex for email validation\n    pattern = r\"^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$\"\n    if re.match(pattern, email):\n        return True\n    return False",
        "file": "tests/fixtures/realistic_repo/utils.py"
      }
    ],
    "tests/fixtures/realistic_repo/app.py": [
      {
        "name": "main",
        "type": "function",
        "start_line": 7,
        "end_line": 102,
        "code": "def main() -> None:\n    \"\"\"Run an expanded demo workflow showcasing more features.\"\"\"\n    print(\"Kit Realistic Repo Demo\\n\" + \"=\"*30)\n\n    auth_service = AuthService()\n\n    # 1. Register users\n    print(\"\\n--- Registering Users ---\")\n    user_alice_details = {\"username\": \"alice\", \"email\": \"alice@example.com\", \"password\": \"password\"}\n    user_bob_details = {\"username\": \"bob\", \"email\": \"bob@example.com\", \"password\": \"password\"}\n\n    alice = auth_service.register_user(**user_alice_details)\n    if alice:\n        print(f\"Registered: {alice.display()}\")\n    else:\n        print(f\"Failed to register {user_alice_details['username']}\")\n\n    bob = auth_service.register_user(**user_bob_details)\n    if bob:\n        print(f\"Registered: {bob.display()}\")\n    else:\n        print(f\"Failed to register {user_bob_details['username']}\")\n\n    # Attempt to register Alice again (should fail)\n    print(\"\\n--- Attempting to re-register Alice ---\")\n    alice_again = auth_service.register_user(**user_alice_details)\n    if not alice_again:\n        print(f\"Successfully prevented re-registration of {user_alice_details['username']}.\")\n    else:\n        print(f\"ERROR: Re-registration of {user_alice_details['username']} was allowed.\")\n\n    # 2. Login Alice\n    print(\"\\n--- Logging in Alice ---\")\n    alice_token = auth_service.login(username=\"alice\", password=\"password\")\n    if alice_token:\n        print(f\"Alice logged in. Token: {alice_token[:8]}...\")\n        assert auth_service.is_valid_token(alice_token), \"Alice's token should be valid\"\n        print(\"Token is valid.\")\n\n        # 3. Get user from token and demonstrate User methods\n        current_user = auth_service.get_user_from_token(alice_token)\n        if current_user:\n            print(f\"\\n--- User details for {current_user.name} (from token) ---\")\n            print(f\"Initial display: {current_user.display()}\")\n            print(f\"Last login timestamp: {current_user.last_login_at}\")\n\n            new_email = \"alice.updated@example.com\"\n            print(f\"Attempting to update email to: {new_email}\")\n            if current_user.update_email(new_email):\n                print(f\"Email updated. New display: {current_user.display()}\")\n            else:\n                print(f\"Failed to update email to {new_email}.\")\n\n            current_user.set_preference(\"theme\", \"dark_mode_pro\")\n            current_user.set_preference(\"notifications_level\", \"critical\")\n            print(f\"Preferences set. Theme: {current_user.get_preference('theme')}\")\n            print(f\"All preferences: {current_user.preferences}\")\n        else:\n            print(\"Could not retrieve user from Alice's token.\")\n\n        # 4. Logout Alice\n        print(\"\\n--- Logging out Alice ---\")\n        auth_service.logout(alice_token)\n        assert not auth_service.is_valid_token(alice_token), \"Alice's token should be invalid after logout\"\n        print(\"Alice logged out. Token is now invalid.\")\n    else:\n        print(\"Alice login failed.\")\n\n    # 5. Demonstrate deactivation with Bob\n    print(\"\\n--- Operations with Bob ---\")\n    bob_token = auth_service.login(username=\"bob\", password=\"password\")\n    if bob_token and bob: \n        print(f\"Bob logged in. Token: {bob_token[:8]}...\")\n        retrieved_bob = auth_service.get_user_from_token(bob_token)\n        if retrieved_bob:\n            print(f\"Deactivating Bob ({retrieved_bob.name})...\")\n            retrieved_bob.deactivate() \n            print(f\"Bob's account status: {'active' if retrieved_bob.is_active else 'inactive'}\")\n\n            print(\"Attempting to log in deactivated Bob...\")\n            bob_deactivated_token = auth_service.login(username=\"bob\", password=\"password\")\n            if not bob_deactivated_token:\n                print(\"Login for deactivated Bob correctly failed.\")\n            else:\n                print(\"ERROR: Deactivated Bob was able to log in.\")\n        else:\n            print(\"Could not retrieve Bob from token for deactivation.\")\n        \n        # Logout Bob if he was logged in\n        auth_service.logout(bob_token)\n        print(\"Bob logged out.\")\n\n    else:\n        print(\"Bob login failed or Bob object not found.\")\n\n    print(f\"\\n{greet('Developer')}! Demo finished.\")",
        "file": "tests/fixtures/realistic_repo/app.py"
      }
    ],
    "tests/fixtures/realistic_repo/models/user.py": [
      {
        "name": "User",
        "type": "class",
        "start_line": 10,
        "end_line": 66,
        "code": "class User:\n    id: int\n    name: str\n    email: str\n    is_active: bool = True\n    created_at: float = field(default_factory=time.time)\n    last_login_at: Optional[float] = None\n    preferences: Dict[str, Any] = field(default_factory=dict)\n\n    def display(self) -> str:\n        \"\"\"Return a human readable representation.\"\"\"\n        status = \"active\" if self.is_active else \"inactive\"\n        return f\"<{self.id}> {self.name} <{self.email}> ({status})\"\n\n    def update_email(self, new_email: str) -> bool:\n        \"\"\"Updates the user's email address after validating it.\n\n        Args:\n            new_email: The new email address.\n\n        Returns:\n            True if the email was updated, False otherwise.\n        \"\"\"\n        if is_valid_email(new_email):\n            self.email = new_email\n            return True\n        return False\n\n    def deactivate(self) -> None:\n        \"\"\"Deactivates the user's account.\"\"\"\n        self.is_active = False\n        print(f\"User {self.name} deactivated.\")\n\n    def record_login(self) -> None:\n        \"\"\"Updates the last_login_at timestamp to the current time.\"\"\"\n        self.last_login_at = time.time()\n\n    def set_preference(self, key: str, value: Any) -> None:\n        \"\"\"Sets a user preference.\n\n        Args:\n            key: The preference key.\n            value: The preference value.\n        \"\"\"\n        self.preferences[key] = value\n\n    def get_preference(self, key: str, default: Optional[Any] = None) -> Optional[Any]:\n        \"\"\"Gets a user preference.\n\n        Args:\n            key: The preference key.\n            default: The default value to return if the key is not found.\n\n        Returns:\n            The preference value or the default.\n        \"\"\"\n        return self.preferences.get(key, default)",
        "file": "tests/fixtures/realistic_repo/models/user.py"
      },
      {
        "name": "display",
        "type": "method",
        "start_line": 19,
        "end_line": 22,
        "code": "def display(self) -> str:\n        \"\"\"Return a human readable representation.\"\"\"\n        status = \"active\" if self.is_active else \"inactive\"\n        return f\"<{self.id}> {self.name} <{self.email}> ({status})\"",
        "file": "tests/fixtures/realistic_repo/models/user.py"
      },
      {
        "name": "update_email",
        "type": "method",
        "start_line": 24,
        "end_line": 36,
        "code": "def update_email(self, new_email: str) -> bool:\n        \"\"\"Updates the user's email address after validating it.\n\n        Args:\n            new_email: The new email address.\n\n        Returns:\n            True if the email was updated, False otherwise.\n        \"\"\"\n        if is_valid_email(new_email):\n            self.email = new_email\n            return True\n        return False",
        "file": "tests/fixtures/realistic_repo/models/user.py"
      },
      {
        "name": "deactivate",
        "type": "method",
        "start_line": 38,
        "end_line": 41,
        "code": "def deactivate(self) -> None:\n        \"\"\"Deactivates the user's account.\"\"\"\n        self.is_active = False\n        print(f\"User {self.name} deactivated.\")",
        "file": "tests/fixtures/realistic_repo/models/user.py"
      },
      {
        "name": "record_login",
        "type": "method",
        "start_line": 43,
        "end_line": 45,
        "code": "def record_login(self) -> None:\n        \"\"\"Updates the last_login_at timestamp to the current time.\"\"\"\n        self.last_login_at = time.time()",
        "file": "tests/fixtures/realistic_repo/models/user.py"
      },
      {
        "name": "set_preference",
        "type": "method",
        "start_line": 47,
        "end_line": 54,
        "code": "def set_preference(self, key: str, value: Any) -> None:\n        \"\"\"Sets a user preference.\n\n        Args:\n            key: The preference key.\n            value: The preference value.\n        \"\"\"\n        self.preferences[key] = value",
        "file": "tests/fixtures/realistic_repo/models/user.py"
      },
      {
        "name": "get_preference",
        "type": "method",
        "start_line": 56,
        "end_line": 66,
        "code": "def get_preference(self, key: str, default: Optional[Any] = None) -> Optional[Any]:\n        \"\"\"Gets a user preference.\n\n        Args:\n            key: The preference key.\n            default: The default value to return if the key is not found.\n\n        Returns:\n            The preference value or the default.\n        \"\"\"\n        return self.preferences.get(key, default)",
        "file": "tests/fixtures/realistic_repo/models/user.py"
      }
    ],
    "tests/fixtures/realistic_repo/models/__init__.py": [],
    "tests/fixtures/realistic_repo/services/auth.py": [
      {
        "name": "AuthService",
        "type": "class",
        "start_line": 8,
        "end_line": 80,
        "code": "class AuthService:\n    \"\"\"Manages user registration, login, and session tokens (in-memory demo).\"\"\"\n\n    def __init__(self):\n        self._users_by_username: Dict[str, User] = {}\n        self._users_by_id: Dict[int, User] = {}\n        self._active_tokens: Dict[str, int] = {}  # token_string -> user_id\n        self._next_user_id: int = 1\n\n    def register_user(self, username: str, email: str, password: str) -> Optional[User]:\n        \"\"\"Registers a new user.\n\n        Args:\n            username: The desired username.\n            email: The user's email address.\n            password: The user's password (ignored for this demo).\n\n        Returns:\n            The created User object, or None if username is taken.\n        \"\"\"\n        if username in self._users_by_username:\n            return None  # Username already taken\n\n        # In a real app, hash the password!\n        user_id = self._next_user_id\n        new_user = User(id=user_id, name=username, email=email)\n        self._next_user_id += 1\n\n        self._users_by_username[username] = new_user\n        self._users_by_id[user_id] = new_user\n        return new_user\n\n    def login(self, *, username: str, password: str) -> Optional[str]:\n        \"\"\"Logs in a user and returns a session token.\n\n        Args:\n            username: The username.\n            password: The password.\n\n        Returns:\n            A session token string if login is successful, None otherwise.\n        \"\"\"\n        user = self._users_by_username.get(username)\n        # Demo: check if user exists and password is 'password' (DO NOT use in real life!)\n        if user and user.is_active and password == \"password\":\n            token = str(uuid.uuid4())\n            self._active_tokens[token] = user.id\n            user.record_login() # Update last_login_at on the User model\n            return token\n        return None\n\n    def logout(self, token: str) -> None:\n        \"\"\"Logs out a user by invalidating their token.\"\"\"\n        if token in self._active_tokens:\n            del self._active_tokens[token]\n\n    def is_valid_token(self, token: str) -> bool:\n        \"\"\"Checks if a token is currently valid.\"\"\"\n        return token in self._active_tokens\n\n    def get_user_from_token(self, token: str) -> Optional[User]:\n        \"\"\"Retrieves the User object associated with a valid token.\n\n        Args:\n            token: The session token.\n\n        Returns:\n            The User object if the token is valid, None otherwise.\n        \"\"\"\n        user_id = self._active_tokens.get(token)\n        if user_id:\n            return self._users_by_id.get(user_id)\n        return None",
        "file": "tests/fixtures/realistic_repo/services/auth.py"
      },
      {
        "name": "__init__",
        "type": "method",
        "start_line": 11,
        "end_line": 15,
        "code": "def __init__(self):\n        self._users_by_username: Dict[str, User] = {}\n        self._users_by_id: Dict[int, User] = {}\n        self._active_tokens: Dict[str, int] = {}  # token_string -> user_id\n        self._next_user_id: int = 1",
        "file": "tests/fixtures/realistic_repo/services/auth.py"
      },
      {
        "name": "register_user",
        "type": "method",
        "start_line": 17,
        "end_line": 38,
        "code": "def register_user(self, username: str, email: str, password: str) -> Optional[User]:\n        \"\"\"Registers a new user.\n\n        Args:\n            username: The desired username.\n            email: The user's email address.\n            password: The user's password (ignored for this demo).\n\n        Returns:\n            The created User object, or None if username is taken.\n        \"\"\"\n        if username in self._users_by_username:\n            return None  # Username already taken\n\n        # In a real app, hash the password!\n        user_id = self._next_user_id\n        new_user = User(id=user_id, name=username, email=email)\n        self._next_user_id += 1\n\n        self._users_by_username[username] = new_user\n        self._users_by_id[user_id] = new_user\n        return new_user",
        "file": "tests/fixtures/realistic_repo/services/auth.py"
      },
      {
        "name": "login",
        "type": "method",
        "start_line": 40,
        "end_line": 57,
        "code": "def login(self, *, username: str, password: str) -> Optional[str]:\n        \"\"\"Logs in a user and returns a session token.\n\n        Args:\n            username: The username.\n            password: The password.\n\n        Returns:\n            A session token string if login is successful, None otherwise.\n        \"\"\"\n        user = self._users_by_username.get(username)\n        # Demo: check if user exists and password is 'password' (DO NOT use in real life!)\n        if user and user.is_active and password == \"password\":\n            token = str(uuid.uuid4())\n            self._active_tokens[token] = user.id\n            user.record_login() # Update last_login_at on the User model\n            return token\n        return None",
        "file": "tests/fixtures/realistic_repo/services/auth.py"
      },
      {
        "name": "logout",
        "type": "method",
        "start_line": 59,
        "end_line": 62,
        "code": "def logout(self, token: str) -> None:\n        \"\"\"Logs out a user by invalidating their token.\"\"\"\n        if token in self._active_tokens:\n            del self._active_tokens[token]",
        "file": "tests/fixtures/realistic_repo/services/auth.py"
      },
      {
        "name": "is_valid_token",
        "type": "method",
        "start_line": 64,
        "end_line": 66,
        "code": "def is_valid_token(self, token: str) -> bool:\n        \"\"\"Checks if a token is currently valid.\"\"\"\n        return token in self._active_tokens",
        "file": "tests/fixtures/realistic_repo/services/auth.py"
      },
      {
        "name": "get_user_from_token",
        "type": "method",
        "start_line": 68,
        "end_line": 80,
        "code": "def get_user_from_token(self, token: str) -> Optional[User]:\n        \"\"\"Retrieves the User object associated with a valid token.\n\n        Args:\n            token: The session token.\n\n        Returns:\n            The User object if the token is valid, None otherwise.\n        \"\"\"\n        user_id = self._active_tokens.get(token)\n        if user_id:\n            return self._users_by_id.get(user_id)\n        return None",
        "file": "tests/fixtures/realistic_repo/services/auth.py"
      }
    ],
    "tests/fixtures/realistic_repo/services/db.py": [
      {
        "name": "connect",
        "type": "function",
        "start_line": 2,
        "end_line": 4,
        "code": "def connect(url: str = \"sqlite://:memory:\") -> str:\n    \"\"\"Pretend to open a DB connection and return a connection ID.\"\"\"\n    return f\"conn-{url}\"",
        "file": "tests/fixtures/realistic_repo/services/db.py"
      }
    ],
    "tests/fixtures/realistic_repo/services/__init__.py": []
  },
  "dependencies": {},
  "metadata": {
    "analysis_time": 0.9764499664306641,
    "timestamp": "2025-05-09 23:33:25"
  }
}